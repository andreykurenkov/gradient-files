WEBVTT

1
00:00:05.400 --> 00:00:10.469
<v Andrey>Hello and welcome to the 18th episode of The Gradient podcast,.

2
00:00:10.470 --> 00:00:14.549
<v Andrey>The Gradient is a digital magazine that aims to be a place for discussion

3
00:00:14.550 --> 00:00:18.629
<v Andrey>about research and trends in artificial intelligence and machine

4
00:00:18.630 --> 00:00:22.619
<v Andrey>learning. We interview various people in AI such as engineers,

5
00:00:22.620 --> 00:00:25.079
<v Andrey>researchers, artists and more.

6
00:00:25.080 --> 00:00:27.539
<v Andrey>I'm your host Andrey Kurenkov.

7
00:00:27.540 --> 00:00:29.519
<v Andrey>In this episode, I'm excited to be interviewing

8
00:00:31.530 --> 00:00:35.669
<v Andrey>Upol Ehsan. Upol Cares about people first and technology second.

9
00:00:35.670 --> 00:00:39.659
<v Andrey>He's a doctoral candidate in the School of Interactive Computing at Georgia

10
00:00:39.660 --> 00:00:44.669
<v Andrey>Tech and an affiliate at the Data and Society Research Institute.

11
00:00:44.670 --> 00:00:48.689
<v Andrey>Combining his expertize in AI and background in philosophy

12
00:00:48.690 --> 00:00:52.649
<v Andrey>as work in explainable AI, or XAI,

13
00:00:52.650 --> 00:00:56.699
<v Andrey>aims to foster a future where anyone, regardless of their background, can

14
00:00:56.700 --> 00:00:59.849
<v Andrey>use AI powered technology with dignity.

15
00:00:59.850 --> 00:01:04.109
<v Andrey>Putting the human first and focusing on how our values shape the use

16
00:01:04.110 --> 00:01:09.179
<v Andrey>and abuse of technology, his work has coined the term human-centered

17
00:01:09.180 --> 00:01:13.829
<v Andrey>explainable AI, which is subfield of expainable AI,

18
00:01:13.830 --> 00:01:16.079
<v Andrey>and charted its visions.

19
00:01:16.080 --> 00:01:20.099
<v Andrey>Actively publishing in top peer reviewed venues like CHI, his work has

20
00:01:20.100 --> 00:01:24.809
<v Andrey>received multiple awards and has been covered in major media outlets.

21
00:01:24.810 --> 00:01:28.829
<v Andrey>Bridging industry and academia, he serves on multiple program

22
00:01:28.830 --> 00:01:32.607
<v Andrey>committees in HCI and AI conferences such as Neurips and DIS,

23
00:01:34.320 --> 00:01:36.375
<v Andrey>and equally connects these communities.

24
00:01:36.376 --> 00:01:40.319
<v Andrey>By promoting equity and ethics in AI he wants to

25
00:01:40.320 --> 00:01:44.369
<v Andrey>ensure stakeholders who aren't at the table do not end

26
00:01:44.370 --> 00:01:45.809
<v Andrey>up on the menu.

27
00:01:46.860 --> 00:01:49.151
<v Andrey>Outside of research he is an advisor for Aalor Asha, an educational

28
00:01:51.570 --> 00:01:55.979
<v Andrey>Institute he started for underprivileged children subjected to child

29
00:01:55.980 --> 00:01:57.269
<v Andrey>labor.

30
00:01:57.270 --> 00:01:59.032
<v Andrey>At Twitter you can follow him at @UpolEhsan - U

31
00:02:01.703 --> 00:02:05.119
<v Andrey>P O L E H S A N.

32
00:02:06.390 --> 00:02:10.319
<v Andrey>So I'm very excited for this, Upol has written to The Gradient before,

33
00:02:10.320 --> 00:02:12.509
<v Andrey>and I think his work is super cool.

34
00:02:12.510 --> 00:02:14.396
<v Andrey>Welcome to the podcast, Upol.

35
00:02:15.540 --> 00:02:18.210
<v Upol>Thank you for having me on. It's pleasure to be here.

36
00:02:19.290 --> 00:02:23.309
<v Andrey>Definitely. So as we usually do in these episodes before

37
00:02:23.310 --> 00:02:27.479
<v Andrey>diving into your work a bit on your sort of background,

38
00:02:27.480 --> 00:02:31.009
<v Andrey>I'm curious, how did you get into working on AI?

39
00:02:31.010 --> 00:02:34.949
<v Andrey>I think your trajectory might be interesting or your background

40
00:02:34.950 --> 00:02:36.989
<v Andrey>in philosophy as well.

41
00:02:36.990 --> 00:02:40.919
<v Upol>Yes, I think I have Isaac Asimov to kind

42
00:02:40.920 --> 00:02:43.679
<v Upol>of attribute that credit to.

43
00:02:43.680 --> 00:02:46.979
<v Upol>When I was very young, I got hooked into his books.

44
00:02:46.980 --> 00:02:50.009
<v Upol>I have read forty seven of his books, not just the science fiction...

45
00:02:51.750 --> 00:02:52.439
<v Andrey>Wow, that's a lot.

46
00:02:52.440 --> 00:02:56.459
<v Upol>I mean, the maestro is is someone who's near and dear to my heart,

47
00:02:56.460 --> 00:03:01.559
<v Upol>which makes watching foundation and Apple TV right now a very scary prospect.

48
00:03:01.560 --> 00:03:06.359
<v Upol>Because I remember those things, but I think Asimov

49
00:03:06.360 --> 00:03:10.289
<v Upol>pushed me to think about artificial intelligence in ways that

50
00:03:10.290 --> 00:03:14.249
<v Upol>I don't think I would have thought of, because all of

51
00:03:14.250 --> 00:03:18.269
<v Upol>his books, if you think about it, it's about how does

52
00:03:18.270 --> 00:03:22.859
<v Upol>how can we find flaws in the three laws of robotics

53
00:03:22.860 --> 00:03:25.229
<v Upol>that kind of he proposed, right?

54
00:03:26.400 --> 00:03:30.329
<v Upol>And in college, I was very, so I grew

55
00:03:30.330 --> 00:03:34.469
<v Upol>up in a philosophy department that had a lot of cognitive scientists in them,

56
00:03:34.470 --> 00:03:36.659
<v Upol>but who were teaching analytic philosophy.

57
00:03:37.680 --> 00:03:41.189
<v Upol>And that's where I actually got into AI. I got hooked into it.

58
00:03:41.190 --> 00:03:45.329
<v Upol>I was like, OK, and maybe initially I had more

59
00:03:45.330 --> 00:03:49.289
<v Upol>ambitious goals of creating something like AGI, so to speak.

60
00:03:49.290 --> 00:03:53.039
<v Upol>But then over time, I started getting more practical about it.

61
00:03:53.040 --> 00:03:57.299
<v Upol>And after graduating, I actually spent a lot of time doing management,

62
00:03:57.300 --> 00:03:59.729
<v Upol>consulting and then ran a startup.

63
00:03:59.730 --> 00:04:02.970
<v Upol>And in those experiences, I was dealing with

64
00:04:03.990 --> 00:04:08.159
<v Upol>AI-mediated applications, but mostly on the consumer side.

65
00:04:08.160 --> 00:04:12.239
<v Upol>So I had clients who are really using this at the enterprise level,

66
00:04:12.240 --> 00:04:16.828
<v Upol>and I was seeing how sometimes despite best intentions,

67
00:04:16.829 --> 00:04:21.059
<v Upol>the real use of these systems were suffering.

68
00:04:21.060 --> 00:04:24.449
<v Upol>So that's one way when I got into the Ph.D.

69
00:04:24.450 --> 00:04:28.379
<v Upol>journey, I started thinking of artificial intelligence,

70
00:04:28.380 --> 00:04:29.699
<v Upol>but from the human side.

71
00:04:31.200 --> 00:04:34.829
<v Andrey>Right, and this was roughly when what year?

72
00:04:34.830 --> 00:04:38.819
<v Upol>Yeah, so I had like so it was like I

73
00:04:38.820 --> 00:04:42.150
<v Upol>started the P.h.D journey roughly around 20 15/16.

74
00:04:44.280 --> 00:04:48.509
<v Upol>But the work that I had done before that was like the last four years

75
00:04:48.510 --> 00:04:51.010
<v Upol>before that, that's around like 2012 13.

76
00:04:52.230 --> 00:04:56.189
<v Upol>So that's like the industry experience very much drives a lot of my insights

77
00:04:56.190 --> 00:05:00.119
<v Upol>into the work today, especially seeing people and

78
00:05:00.120 --> 00:05:02.729
<v Upol>I do consult even now.

79
00:05:02.730 --> 00:05:07.139
<v Upol>So I'm very much in the applied setting of these research discussions,

80
00:05:07.140 --> 00:05:08.939
<v Upol>which help me kind of bridge too.

81
00:05:08.940 --> 00:05:12.989
<v Upol>That's why you'll see, even in my work, I do tend to have a more applied

82
00:05:12.990 --> 00:05:14.100
<v Upol>kind of a connotation.

83
00:05:15.150 --> 00:05:19.139
<v Andrey>Yeah, yeah. I was just wondering because I think, you know, obviously

84
00:05:19.140 --> 00:05:23.789
<v Andrey>there's been a huge boom in AI over the past decade and explainable AI,

85
00:05:23.790 --> 00:05:27.969
<v Andrey>which you know your in has been more and more

86
00:05:27.970 --> 00:05:32.509
<v Andrey>an area of study, but I think it took it a little while

87
00:05:32.510 --> 00:05:37.019
<v Andrey>it sort of is catching up in some sense as as AI is getting deployed.

88
00:05:37.020 --> 00:05:41.099
<v Andrey>Yeah. And then so you started your PhD journey in 2015, did you go

89
00:05:41.100 --> 00:05:45.239
<v Andrey>to explainable AI right right away or did it sort of did you find

90
00:05:45.240 --> 00:05:47.459
<v Andrey>your way there a bit later?

91
00:05:47.460 --> 00:05:51.599
<v Upol>That's a really great question. No, I actually started my journey doing

92
00:05:51.600 --> 00:05:55.949
<v Upol>affective computing, so I was very much interested in helping

93
00:05:55.950 --> 00:06:00.209
<v Upol>children with autism, learn about non-verbal

94
00:06:00.210 --> 00:06:04.229
<v Upol>communication to head up displays, and Google Glass was very hot

95
00:06:04.230 --> 00:06:05.939
<v Upol>back then. Oh yeah.

96
00:06:05.940 --> 00:06:09.899
<v Upol>So I was trying to develop algorithms trying to help people

97
00:06:09.900 --> 00:06:14.219
<v Upol>who had had difficulties processing social signals

98
00:06:14.220 --> 00:06:18.239
<v Upol>to use some kind of a prosthetic to kind of augment that social

99
00:06:18.240 --> 00:06:20.759
<v Upol>interaction. So that's how I actually started.

100
00:06:20.760 --> 00:06:24.539
<v Upol>And then after that, I am originally from Bangladesh.

101
00:06:24.540 --> 00:06:28.859
<v Upol>So I, the global south has been very much and is still

102
00:06:28.860 --> 00:06:32.009
<v Upol>very much a core part of my existence.

103
00:06:32.010 --> 00:06:36.629
<v Upol>So after that, I started looking at how do these technologies kind of behave

104
00:06:36.630 --> 00:06:40.869
<v Upol>in the global south, where the technology is not necessarily made

105
00:06:40.870 --> 00:06:41.870
<v Upol>in?

106
00:06:42.900 --> 00:06:46.889
<v Upol>After that, I think it was in two thousand sixteen

107
00:06:46.890 --> 00:06:51.389
<v Upol>or seventeen where DARPA had that XAI grant

108
00:06:51.390 --> 00:06:55.379
<v Upol>and that was the first time where

109
00:06:55.380 --> 00:06:59.459
<v Upol>because it's interesting, right? Like explainability of AI is not real.

110
00:06:59.460 --> 00:07:03.419
<v Upol>If you look at the literature in the 80s, there is a lot of work,

111
00:07:03.420 --> 00:07:06.929
<v Upol>in fact, that comics label and I was coined back in the 80s of the 90s.

112
00:07:08.130 --> 00:07:11.849
<v Upol>This was based on the knowledge, you know, the knowledge based systems like we

113
00:07:11.850 --> 00:07:13.769
<v Upol>had a second there.

114
00:07:13.770 --> 00:07:18.029
<v Upol>But with the advent of Deep Learning and Deep Learning becoming kind

115
00:07:18.030 --> 00:07:23.009
<v Upol>of enterprise level almost like coming of age,

116
00:07:23.010 --> 00:07:26.939
<v Upol>you see, then there is this need to hold these

117
00:07:26.940 --> 00:07:28.919
<v Upol>systems accountable.

118
00:07:28.920 --> 00:07:33.059
<v Upol>So I actually had walked into my advisor's

119
00:07:33.060 --> 00:07:37.019
<v Upol>office at that time and I was asking, you know, what

120
00:07:37.020 --> 00:07:39.479
<v Upol>kind of projects do we have to work on?

121
00:07:39.480 --> 00:07:43.979
<v Upol>And he said, And my advisor is fantastic Mark Riedl.

122
00:07:43.980 --> 00:07:48.119
<v Upol>And Mark kind of said that, hey, there is this other project

123
00:07:48.120 --> 00:07:52.259
<v Upol>that no one really has taken upon themselves because we don't really

124
00:07:52.260 --> 00:07:54.899
<v Upol>know what it would look like.

125
00:07:54.900 --> 00:07:56.949
<v Upol>And I said, What is it this explainable AI?

126
00:07:56.950 --> 00:08:01.529
<v Upol>I think until at that time, like I had not heard about the term,

127
00:08:01.530 --> 00:08:05.519
<v Upol>I was like, This sounds like interesting. And I think upon reflection,

128
00:08:05.520 --> 00:08:09.509
<v Upol>what I realized about myself is I do very well when it's an empty

129
00:08:09.510 --> 00:08:13.529
<v Upol>slate and I get to paint my own picture rather than

130
00:08:13.530 --> 00:08:17.759
<v Upol>a very well formed slate. So I was very lucky to get into

131
00:08:17.760 --> 00:08:20.279
<v Upol>that debate very early on.

132
00:08:20.280 --> 00:08:24.359
<v Upol>In the second resurgence, I would argue, because the second life XAI has

133
00:08:24.360 --> 00:08:28.529
<v Upol>had is, I think, much more longer than the first

134
00:08:28.530 --> 00:08:32.339
<v Upol>life it had because it was there and but it also wasn't there in the early

135
00:08:32.340 --> 00:08:33.340
<v Upol>1980s.

136
00:08:34.830 --> 00:08:38.999
<v Upol>So then I started looking into it.

137
00:08:39.000 --> 00:08:42.989
<v Upol>I started on the algorithmic side, frankly, and then trying to work with

138
00:08:42.990 --> 00:08:47.009
<v Upol>algorithms. And then over time, I got on my Human side and you

139
00:08:47.010 --> 00:08:51.119
<v Upol>are right. I think explainable AI is

140
00:08:51.120 --> 00:08:52.979
<v Upol>very much in flux.

141
00:08:52.980 --> 00:08:55.799
<v Upol>That's how I would talk about it.

142
00:08:55.800 --> 00:08:59.819
<v Upol>I think we as a community, we are still trying to figure out how

143
00:08:59.820 --> 00:09:03.899
<v Upol>to navigate this field, being

144
00:09:03.900 --> 00:09:07.979
<v Upol>consistent in our terminology in the way we

145
00:09:07.980 --> 00:09:12.359
<v Upol>do our work. But there is also a certain level of beauty

146
00:09:12.360 --> 00:09:16.289
<v Upol>in that. And in that case, I'm kind of drawn by

147
00:09:16.290 --> 00:09:20.219
<v Upol>the social construction of technology lenses,

148
00:09:20.220 --> 00:09:24.209
<v Upol>something pioneered by way, a biker, and he talked about

149
00:09:24.210 --> 00:09:25.889
<v Upol>relevant social groups.

150
00:09:25.890 --> 00:09:29.909
<v Upol>So in any piece of technology, you will have relevant social groups in

151
00:09:29.910 --> 00:09:33.899
<v Upol>that. Is why they was talking about bicycles, so bicycles have

152
00:09:33.900 --> 00:09:38.099
<v Upol>very other social groups, and each relevant

153
00:09:38.100 --> 00:09:42.029
<v Upol>social groups are these are stakeholders who have skin in the game actually

154
00:09:42.030 --> 00:09:45.899
<v Upol>give meaning to the technology as much as the technology gives meaning to the

155
00:09:45.900 --> 00:09:50.249
<v Upol>rights of. If you think about the mountain bikes and BMX bikes

156
00:09:50.250 --> 00:09:53.909
<v Upol>now, you know, like racing bikes on different bikes.

157
00:09:53.910 --> 00:09:55.859
<v Upol>And it's because of the stakeholders.

158
00:09:55.860 --> 00:09:59.399
<v Upol>They get very different, meaning all of them are bicycles, but they look very

159
00:09:59.400 --> 00:10:03.449
<v Upol>different. And I think within explainability we have people from the

160
00:10:03.450 --> 00:10:07.859
<v Upol>algorithmic side, basically the items from the

161
00:10:07.860 --> 00:10:12.029
<v Upol>HCI side. And now we are having stakeholders in the public policy

162
00:10:12.030 --> 00:10:15.839
<v Upol>side, in the regulation side, in the auditing side.

163
00:10:15.840 --> 00:10:19.769
<v Upol>So I think each of these stakeholders are also adding their own lenses to

164
00:10:19.770 --> 00:10:23.729
<v Upol>what is explainable and which is why you will see a lot

165
00:10:23.730 --> 00:10:24.730
<v Upol>of flux.

166
00:10:25.440 --> 00:10:29.369
<v Andrey>Yeah, it's super interesting seeing this field kind of grow,

167
00:10:29.370 --> 00:10:32.999
<v Andrey>and there's so much area to cover that.

168
00:10:33.000 --> 00:10:36.869
<v Andrey>I think, you know, maybe compared to selling computer here.

169
00:10:36.870 --> 00:10:41.249
<v Andrey>And you know, I think there's a lot more kind of maybe

170
00:10:41.250 --> 00:10:44.880
<v Andrey>foundational or at least a conceptually

171
00:10:46.650 --> 00:10:50.369
<v Andrey>important work. And then we'll get into what I think are yours and they could

172
00:10:50.370 --> 00:10:52.169
<v Andrey>be could be called that.

173
00:10:52.170 --> 00:10:55.589
<v Andrey>Yeah, your journey is really interesting. It's always fun to hear about how

174
00:10:55.590 --> 00:10:59.549
<v Andrey>people bring in their experience before repeatedly and how that

175
00:10:59.550 --> 00:11:03.329
<v Andrey>sort of guides their their direction.

176
00:11:03.330 --> 00:11:06.539
<v Andrey>In my case, I started in robotics, in high school and then, you know, I did it

177
00:11:06.540 --> 00:11:10.319
<v Andrey>in college. And then, you know, even when I went in some of the interactions

178
00:11:10.320 --> 00:11:12.299
<v Andrey>and I came back to it.

179
00:11:12.300 --> 00:11:16.199
<v Andrey>So it's always interesting to see how it happens.

180
00:11:16.200 --> 00:11:18.509
<v Upol>I love that story because it's weird, right?

181
00:11:18.510 --> 00:11:22.109
<v Upol>Because I have an undergrad, I have a like a B.S.

182
00:11:22.110 --> 00:11:24.029
<v Upol>in electrical engineering and a B.A.

183
00:11:24.030 --> 00:11:28.829
<v Upol>in philosophy, right? And I never thought I would use that philosophy degree

184
00:11:28.830 --> 00:11:33.049
<v Upol>on a daily basis as much as I use it today.

185
00:11:33.050 --> 00:11:37.159
<v Upol>In fact, my edge in explainable air actually

186
00:11:37.160 --> 00:11:42.019
<v Upol>comes from my philosophy training because I can't access

187
00:11:42.020 --> 00:11:45.499
<v Upol>the writing that is coming from the air because even as academics are part of

188
00:11:45.500 --> 00:11:49.399
<v Upol>our training is how to read a certain body of work.

189
00:11:49.400 --> 00:11:52.700
<v Upol>But then when you're also trained in computer science, you can bridge it.

190
00:11:53.870 --> 00:11:57.889
<v Upol>And I think there is something to be said there, especially for PhD student

191
00:11:57.890 --> 00:12:01.819
<v Upol>or other practitioners and researchers. Listening is I have been

192
00:12:01.820 --> 00:12:04.849
<v Upol>my mentors have always said like, you know, if you really want to make a name,

193
00:12:04.850 --> 00:12:09.799
<v Upol>pick an area and pick an Area B and then intersect them

194
00:12:09.800 --> 00:12:13.729
<v Upol>and you might actually get a C that is has a has an interesting

195
00:12:13.730 --> 00:12:18.199
<v Upol>angle to it that makes your work more relevant, more impactful.

196
00:12:18.200 --> 00:12:22.009
<v Upol>So I love also your story about robotics and how your back full circle.

197
00:12:22.010 --> 00:12:25.969
<v Upol>I think many of us in some ways are at the other end up

198
00:12:25.970 --> 00:12:28.459
<v Upol>where our interest kind of started.

199
00:12:28.460 --> 00:12:30.919
<v Andrey>Yeah, for sure. It's it's quite interesting.

200
00:12:30.920 --> 00:12:34.759
<v Andrey>You know, I worked in robotics a lot in undergrad and I worked a lot and then

201
00:12:34.760 --> 00:12:38.779
<v Andrey>were kind of classical robotic algorithms not knowing you all nets.

202
00:12:38.780 --> 00:12:42.949
<v Andrey>And then that definitely informed by understanding and my ability

203
00:12:42.950 --> 00:12:46.370
<v Andrey>to get into it. So always, always call to see how that happens.

204
00:12:47.750 --> 00:12:51.739
<v Andrey>So that's kind of my introduction to how you

205
00:12:51.740 --> 00:12:55.879
<v Andrey>got here out of a way. Let's start diving into your work

206
00:12:55.880 --> 00:13:00.259
<v Andrey>will be focusing a lot on a particular paper

207
00:13:00.260 --> 00:13:02.179
<v Andrey>that I think is very cool.

208
00:13:02.180 --> 00:13:06.799
<v Andrey>But before that, let's just give the listeners a bit of a conceptual

209
00:13:06.800 --> 00:13:11.569
<v Andrey>kind of introduction to a field, I suppose, and then you general work.

210
00:13:11.570 --> 00:13:15.619
<v Andrey>So just common basics, you

211
00:13:15.620 --> 00:13:19.679
<v Andrey>know, quick introduction can you explain what explainability

212
00:13:19.680 --> 00:13:23.989
<v Andrey>is, maybe, you know, and that's a pretty flat surface level and

213
00:13:23.990 --> 00:13:25.399
<v Andrey>why it's important.

214
00:13:25.400 --> 00:13:29.209
<v Upol>Yeah. So let's start with why it's important and then I'll share why what it is

215
00:13:29.210 --> 00:13:31.669
<v Upol>and I think the importance of drives what it is.

216
00:13:31.670 --> 00:13:35.959
<v Upol>So with with with today, like the AI powered decision

217
00:13:35.960 --> 00:13:40.459
<v Upol>making is everywhere from radiation radiologists

218
00:13:40.460 --> 00:13:44.869
<v Upol>using AI powered decision support systems to diagnose

219
00:13:44.870 --> 00:13:49.099
<v Upol>chest COVID pneumonia on chest x rays right to

220
00:13:49.100 --> 00:13:53.059
<v Upol>loan officers using algorithms to determine if you are loan worthy

221
00:13:53.060 --> 00:13:57.289
<v Upol>or not. Do you know the recidivism cases,

222
00:13:57.290 --> 00:14:01.279
<v Upol>right? So as we go on,

223
00:14:01.280 --> 00:14:05.449
<v Upol>more and more consequential decisions that we are making

224
00:14:05.450 --> 00:14:10.309
<v Upol>are either powered through AI or automated by.

225
00:14:10.310 --> 00:14:14.509
<v Upol>So this actually creates a need for AI

226
00:14:14.510 --> 00:14:16.549
<v Upol>to be held accountable.

227
00:14:16.550 --> 00:14:20.599
<v Upol>Like if something is doing something consequential, I need to

228
00:14:20.600 --> 00:14:23.629
<v Upol>be able to ask why?

229
00:14:23.630 --> 00:14:28.009
<v Upol>Hmm. And the answer to that question is where explainable

230
00:14:28.010 --> 00:14:29.010
<v Upol>AI comes in.

231
00:14:29.830 --> 00:14:33.849
<v Upol>Broadly speaking, and many people have many different definitions

232
00:14:33.850 --> 00:14:37.849
<v Upol>of it, at least the way our lab and I have conceptualized it in the

233
00:14:37.850 --> 00:14:40.479
<v Upol>years of work we have done is explainable.

234
00:14:40.480 --> 00:14:44.889
<v Upol>AI refers to the techniques, the strategies,

235
00:14:44.890 --> 00:14:49.749
<v Upol>the philosophies that can help us

236
00:14:49.750 --> 00:14:53.889
<v Upol>as stakeholders within the AI system so it could be end users,

237
00:14:53.890 --> 00:14:58.269
<v Upol>developers, data scientists understand

238
00:14:58.270 --> 00:15:02.259
<v Upol>why the the system did what

239
00:15:02.260 --> 00:15:03.260
<v Upol>it did.

240
00:15:04.090 --> 00:15:08.109
<v Speaker>And again, this is why it's also human centered in the sense that

241
00:15:08.110 --> 00:15:09.819
<v Speaker>it's not just the algorithm, right?

242
00:15:09.820 --> 00:15:13.779
<v Speaker>There's a human at the end of it trying to understand it so it

243
00:15:13.780 --> 00:15:15.819
<v Speaker>can take many forms.

244
00:15:15.820 --> 00:15:19.809
<v Speaker>Sometimes these explanations can be in the form of natural language,

245
00:15:19.810 --> 00:15:24.069
<v Speaker>plain English, for instance, explanations like textual.

246
00:15:24.070 --> 00:15:28.090
<v Speaker>Sometimes these explanations can be in the form of visualizations.

247
00:15:29.170 --> 00:15:33.609
<v Speaker>Sometimes these explanations can be in the form of data structures,

248
00:15:33.610 --> 00:15:37.569
<v Speaker>so they have the guts of a neural net where you are trying to figure out which

249
00:15:37.570 --> 00:15:39.639
<v Speaker>layer is what's important.

250
00:15:39.640 --> 00:15:43.869
<v Speaker>So these explanations and explainable AI I think the takeaway

251
00:15:43.870 --> 00:15:46.419
<v Speaker>is very pluralistic. It's not monolithic.

252
00:15:46.420 --> 00:15:49.419
<v Speaker>It's not. There's not one little thing that fits all.

253
00:15:49.420 --> 00:15:53.529
<v Speaker>But at the core of it, it's about understanding the decision

254
00:15:53.530 --> 00:15:57.699
<v Speaker>making in a way that makes sense to the user in a way that makes sense

255
00:15:57.700 --> 00:15:59.200
<v Speaker>for the person interpreting it.

256
00:16:01.700 --> 00:16:03.459
<v Andrey>That explains it. I think quite well.

257
00:16:03.460 --> 00:16:07.539
<v Andrey>And I guess it's worth noting that

258
00:16:07.540 --> 00:16:11.529
<v Andrey>this is especially difficult these days because we are working a

259
00:16:11.530 --> 00:16:13.419
<v Andrey>lot to a Deep Learning.

260
00:16:13.420 --> 00:16:17.289
<v Andrey>The way that works is you have a huge model of what awaits you.

261
00:16:17.290 --> 00:16:20.109
<v Andrey>You trained on that on a dataset. And then what you get is a phase where you

262
00:16:20.110 --> 00:16:22.802
<v Andrey>can throw in an input and get an output right, and the challenges is,

263
00:16:24.040 --> 00:16:27.729
<v Andrey>now explain why it's doing what it's doing, right?

264
00:16:27.730 --> 00:16:31.689
<v Upol>Absolutely. Yeah. And actually, now that brings to another

265
00:16:31.690 --> 00:16:35.859
<v Upol>point, you know, there are many ways and then you hear different words being

266
00:16:35.860 --> 00:16:37.619
<v Upol>kind of used.

267
00:16:37.620 --> 00:16:41.709
<v Upol>In my view, I kind of split explainability into

268
00:16:41.710 --> 00:16:45.909
<v Upol>like transparency, interpretability

269
00:16:45.910 --> 00:16:49.359
<v Upol>kind of branches and then post hoc explainability.

270
00:16:49.360 --> 00:16:51.549
<v Upol>So I'll cover all eight of these.

271
00:16:51.550 --> 00:16:55.509
<v Upol>So transparency would be almost like clear boxing it so like instead

272
00:16:55.510 --> 00:16:59.109
<v Upol>of like black boxing it could you just make the model just completely

273
00:16:59.110 --> 00:17:01.629
<v Upol>transparent, like that's just one of the ideal to

274
00:17:01.630 --> 00:17:03.969
<v Andrey>understand the model itself.

275
00:17:03.970 --> 00:17:08.139
<v Upol>Then interpretability involves, I add in my view, the able

276
00:17:08.140 --> 00:17:11.709
<v Upol>to scrutinize an algorithm. So in other words, like like a decision tree life,

277
00:17:11.710 --> 00:17:16.269
<v Upol>like the infrastructure or the architecture of forms,

278
00:17:16.270 --> 00:17:20.559
<v Upol>the fact that I can poke and prod and I can get

279
00:17:20.560 --> 00:17:25.118
<v Upol>a good understanding and I can interpret what the model is doing right.

280
00:17:25.119 --> 00:17:29.049
<v Upol>But that also requires a level of expertize like you need to have the training

281
00:17:29.050 --> 00:17:32.259
<v Upol>to interpret a decision tree. You cannot just, you know, you can't just give

282
00:17:32.260 --> 00:17:34.359
<v Upol>anyone on the street like, Hey, here's a decision tree?

283
00:17:34.360 --> 00:17:35.679
<v Upol>Interpret it, right?

284
00:17:35.680 --> 00:17:40.119
<v Speaker>So there's this level of interpretation that comes in, but the architecture

285
00:17:40.120 --> 00:17:43.389
<v Speaker>of the model should also be able to support it.

286
00:17:43.390 --> 00:17:47.469
<v Speaker>Not as you seem like deep learning algorithms are not really interpretive or by

287
00:17:47.470 --> 00:17:51.519
<v Speaker>their architecture, right? Like they're not very friendly on their side.

288
00:17:51.520 --> 00:17:55.509
<v Speaker>So recently, there has been a very big push towards what we call

289
00:17:55.510 --> 00:17:57.699
<v Speaker>post hoc explanations. Right?

290
00:17:57.700 --> 00:18:01.629
<v Speaker>So adding a layer, a model on top of the black box, so

291
00:18:01.630 --> 00:18:04.149
<v Speaker>to speak, and make it somewhat transparent.

292
00:18:04.150 --> 00:18:07.959
<v Speaker>So in other words, can I generate the explanation after the decision has been

293
00:18:07.960 --> 00:18:12.009
<v Speaker>made? So those are the three main branches you see

294
00:18:12.010 --> 00:18:15.999
<v Speaker>work within explainable AI these days, and a lot of people do

295
00:18:16.000 --> 00:18:20.559
<v Speaker>use the word explainability and interpretability interchangeably.

296
00:18:20.560 --> 00:18:24.609
<v Speaker>I don't. I tend to see explainability as a larger umbrella

297
00:18:25.690 --> 00:18:30.099
<v Speaker>that can house, but doesn't mean I'm right to be honest, like it's being

298
00:18:30.100 --> 00:18:32.500
<v Speaker>very precise about what you're saying when you're saying it.

299
00:18:33.520 --> 00:18:37.499
<v Speaker>Does that help like kind of give the demarcation of the landscape as well

300
00:18:37.500 --> 00:18:38.739
<v Speaker>the area in the work?

301
00:18:38.740 --> 00:18:42.669
<v Andrey>Yeah, yeah. Of course it's it's interesting that at least you can think of it

302
00:18:42.670 --> 00:18:46.780
<v Andrey>in these different dimensions, and I think that also helps understand sort of

303
00:18:47.870 --> 00:18:50.289
<v Andrey>the ways you might approach it.

304
00:18:50.290 --> 00:18:54.879
<v Andrey>And speaking of that, as you introduced in the intro,

305
00:18:54.880 --> 00:18:59.229
<v Andrey>your work focuses in particular on human centered XAI,

306
00:18:59.230 --> 00:19:03.809
<v Andrey>which is in some ways in contrast to algorithm centered XAI.

307
00:19:03.810 --> 00:19:08.229
<v Andrey>So what is human centered XAI, in your view?

308
00:19:08.230 --> 00:19:10.239
<v Andrey>Again, as kind of a surface level?

309
00:19:10.240 --> 00:19:14.279
<v Upol>Yeah, it's about, I guess, the way to kind of think about

310
00:19:14.280 --> 00:19:18.219
<v Upol>Incentive XAI is the following like, there is a myth often in explainable AI,

311
00:19:18.220 --> 00:19:22.239
<v Upol>where we tend to think that if we could just open

312
00:19:22.240 --> 00:19:25.179
<v Upol>the black box, everything will be fine.

313
00:19:25.180 --> 00:19:29.299
<v Upol>Right? And my my my response to the myth is also.

314
00:19:29.300 --> 00:19:33.259
<v Upol>And not everything that matters actually is inside

315
00:19:33.260 --> 00:19:35.209
<v Upol>the box. Why?

316
00:19:35.210 --> 00:19:39.229
<v Upol>Because humans don't live inside the black box office, they're outside

317
00:19:39.230 --> 00:19:40.909
<v Upol>and around it.

318
00:19:40.910 --> 00:19:45.139
<v Upol>And given, you know, humans are

319
00:19:45.140 --> 00:19:48.259
<v Upol>so instrumental in this ecosystem, right?

320
00:19:50.030 --> 00:19:54.109
<v Upol>It might not be a bad idea to start looking around the box

321
00:19:54.110 --> 00:19:56.539
<v Upol>to understand what are these value systems?

322
00:19:56.540 --> 00:20:00.619
<v Upol>What are people's ways of thinking that can ultimately

323
00:20:00.620 --> 00:20:04.849
<v Upol>aid that understanding ability that is so instrumental, explainable

324
00:20:04.850 --> 00:20:07.369
<v Upol>and so human centered, explainable AI?

325
00:20:07.370 --> 00:20:11.299
<v Upol>What it does is it fundamentally shifts the attention,

326
00:20:11.300 --> 00:20:15.699
<v Upol>and it doesn't say that algorithm centered work is bad by any means.

327
00:20:15.700 --> 00:20:19.669
<v Upol>It's not saying that what we're saying is we need to put just

328
00:20:19.670 --> 00:20:23.929
<v Upol>as much attention on the human on who

329
00:20:23.930 --> 00:20:27.889
<v Upol>is opening the box as much as opening the box.

330
00:20:27.890 --> 00:20:31.849
<v Andrey>Right? Do you need to sort of pay attention, care about

331
00:20:31.850 --> 00:20:36.049
<v Andrey>the human aspect and not just think about the model, And then,

332
00:20:36.050 --> 00:20:40.129
<v Andrey>you know, maybe we humans can take what you develop a model

333
00:20:40.130 --> 00:20:41.839
<v Andrey>later and they can figure it out.

334
00:20:41.840 --> 00:20:46.279
<v Andrey>That makes a lot of sense, and you have a great motivating example

335
00:20:46.280 --> 00:20:50.299
<v Andrey>of this in your Gradient article having to do

336
00:20:50.300 --> 00:20:54.589
<v Andrey>with this Fire Wall management thing and why human centered

337
00:20:54.590 --> 00:20:58.279
<v Andrey>aspect was necessary. So, yeah, I find that very cool.

338
00:20:58.280 --> 00:21:00.079
<v Andrey>Can you go ahead?

339
00:21:00.080 --> 00:21:04.009
<v Upol>Yeah. So this was a this was a consulting project,

340
00:21:04.010 --> 00:21:07.609
<v Upol>but I had the privilege of kind of helping out with.

341
00:21:07.610 --> 00:21:11.539
<v Upol>They had a cybersecurity company, had hired me to address

342
00:21:11.540 --> 00:21:15.519
<v Upol>a very interesting issue of this firewall management system.

343
00:21:15.520 --> 00:21:19.789
<v Upol>And in that environment, one thing that happens is the problem

344
00:21:19.790 --> 00:21:22.279
<v Upol>was that bloat. So what is it? Bloat?

345
00:21:22.280 --> 00:21:26.299
<v Upol>Bloat is what happens when people open course on a firewall and forget

346
00:21:26.300 --> 00:21:30.859
<v Upol>to close them. So over time, you get a bunch of stuff

347
00:21:30.860 --> 00:21:35.179
<v Upol>that is open. But then what happens is at an enterprise scale,

348
00:21:35.180 --> 00:21:39.259
<v Upol>there is so many open course that is humanly impossible to go to every

349
00:21:39.260 --> 00:21:41.589
<v Upol>one of them and check. Oh, wow.

350
00:21:41.590 --> 00:21:45.799
<v Upol>Right. So they had a system that would analyze all these ports

351
00:21:45.800 --> 00:21:49.969
<v Upol>and suggest which ones do remain closed versus which ones do remain

352
00:21:49.970 --> 00:21:53.929
<v Upol>open. The problem was the problem here was

353
00:21:53.930 --> 00:21:55.729
<v Upol>rather tricky.

354
00:21:55.730 --> 00:21:59.719
<v Upol>The system was actually performing rather well around the 90 percent

355
00:21:59.720 --> 00:22:04.039
<v Upol>accuracy. It had really good algorithmic transparency.

356
00:22:04.040 --> 00:22:08.109
<v Upol>But the problem was, less than two percent of

357
00:22:08.110 --> 00:22:12.399
<v Upol>the workforce was actually engaging with it and using

358
00:22:12.400 --> 00:22:13.400
<v Upol>it.

359
00:22:14.090 --> 00:22:17.059
<v Andrey>Yeah, and that's not what you want.

360
00:22:17.060 --> 00:22:18.979
<v Andrey>Yeah, and then what was that?

361
00:22:18.980 --> 00:22:19.699
<v Andrey>Yeah.

362
00:22:19.700 --> 00:22:24.229
<v Upol>So and you know, I was brought in with the task of

363
00:22:24.230 --> 00:22:27.799
<v Upol>fixing this and the assumption was still back then and this was before we kind

364
00:22:27.800 --> 00:22:29.659
<v Upol>of coined the term Human-Centered XAI.

365
00:22:29.660 --> 00:22:33.709
<v Upol>And this is the project that actually drives a lot of that thinking.

366
00:22:33.710 --> 00:22:37.339
<v Upol>And the assumption was, you know, maybe the solution is within the algorithm,

367
00:22:37.340 --> 00:22:41.539
<v Upol>just fix the algorithm, maybe make it explain better, maybe open

368
00:22:41.540 --> 00:22:43.369
<v Upol>the box differently, so to speak.

369
00:22:44.900 --> 00:22:48.949
<v Upol>And what I found at the end of the day just to give a cut the

370
00:22:48.950 --> 00:22:52.909
<v Upol>long story short, I guess, is, there

371
00:22:52.910 --> 00:22:56.509
<v Upol>was nothing that was wrong with the algorithm.

372
00:22:56.510 --> 00:23:00.559
<v Upol>The explainability that this company was looking for

373
00:23:00.560 --> 00:23:04.579
<v Upol>was at the intersection of the human and the machine not included

374
00:23:04.580 --> 00:23:05.580
<v Upol>in the machine.

375
00:23:06.290 --> 00:23:10.789
<v Upol>So what we found in this project presumption was still that

376
00:23:10.790 --> 00:23:12.509
<v Upol>something must be wrong with the algorithm.

377
00:23:12.510 --> 00:23:15.679
<v Upol>This was before we had coined the term Human-Centered XAI.

378
00:23:15.680 --> 00:23:19.490
<v Upol>A lot of the work here actually drove the philosophy behind it.

379
00:23:20.630 --> 00:23:24.619
<v Upol>And one thing that that came up was nothing was

380
00:23:24.620 --> 00:23:28.879
<v Upol>actually like we couldn't do much at the algorithmic level

381
00:23:28.880 --> 00:23:33.199
<v Upol>that helped the explainability of the system, the changes

382
00:23:33.200 --> 00:23:37.279
<v Upol>that had to be done, which actually I think we'll get into when we discuss the

383
00:23:37.280 --> 00:23:41.689
<v Upol>expanding explainability paper is at the social

384
00:23:41.690 --> 00:23:42.469
<v Upol>level.

385
00:23:42.470 --> 00:23:47.359
<v Upol>So what was the problem here was people had no idea

386
00:23:47.360 --> 00:23:51.649
<v Upol>how to calibrate their trust on this system

387
00:23:51.650 --> 00:23:55.669
<v Upol>that without really understanding how others

388
00:23:55.670 --> 00:23:58.129
<v Upol>are also interacting with the system.

389
00:23:58.130 --> 00:24:01.699
<v Upol>Right. So for instance, if I'm faced with a new system and there is no notion

390
00:24:01.700 --> 00:24:03.949
<v Upol>of the ground truth, right?

391
00:24:03.950 --> 00:24:08.029
<v Upol>And the easiest example to share here was there was a young analyst

392
00:24:08.030 --> 00:24:11.810
<v Upol>and I'm using pseudonyms like Julie and Julie

393
00:24:13.010 --> 00:24:17.029
<v Upol>had a recommendation from the AI system and to close

394
00:24:17.030 --> 00:24:20.890
<v Upol>a few ports. And on paper, the recommendation was not wrong.

395
00:24:22.070 --> 00:24:25.999
<v Upol>I have suggested that, hey, you know, if you close these ports because

396
00:24:26.000 --> 00:24:28.069
<v Upol>they have been open for a long time, they have not been used.

397
00:24:28.070 --> 00:24:31.279
<v Upol>So technically, these are not bad suggestions.

398
00:24:31.280 --> 00:24:35.959
<v Upol>Julie, not knowing a lot of the institutional history and how things are done

399
00:24:35.960 --> 00:24:37.640
<v Upol>accepted this decision.

400
00:24:38.700 --> 00:24:41.849
<v Upol>Two weeks later, the company faced a breach.

401
00:24:43.240 --> 00:24:46.200
<v Upol>And then lost around $2 billion in one.

402
00:24:47.810 --> 00:24:52.939
<v Upol>What had happened was Julie had accidentally closed

403
00:24:52.940 --> 00:24:56.419
<v Upol>following the A's recommendation, the backup center reports.

404
00:24:56.420 --> 00:25:00.349
<v Upol>Right. So because their backups are reports, of course, it's good that they

405
00:25:00.350 --> 00:25:04.219
<v Upol>have not been used, right? It is also good that they're open.

406
00:25:04.220 --> 00:25:08.359
<v Upol>So this kind of highlights a very interesting tension here that

407
00:25:08.360 --> 00:25:12.559
<v Upol>even though the air system was technically not right, Julie

408
00:25:12.560 --> 00:25:14.299
<v Upol>actually got fired.

409
00:25:14.300 --> 00:25:16.169
<v Upol>Oh, that's that's a shame.

410
00:25:16.170 --> 00:25:20.659
<v Upol>Yeah, yeah. So the accountability is squarely light on the human user,

411
00:25:20.660 --> 00:25:23.989
<v Upol>even though the human user in this case, they are not data scientist, either

412
00:25:23.990 --> 00:25:28.639
<v Upol>cybersecurity analysts, they shouldn't have to know how this guy is working.

413
00:25:28.640 --> 00:25:33.739
<v Upol>So it's very hard in real world situations to answer the following question

414
00:25:33.740 --> 00:25:36.529
<v Upol>one does this AI not know,

415
00:25:38.240 --> 00:25:42.379
<v Upol>right? And to address that question is almost an unknown,

416
00:25:42.380 --> 00:25:43.380
<v Upol>unknown, right?

417
00:25:44.210 --> 00:25:48.649
<v Upol>You need and in this case, in this case study, they needed this thing.

418
00:25:48.650 --> 00:25:52.699
<v Upol>What the socio organizational context to help them understand

419
00:25:52.700 --> 00:25:56.929
<v Upol>how are other people dealing with it and and watching how others are acting

420
00:25:56.930 --> 00:26:01.099
<v Upol>with it, they were able to develop more robust mental models of how

421
00:26:01.100 --> 00:26:03.559
<v Upol>to calibrate that trust on the system.

422
00:26:03.560 --> 00:26:08.179
<v Upol>In other words, which are the situations that I want to see really well

423
00:26:08.180 --> 00:26:11.599
<v Upol>and which are the situations that AI does not perform really well because even

424
00:26:11.600 --> 00:26:15.619
<v Upol>if the performance is not uniform, that's the other reality in these real

425
00:26:15.620 --> 00:26:17.179
<v Upol>world systems.

426
00:26:17.180 --> 00:26:21.589
<v Upol>So that's just, you know, just a quick summarization of this out of that

427
00:26:21.590 --> 00:26:25.909
<v Upol>case study, which kind of showed me that there were elements outside

428
00:26:25.910 --> 00:26:30.109
<v Upol>the black box that we really needed to incorporate in the decision

429
00:26:30.110 --> 00:26:34.159
<v Upol>making to help decision makers do it

430
00:26:34.160 --> 00:26:38.179
<v Upol>right and to make sure accountability was shared rather

431
00:26:38.180 --> 00:26:42.109
<v Upol>than be inappropriately placed all on the human and

432
00:26:42.110 --> 00:26:43.879
<v Upol>nothing on the machine.

433
00:26:43.880 --> 00:26:48.019
<v Andrey>Yeah, yeah, it's interesting. I think a lot of listeners might now

434
00:26:48.020 --> 00:26:52.359
<v Andrey>appreciate the importance of this kind of work in terms of,

435
00:26:52.360 --> 00:26:56.599
<v Andrey>you know, outcome come here. And I think we'll dig in a bit more into

436
00:26:56.600 --> 00:27:00.539
<v Andrey>where you want are in terms of how you do it, which was

437
00:27:00.540 --> 00:27:01.540
<v Andrey>really interesting.

438
00:27:02.390 --> 00:27:06.679
<v Andrey>Now, with a lot of these concepts laid out before we get into

439
00:27:06.680 --> 00:27:10.789
<v Andrey>kind of our main focus, I thought it'd be fun to walk through kind

440
00:27:10.790 --> 00:27:15.179
<v Andrey>of your journey in some sense of

441
00:27:15.180 --> 00:27:19.219
<v Andrey>your trajectory, starting out less human centered and then

442
00:27:19.220 --> 00:27:23.209
<v Andrey>sort of discovering that and more and more coming closer to where you

443
00:27:23.210 --> 00:27:27.309
<v Andrey>are now. So first, you had kind of,

444
00:27:27.310 --> 00:27:32.239
<v Andrey>let's say, a more traditional maybe XAI called rationalization

445
00:27:32.240 --> 00:27:37.009
<v Andrey>and neural machine translation approach to generating natural language

446
00:27:37.010 --> 00:27:41.059
<v Andrey>explanations. So just in brief, you know, what was this

447
00:27:41.060 --> 00:27:45.289
<v Andrey>paper and sort of what was the contribution

448
00:27:45.290 --> 00:27:46.039
<v Andrey>there?

449
00:27:46.040 --> 00:27:49.759
<v Upol>No, thank you for asking that. I think this is the phase in my dissertation

450
00:27:49.760 --> 00:27:51.829
<v Upol>that I call turn to the machine.

451
00:27:51.830 --> 00:27:56.809
<v Upol>Mm hmm. I've kind of takes a few turns in this turn to the machine

452
00:27:56.810 --> 00:27:59.149
<v Upol>mark and I kind of end Brandt.

453
00:27:59.150 --> 00:28:01.939
<v Upol>So I just when I thought on my coauthors like Brant Harrison, who is at the

454
00:28:01.940 --> 00:28:05.299
<v Upol>University of Kentucky, Marl Riedl, obviously his tech

455
00:28:06.470 --> 00:28:11.149
<v Upol>and per now is also now I think is a PhD Student at Georgia Tech and

456
00:28:11.150 --> 00:28:14.359
<v Upol>Larry Chen, who is now graduated from Georgia Tech.

457
00:28:14.360 --> 00:28:17.390
<v Upol>We kind of started thinking that, you know,

458
00:28:18.530 --> 00:28:22.459
<v Upol>wouldn't it be nice if the AI system

459
00:28:22.460 --> 00:28:25.789
<v Upol>talk to you or thought out loud in plain English?

460
00:28:27.140 --> 00:28:32.359
<v Upol>And the reason why we kind of thought about that was, Hey, I'm

461
00:28:32.360 --> 00:28:37.159
<v Upol>not everyone has the background to interpret models, right?

462
00:28:37.160 --> 00:28:41.179
<v Upol>And our a lot of our end users are not AI experts,

463
00:28:41.180 --> 00:28:45.349
<v Upol>but everyone, if they can speak and read and write in English, could understand

464
00:28:45.350 --> 00:28:48.499
<v Upol>English, right? In fact, that's how we even communicate.

465
00:28:48.500 --> 00:28:52.819
<v Upol>So I in this paper actually do a lot of inspiration

466
00:28:52.820 --> 00:28:57.199
<v Upol>from philosophy of language, namely the work of Jerry Fodor

467
00:28:58.430 --> 00:29:02.089
<v Upol>to kind of and work with Brant to kind of develop the algorithmic

468
00:29:02.090 --> 00:29:05.929
<v Upol>infrastructure to answer the following question.

469
00:29:05.930 --> 00:29:10.129
<v Upol>And then this is the question that is asked me in this paper Can

470
00:29:10.130 --> 00:29:15.169
<v Upol>we? This is almost like an existence proof, like can we generate rationales

471
00:29:15.170 --> 00:29:18.829
<v Upol>from using a neural machine translation approach?

472
00:29:18.830 --> 00:29:20.809
<v Upol>And this was the first one.

473
00:29:20.810 --> 00:29:24.859
<v Upol>Yeah, to our knowledge, that uses an NMT mechanism.

474
00:29:24.860 --> 00:29:29.119
<v Upol>Instead of translating from like English to Bengali,

475
00:29:29.120 --> 00:29:33.199
<v Upol>like natural language to natural language, we we felt what if we replace

476
00:29:33.200 --> 00:29:35.659
<v Upol>one of the natural languages with some data structures?

477
00:29:35.660 --> 00:29:38.659
<v Upol>Right, right. And that's the insight in this case.

478
00:29:38.660 --> 00:29:42.679
<v Upol>And the innovation was we were able to back in the day, like when this paper

479
00:29:42.680 --> 00:29:46.999
<v Upol>was published back in 2017 18, there was a lot of work going on automated

480
00:29:47.000 --> 00:29:51.529
<v Upol>image. Captioning and stuff like that, but very little work was done

481
00:29:51.530 --> 00:29:54.679
<v Upol>on sequential decision making, right?

482
00:29:54.680 --> 00:29:58.099
<v Upol>So like, you know, if you can think of robotics, right, like getting a robot

483
00:29:58.100 --> 00:30:02.089
<v Upol>from one point in the kitchen to be in the kitchen is

484
00:30:02.090 --> 00:30:04.069
<v Upol>a sequential decision making task.

485
00:30:04.070 --> 00:30:08.269
<v Upol>So we actually took a sequential decision making environment and need

486
00:30:08.270 --> 00:30:12.349
<v Upol>an agent navigate it while being

487
00:30:12.350 --> 00:30:15.019
<v Upol>able to think out loud in plain English.

488
00:30:15.020 --> 00:30:18.929
<v Andrey>If I remember correctly, this was like the game frogger?

489
00:30:18.930 --> 00:30:20.749
<v Upol>Yes, yes. Yes.

490
00:30:20.750 --> 00:30:24.859
<v Upol>So that that was an homage to a lot of the game work that goes at

491
00:30:24.860 --> 00:30:28.669
<v Upol>the entertainment intelligent and Human-Centered AI Lab at Georgia Tech.

492
00:30:28.670 --> 00:30:32.719
<v Upol>So we kind of leveraged a lot of our game AI history, which I know,

493
00:30:32.720 --> 00:30:35.779
<v Upol>you know, I know you were at Georgia Tech for undergrad, so I think you might

494
00:30:35.780 --> 00:30:37.729
<v Upol>also be familiar with a bit of that.

495
00:30:37.730 --> 00:30:41.689
<v Andrey>Oh, yeah, yeah, yeah. And yeah, Froger is a fine example because it's

496
00:30:41.690 --> 00:30:43.479
<v Andrey>pretty intuitive, right?

497
00:30:43.480 --> 00:30:48.049
<v Andrey>You know, why do you want to jump forward well as a car racing towards

498
00:30:48.050 --> 00:30:49.050
<v Andrey>me so I wanna avoid it.

499
00:30:50.420 --> 00:30:54.199
<v Andrey>Yeah, but that was a cool start and certainly interesting.

500
00:30:54.200 --> 00:30:58.399
<v Andrey>But since when you have moved more towards the human-centered

501
00:30:58.400 --> 00:31:02.419
<v Andrey>aspect, so that's going to the next step, I suppose

502
00:31:02.420 --> 00:31:06.679
<v Andrey>return to the human, which I think started with this other paper

503
00:31:06.680 --> 00:31:10.729
<v Andrey>automated rationale generation kind of extending this, but then

504
00:31:10.730 --> 00:31:14.150
<v Andrey>a technique for explainable AI and its effects on human perception.

505
00:31:15.650 --> 00:31:17.929
<v Andrey>So how did that come about?

506
00:31:17.930 --> 00:31:21.979
<v Upol>So, yeah, so this one, so after we ask the question, can we

507
00:31:21.980 --> 00:31:24.289
<v Upol>generate? And the answer was yes.

508
00:31:24.290 --> 00:31:26.899
<v Upol>Now we ask the question, OK.

509
00:31:26.900 --> 00:31:30.689
<v Upol>These generated rationales, are they any good, right?

510
00:31:30.690 --> 00:31:34.729
<v Upol>Like because back then, if you think about how we used to evaluate

511
00:31:34.730 --> 00:31:39.229
<v Upol>these generative systems, you know, blue score or other procedural

512
00:31:39.230 --> 00:31:43.309
<v Upol>techniques are good, but we don't really get a sense of how

513
00:31:43.310 --> 00:31:45.139
<v Upol>good they are to human beings.

514
00:31:45.140 --> 00:31:48.979
<v Upol>Right? Like, do people actually find these plausible?

515
00:31:48.980 --> 00:31:53.479
<v Upol>So in this paper, ours are like kind of starts to turn to the human.

516
00:31:53.480 --> 00:31:57.559
<v Upol>We presented the first work that gave

517
00:31:57.560 --> 00:32:01.969
<v Upol>a robust human centered use our study along certain dimensions

518
00:32:01.970 --> 00:32:06.079
<v Upol>of user perceptions to evaluate these

519
00:32:06.080 --> 00:32:08.389
<v Upol>rationale generating systems.

520
00:32:08.390 --> 00:32:12.499
<v Upol>And what we found was that we bridged a lot of work.

521
00:32:12.500 --> 00:32:17.209
<v Upol>So I took all of these measures and adapted it from work in HCI

522
00:32:17.210 --> 00:32:21.349
<v Upol>human robot interaction, as well as the technology acceptance models

523
00:32:21.350 --> 00:32:24.890
<v Upol>from back in the 90s when automation was becoming hot.

524
00:32:25.970 --> 00:32:30.019
<v Upol>And we found fascinating things around, not just the fact that these

525
00:32:30.020 --> 00:32:34.369
<v Upol>were plausible in this paper. We just didn't make the Frogger kind of say

526
00:32:34.370 --> 00:32:38.539
<v Upol>things. In one way, we were able to tweak the network in a way

527
00:32:38.540 --> 00:32:42.589
<v Upol>that I could make Frogger talk more in detail

528
00:32:42.590 --> 00:32:46.400
<v Upol>versus, say things more shortly in its rationales.

529
00:32:47.540 --> 00:32:52.939
<v Upol>And we found that the level of detail also had a lot of interesting

530
00:32:52.940 --> 00:32:58.039
<v Upol>interweaving effects on people's trust, people's confidence,

531
00:32:58.040 --> 00:33:02.119
<v Upol>how tolerant, where they when the robots like that further failed,

532
00:33:02.120 --> 00:33:05.479
<v Upol>right? So this was a really interesting deep dive.

533
00:33:05.480 --> 00:33:09.229
<v Upol>And we just not only did the quality quantitative part, we did a really good

534
00:33:09.230 --> 00:33:11.509
<v Upol>qualitative part as well.

535
00:33:11.510 --> 00:33:13.369
<v Upol>These are the crowd workers.

536
00:33:13.370 --> 00:33:17.419
<v Upol>And, you know, getting Amazon Mechanical Turk first to

537
00:33:17.420 --> 00:33:21.499
<v Upol>take a forty five minute task is not easy, I

538
00:33:21.500 --> 00:33:24.829
<v Upol>think. And so we were liking the methodology part.

539
00:33:24.830 --> 00:33:28.460
<v Upol>I think we were very happy with it, and I'm so proud of the team that did it.

540
00:33:30.160 --> 00:33:34.189
<v Upol>They were saying Han and another research assistant were undergrads

541
00:33:34.190 --> 00:33:38.209
<v Upol>at Georgia Tech who helped us create a really good data collection

542
00:33:38.210 --> 00:33:42.259
<v Upol>pipeline that helped us collect these rationales to train.

543
00:33:42.260 --> 00:33:44.719
<v Upol>And then we not only train, but we also tested it.

544
00:33:44.720 --> 00:33:48.679
<v Upol>So that was the end to end kind of application of this that really

545
00:33:48.680 --> 00:33:50.904
<v Upol>made the paper one of my favorite papers that I've written.

546
00:33:52.550 --> 00:33:56.059
<v Andrey>Yeah, is this reminds me a little bit of

547
00:33:57.530 --> 00:34:01.099
<v Andrey>the whole like. Some field of social robotics is quite interesting because

548
00:34:01.100 --> 00:34:05.149
<v Andrey>again, there's a lot to do with human perceptions and like,

549
00:34:05.150 --> 00:34:09.169
<v Andrey>how do you communicate intent of grasping a couch in a way that

550
00:34:09.170 --> 00:34:13.428
<v Andrey>you know, people can understand? Or how do you appear

551
00:34:13.429 --> 00:34:17.718
<v Andrey>friendly and so on? That's its own whole thing, and it's always interesting

552
00:34:17.719 --> 00:34:21.859
<v Andrey>to see that, you know, aside from all of the social models,

553
00:34:21.860 --> 00:34:26.629
<v Andrey>if you need air during the real world, this is also a big challenge

554
00:34:26.630 --> 00:34:28.019
<v Andrey>indeed.

555
00:34:28.020 --> 00:34:32.249
<v Upol>Yes, so in this one, one aspect that

556
00:34:32.250 --> 00:34:36.448
<v Upol>differs from the other white we'll we'll get to soon

557
00:34:36.449 --> 00:34:40.678
<v Upol>is here, you sort of are still dealing with one to one interaction

558
00:34:40.679 --> 00:34:44.939
<v Upol>versus playing game and then the agent is kind of trying

559
00:34:44.940 --> 00:34:47.339
<v Upol>to make it clear what's going on.

560
00:34:47.340 --> 00:34:51.419
<v Upol>And you already mentioned in your example that you know you need and

561
00:34:51.420 --> 00:34:55.379
<v Upol>many real war situations to go beyond that, you need organizational context.

562
00:34:55.380 --> 00:34:58.619
<v Upol>You need to understand groups of people, so to speak.

563
00:34:58.620 --> 00:35:02.969
<v Upol>And that takes us to the concept of

564
00:35:02.970 --> 00:35:04.679
<v Upol>socio technical challenges.

565
00:35:06.100 --> 00:35:10.259
<v Upol>Yes. So how did you make that turn

566
00:35:10.260 --> 00:35:14.219
<v Upol>and what is that compared to this one to one

567
00:35:14.220 --> 00:35:15.239
<v Upol>paradigm?

568
00:35:15.240 --> 00:35:19.199
<v Upol>Absolutely. So you hit the nail on the head, right?

569
00:35:19.200 --> 00:35:23.129
<v Upol>There is like a lot of the way we were thinking about the rational generation

570
00:35:23.130 --> 00:35:26.789
<v Upol>or the interaction paradigm was very much one to one.

571
00:35:26.790 --> 00:35:30.989
<v Upol>And, you know, based on my prior work in industry settings, I started

572
00:35:30.990 --> 00:35:34.530
<v Upol>realizing that is that truly representative of what happens.

573
00:35:35.700 --> 00:35:39.749
<v Upol>And I started realizing that, no, we need to think more about like

574
00:35:39.750 --> 00:35:43.199
<v Upol>these AI systems. I'm never in a vacuum.

575
00:35:43.200 --> 00:35:47.399
<v Upol>They're often situated in larger organizational environments.

576
00:35:47.400 --> 00:35:50.219
<v Upol>So in that case, how do we think about this?

577
00:35:50.220 --> 00:35:52.409
<v Upol>How do we conceptualize this?

578
00:35:52.410 --> 00:35:56.369
<v Upol>So this kind of forced us, and this is probably the first kind of

579
00:35:56.370 --> 00:36:00.569
<v Upol>conceptual paper that I have written is to kind of outline.

580
00:36:00.570 --> 00:36:04.569
<v Upol>So we kind of coined the term human centered essay, but we also wanted to

581
00:36:04.570 --> 00:36:07.349
<v Upol>seen how do you operationalize this thing?

582
00:36:07.350 --> 00:36:11.459
<v Upol>So we bridged theories from critical AI studies like

583
00:36:11.460 --> 00:36:16.169
<v Upol>critical technical practice in HCI, like reflective design

584
00:36:16.170 --> 00:36:18.509
<v Upol>and value sensitive design.

585
00:36:18.510 --> 00:36:22.679
<v Upol>And we kind of talked a little bit about, OK, now we have this insight

586
00:36:22.680 --> 00:36:27.269
<v Upol>that we have to not just care about one person, but also multiple stakeholders

587
00:36:27.270 --> 00:36:31.289
<v Upol>in the system. So going back to the cybersecurity example, right, it's not

588
00:36:31.290 --> 00:36:35.009
<v Upol>just the analyst who is making the decision, it's also the decision of the

589
00:36:35.010 --> 00:36:38.969
<v Upol>analysts previous who had made similar decisions in the

590
00:36:38.970 --> 00:36:43.649
<v Upol>past. So that kind of forced us to kind of imagine and envision

591
00:36:43.650 --> 00:36:47.759
<v Upol>AI explainable AI paradigm that is more human centered and not just one

592
00:36:47.760 --> 00:36:51.030
<v Upol>human, but also incorporates many humans.

593
00:36:52.240 --> 00:36:56.109
<v Andrey>Mm hmm. Yeah, so there comes a socio technical aspect.

594
00:36:56.110 --> 00:37:00.129
<v Andrey>You know, social being, you know, interactions between

595
00:37:00.130 --> 00:37:02.799
<v Andrey>people and even organizations.

596
00:37:02.800 --> 00:37:07.119
<v Andrey>So where you marry sort of the groups of people with the technical

597
00:37:07.120 --> 00:37:12.009
<v Andrey>problem, which now you really need to think about both.

598
00:37:12.010 --> 00:37:15.999
<v Andrey>And that as far was

599
00:37:16.000 --> 00:37:20.229
<v Andrey>sort of kind of new direction that wasn't

600
00:37:20.230 --> 00:37:23.679
<v Andrey>really the norm or stylish in the field.

601
00:37:23.680 --> 00:37:26.079
<v Upol>Yeah. And I think that's a very important point.

602
00:37:26.080 --> 00:37:30.579
<v Upol>In this case. I had drawn a lot of inspiration

603
00:37:30.580 --> 00:37:35.229
<v Upol>from the fact literature of the fairness, accountability and transparency

604
00:37:35.230 --> 00:37:39.129
<v Upol>literature where they were very much at that time thinking very socially or

605
00:37:39.130 --> 00:37:43.119
<v Upol>technically. And I am always reminded of I watched this video

606
00:37:43.120 --> 00:37:45.849
<v Upol>from Microsoft Research is like responsible.

607
00:37:45.850 --> 00:37:48.579
<v Upol>I kind of in visions.

608
00:37:48.580 --> 00:37:52.569
<v Upol>And Hannah Wallach, who is at Amazon New York, had

609
00:37:52.570 --> 00:37:55.149
<v Upol>this fascinating line that I cannot like repeat verbatim.

610
00:37:55.150 --> 00:37:58.119
<v Upol>But the version that I remember is today.

611
00:37:58.120 --> 00:38:02.049
<v Upol>Our systems are AI systems are embedded

612
00:38:02.050 --> 00:38:05.679
<v Upol>in very complex social environments.

613
00:38:05.680 --> 00:38:09.759
<v Upol>So that means out the effects that these technical systems

614
00:38:09.760 --> 00:38:12.309
<v Upol>have our social.

615
00:38:12.310 --> 00:38:16.299
<v Upol>So that means that fundamentally socio technical in nature, in terms

616
00:38:16.300 --> 00:38:18.699
<v Upol>of their complexities as well as that impacts.

617
00:38:18.700 --> 00:38:23.229
<v Upol>So when we keep that in mind, I started asking myself,

618
00:38:23.230 --> 00:38:27.879
<v Upol>how can we get a good idea about explainable

619
00:38:27.880 --> 00:38:32.619
<v Upol>AI if we do not take a socio technical perspective

620
00:38:32.620 --> 00:38:36.669
<v Upol>given, you know, in the real world, that's how these systems are.

621
00:38:36.670 --> 00:38:40.899
<v Upol>So that's actually a lot of the things that drove these socio technical lens,

622
00:38:40.900 --> 00:38:44.859
<v Upol>so to speak. And you are right, like this was the first paper, to our

623
00:38:44.860 --> 00:38:48.969
<v Upol>knowledge, to kind of highlight that explicitly in the context

624
00:38:48.970 --> 00:38:49.919
<v Upol>of explainable.

625
00:38:49.920 --> 00:38:52.569
<v Andrey>I yeah, I find it interesting.

626
00:38:52.570 --> 00:38:56.559
<v Andrey>I think it it seems like it would be easy to not have this

627
00:38:56.560 --> 00:39:00.879
<v Andrey>realization if you come from a traditional sort of AI

628
00:39:00.880 --> 00:39:05.259
<v Andrey>computer science research background where you just jump into a Ph.D., you know

629
00:39:05.260 --> 00:39:09.429
<v Andrey>where you work, in your office, in the computer science building,

630
00:39:09.430 --> 00:39:13.119
<v Andrey>you know, doing your research. And it's easy to forget sort of about the

631
00:39:13.120 --> 00:39:17.379
<v Andrey>outside world. So I think it's interesting also that having

632
00:39:17.380 --> 00:39:21.339
<v Andrey>had all this background outside working in actual

633
00:39:21.340 --> 00:39:24.159
<v Andrey>organizations, I think I would imagine that also

634
00:39:25.660 --> 00:39:28.449
<v Andrey>made it easier for you to get here.

635
00:39:28.450 --> 00:39:31.989
<v Upol>Yeah, it was. And it's humbling, right? Because you fail so many times trying

636
00:39:31.990 --> 00:39:34.569
<v Upol>to do this, and that's the only way sometimes we learn.

637
00:39:34.570 --> 00:39:38.499
<v Upol>Right? I, you know, my consulting projects, I never like linear or

638
00:39:38.500 --> 00:39:41.619
<v Upol>straightforward because they often reach out to me when problems are so

639
00:39:41.620 --> 00:39:44.920
<v Upol>complicated that in-house teams need external help.

640
00:39:46.060 --> 00:39:50.019
<v Upol>And I think, you know, a lot of us then learn the lesson that I have learned

641
00:39:50.020 --> 00:39:54.039
<v Upol>through all of this is embracing a sense of,

642
00:39:55.360 --> 00:39:59.349
<v Upol>you know, taking a learning mentality from a lot of the there

643
00:39:59.350 --> 00:40:03.369
<v Upol>is a famous paper. I forget the name of the author who kind of framed

644
00:40:03.370 --> 00:40:07.329
<v Upol>mistakes as mistakes like,

645
00:40:07.330 --> 00:40:11.739
<v Upol>you know, in a movie, you take multiple takes and not all the takes work.

646
00:40:11.740 --> 00:40:14.229
<v Upol>So a lot of them are mistakes, right?

647
00:40:14.230 --> 00:40:17.889
<v Upol>So I really embrace that mentality of mistakes.

648
00:40:17.890 --> 00:40:19.659
<v Upol>Not all projects will work out.

649
00:40:19.660 --> 00:40:22.509
<v Upol>You have a lot of mistakes, I guess.

650
00:40:22.510 --> 00:40:26.589
<v Upol>Nothing is a mistake, per se. And I think that really helped me have a more

651
00:40:26.590 --> 00:40:30.549
<v Upol>iterative mindset, which has paid a lot of dividends in getting a lot of this

652
00:40:30.550 --> 00:40:31.749
<v Upol>work done.

653
00:40:31.750 --> 00:40:36.459
<v Andrey>Yeah, I think it's interesting how in some sense, especially

654
00:40:36.460 --> 00:40:39.879
<v Andrey>doing it really enforces that.

655
00:40:39.880 --> 00:40:44.409
<v Andrey>You really have to adapt to that because you got to have failures,

656
00:40:44.410 --> 00:40:48.069
<v Andrey>but almost always will inform your understanding and ultimately guide you to

657
00:40:48.070 --> 00:40:50.040
<v Andrey>something interesting, ideally, you know.

658
00:40:51.400 --> 00:40:51.609
<v Andrey>Yeah.

659
00:40:51.610 --> 00:40:56.379
<v Andrey>So as you did research that led to

660
00:40:56.380 --> 00:41:00.939
<v Andrey>this paper, human centered explainable AI towards a reflective

661
00:41:00.940 --> 00:41:04.959
<v Andrey>socio technical approach where you lay a lot of groundwork for

662
00:41:04.960 --> 00:41:07.809
<v Andrey>how you can move towards that.

663
00:41:07.810 --> 00:41:10.599
<v Andrey>And we we really can't get too much into it.

664
00:41:10.600 --> 00:41:12.939
<v Andrey>It's it's quite detailed and self.

665
00:41:12.940 --> 00:41:16.959
<v Andrey>But he did write this excellent piece on a gradient towards

666
00:41:16.960 --> 00:41:21.249
<v Andrey>human centered, explainable AI every journey so far.

667
00:41:21.250 --> 00:41:25.179
<v Andrey>So we're going to link to that in the description, and you can just fly a

668
00:41:25.180 --> 00:41:28.449
<v Andrey>gradient and recommend you read that.

669
00:41:28.450 --> 00:41:32.579
<v Andrey>But for now, we're going to actually focus on a more recent work expanding

670
00:41:32.580 --> 00:41:36.579
<v Andrey>explainability towards social transparency

671
00:41:36.580 --> 00:41:37.810
<v Andrey>in AI systems.

672
00:41:39.100 --> 00:41:43.059
<v Andrey>So to get into it before even getting to any of the details,

673
00:41:44.350 --> 00:41:48.669
<v Andrey>you know, what was your goal in starting this project and sort of a problem

674
00:41:48.670 --> 00:41:49.870
<v Andrey>that motivated it?

675
00:41:51.130 --> 00:41:55.179
<v Upol>This is. Frankly, I feel like this was the paper

676
00:41:55.180 --> 00:41:59.139
<v Upol>that, like the Human-Centered AI paper, was the paper that needed to

677
00:41:59.140 --> 00:42:02.349
<v Upol>be written first for me to actually write this paper.

678
00:42:02.350 --> 00:42:06.339
<v Upol>And you know, a lot of the work in that cybersecurity company

679
00:42:06.340 --> 00:42:08.169
<v Upol>kind of really informed this.

680
00:42:08.170 --> 00:42:12.099
<v Upol>So, you know, for the longest time, I have been kind of arguing that we need to

681
00:42:12.100 --> 00:42:14.000
<v Upol>look outside the box, right?

682
00:42:15.100 --> 00:42:19.209
<v Upol>Yeah. So then, you know, largely speaking, the community will come back and ask

683
00:42:19.210 --> 00:42:23.439
<v Upol>her to call. I kind of get what you're trying to say, but what outside?

684
00:42:23.440 --> 00:42:25.859
<v Upol>What is outside? What do you want us to think about?

685
00:42:26.860 --> 00:42:30.849
<v Upol>And the the kernel of this paper is fundamentally

686
00:42:30.850 --> 00:42:35.559
<v Upol>and as as the title kind of says, extending explainability, it expands

687
00:42:35.560 --> 00:42:39.579
<v Upol>our conception of explainable AI

688
00:42:39.580 --> 00:42:43.689
<v Upol>beyond the realms of algorithmic transparency.

689
00:42:43.690 --> 00:42:47.679
<v Upol>By doing what? By adding this new

690
00:42:47.680 --> 00:42:51.399
<v Upol>concept called social transparency, which is actually like, not like New New in

691
00:42:51.400 --> 00:42:55.059
<v Upol>the sense that we created it in the in the context of XXXII, it's new.

692
00:42:55.060 --> 00:42:59.309
<v Upol>There is sort of transparency in online systems back from the 90s.

693
00:42:59.310 --> 00:43:03.969
<v Upol>And in the paper, we kind of pay homage to a lot of those work,

694
00:43:03.970 --> 00:43:07.749
<v Upol>but it's fundamentally making the following observation.

695
00:43:07.750 --> 00:43:12.159
<v Upol>So within AI systems, and I think this is where it becomes very tricky

696
00:43:12.160 --> 00:43:16.749
<v Upol>when when we say AI systems is actually somewhat of a misnomer

697
00:43:16.750 --> 00:43:20.709
<v Upol>because when we say AI systems are a very important part is left

698
00:43:20.710 --> 00:43:24.219
<v Upol>out and it's often implicit, which is the human part.

699
00:43:24.220 --> 00:43:28.719
<v Upol>Implicit in AI systems are what we call human

700
00:43:28.720 --> 00:43:30.969
<v Upol>AI assemblages. Right?

701
00:43:30.970 --> 00:43:32.889
<v Upol>So these are two coupled points.

702
00:43:32.890 --> 00:43:36.969
<v Upol>So ideally, what you're really going for is the explainability

703
00:43:36.970 --> 00:43:41.379
<v Upol>of this assemblage, right? The human part of being often implicit.

704
00:43:41.380 --> 00:43:45.939
<v Upol>But but how can you get the explainability?

705
00:43:45.940 --> 00:43:49.989
<v Upol>All of these assemblage, these two part system, the human and the AI

706
00:43:49.990 --> 00:43:53.919
<v Upol>by just focusing on the air and asking the question that is asking

707
00:43:53.920 --> 00:43:56.679
<v Upol>in this paper? So then the question becomes IRA to Paul.

708
00:43:56.680 --> 00:44:00.069
<v Upol>I get it like, you know, you can add, know you need the human part, but what

709
00:44:00.070 --> 00:44:02.649
<v Upol>about it in looking very Typekit?

710
00:44:02.650 --> 00:44:06.579
<v Upol>So that's where if you add the transparency on the human side, we

711
00:44:06.580 --> 00:44:10.989
<v Upol>kind of introduce this notion of social transparency in AI systems,

712
00:44:10.990 --> 00:44:14.349
<v Upol>right? Operationalize a little bit of this in the paper.

713
00:44:14.350 --> 00:44:18.519
<v Andrey>Right. So yeah, it's I think very interesting about this in terms

714
00:44:18.520 --> 00:44:20.679
<v Andrey>of operationalizing that.

715
00:44:20.680 --> 00:44:24.809
<v Andrey>Not only do you highlight this need, which I think is very intuitive, but

716
00:44:24.810 --> 00:44:28.629
<v Andrey>actually exquisite talk about how to do this, how to be useful and really

717
00:44:28.630 --> 00:44:32.949
<v Andrey>beyond technical transparency and how to integrate

718
00:44:32.950 --> 00:44:36.579
<v Andrey>that. And actually, you know, where do you start?

719
00:44:36.580 --> 00:44:39.799
<v Andrey>How do you how do you do it and so on, right?

720
00:44:39.800 --> 00:44:40.800
<v Andrey>Hmm.

721
00:44:41.560 --> 00:44:45.669
<v Andrey>So I guess maybe can we dove in a bit more

722
00:44:45.670 --> 00:44:47.409
<v Andrey>about this idea of social transparency?

723
00:44:47.410 --> 00:44:49.309
<v Andrey>Ethics is so important.

724
00:44:49.310 --> 00:44:53.319
<v Andrey>And so we know because fancy sort of trying to understand what

725
00:44:53.320 --> 00:44:57.339
<v Andrey>the algorithmic side of it is doing, what

726
00:44:57.340 --> 00:45:01.539
<v Andrey>the model is thinking, but what is social rationality

727
00:45:01.540 --> 00:45:06.109
<v Andrey>one of its components? And yeah, how should people understand it?

728
00:45:06.110 --> 00:45:08.349
<v Upol>Yeah. Well, that's a that's a fascinating question.

729
00:45:09.370 --> 00:45:13.299
<v Upol>So to understand social transparency, I think we

730
00:45:13.300 --> 00:45:15.579
<v Upol>have to accept a few things.

731
00:45:15.580 --> 00:45:19.839
<v Upol>First, we have to understand and acknowledge that

732
00:45:19.840 --> 00:45:23.079
<v Upol>work is social, right?

733
00:45:23.080 --> 00:45:24.909
<v Upol>You know, we don't work in silos.

734
00:45:24.910 --> 00:45:27.759
<v Upol>Most of us, we work within teams.

735
00:45:27.760 --> 00:45:29.529
<v Upol>So there meet. That means right.

736
00:45:29.530 --> 00:45:33.549
<v Upol>There is some need to add this transparency data

737
00:45:33.550 --> 00:45:37.699
<v Upol>in an office when you're working with a team or virtually through Slack, right?

738
00:45:37.700 --> 00:45:41.529
<v Upol>There's a lot of chatter that is going on and there is a necessity behind that.

739
00:45:41.530 --> 00:45:45.489
<v Upol>So that is fast the the fast realization that work

740
00:45:45.490 --> 00:45:49.599
<v Upol>is social. That means there might be a need to make that social nature

741
00:45:49.600 --> 00:45:52.959
<v Upol>a little bit transparent, especially when we're dealing with AI mediated

742
00:45:52.960 --> 00:45:54.819
<v Upol>decision support systems.

743
00:45:54.820 --> 00:45:59.049
<v Upol>So and you know, as we share in the paper, we were trying

744
00:45:59.050 --> 00:45:59.273
<v Upol>to.

745
00:45:59.274 --> 00:46:02.709
<v Upol>So this is also difficult as well to some extent, right?

746
00:46:02.710 --> 00:46:06.729
<v Upol>One of the challenges that AI researchers face is how do we

747
00:46:06.730 --> 00:46:10.779
<v Upol>know what the future really looks like without

748
00:46:10.780 --> 00:46:15.209
<v Upol>really investing months and months of work, building

749
00:46:15.210 --> 00:46:18.489
<v Upol>large infrastructures and models and then realizing we're actually not very

750
00:46:18.490 --> 00:46:23.089
<v Upol>useful, but that that is a very hard cost

751
00:46:23.090 --> 00:46:27.459
<v Upol>of doing this. So due to kind of explain that

752
00:46:27.460 --> 00:46:31.119
<v Upol>we used this notion of scenario based design.

753
00:46:31.120 --> 00:46:35.109
<v Upol>So this is coming from the traditions of design fiction,

754
00:46:35.110 --> 00:46:39.999
<v Upol>as I'm drawing a lot of this actually from the theoretical underpinnings

755
00:46:40.000 --> 00:46:43.209
<v Upol>of the human centered explainable AI paper that we just talked about.

756
00:46:43.210 --> 00:46:47.979
<v Upol>Mm-Hmm. And so using scenario based design, we conducted

757
00:46:47.980 --> 00:46:51.339
<v Upol>around four workshops with a lot of people.

758
00:46:51.340 --> 00:46:55.299
<v Upol>From different technology companies, just to get a sense of what

759
00:46:55.300 --> 00:46:59.229
<v Upol>are the things that are outside the black box that

760
00:46:59.230 --> 00:47:03.219
<v Upol>people want when they make a decision within

761
00:47:03.220 --> 00:47:07.149
<v Upol>the AI system. Right. So that's the workshop is meant to kind of get a more

762
00:47:07.150 --> 00:47:09.069
<v Upol>formative understanding, right?

763
00:47:09.070 --> 00:47:13.299
<v Andrey>What needs to be made transparent in this social system in terms

764
00:47:13.300 --> 00:47:13.929
<v Andrey>of

765
00:47:13.930 --> 00:47:17.289
<v Upol>right, because there are so many things you can make transparent, right?

766
00:47:17.290 --> 00:47:20.499
<v Upol>Because how do you know which one is the right thing to do right?

767
00:47:20.500 --> 00:47:24.789
<v Upol>And I think through these workshops and this is pre-COVID,

768
00:47:24.790 --> 00:47:27.759
<v Upol>so we have the ability to kind of get in person and kind of have these

769
00:47:27.760 --> 00:47:31.779
<v Upol>workshops. And what we learn was that

770
00:47:31.780 --> 00:47:35.949
<v Upol>out of this and this is, I think, what we what we call in the paper, the four

771
00:47:35.950 --> 00:47:40.869
<v Upol>ws, right? So in addition to

772
00:47:40.870 --> 00:47:45.729
<v Upol>the i's technical transparency or algorithmic transparency,

773
00:47:45.730 --> 00:47:50.349
<v Upol>these practitioners, data scientists, analysts and others

774
00:47:50.350 --> 00:47:54.369
<v Upol>wanted to know for things who

775
00:47:54.370 --> 00:47:56.919
<v Upol>did what, when and why.

776
00:47:58.180 --> 00:48:02.259
<v Upol>So those became again, we are not saying this is the end all, be all to all

777
00:48:02.260 --> 00:48:06.279
<v Upol>social transparency. There might be other socially transparent systems that

778
00:48:06.280 --> 00:48:09.939
<v Upol>that do very well, but actually in the cybersecurity example, going back to

779
00:48:09.940 --> 00:48:13.959
<v Upol>that when we implemented this aspect of

780
00:48:13.960 --> 00:48:18.549
<v Upol>who did what, when and why. So imagine, like, you know, next to a threat,

781
00:48:18.550 --> 00:48:20.259
<v Upol>you know, close these ports, right?

782
00:48:20.260 --> 00:48:23.979
<v Upol>Let's imagine that if Julie have social transparency, what would do we have

783
00:48:23.980 --> 00:48:26.649
<v Upol>seen? Julie, who got fired before?

784
00:48:26.650 --> 00:48:29.169
<v Upol>So when you get this

785
00:48:30.580 --> 00:48:34.599
<v Upol>new disease, the air is recommending the ports to be closed and you're

786
00:48:34.600 --> 00:48:37.539
<v Upol>like, OK. Is that true? Like, is that real or not?

787
00:48:37.540 --> 00:48:39.669
<v Upol>I don't know if it's a false positive.

788
00:48:39.670 --> 00:48:43.689
<v Upol>But then Julie is able to see, you know, maybe 10 other people

789
00:48:43.690 --> 00:48:46.629
<v Upol>dealing with a very similar situation in the past.

790
00:48:46.630 --> 00:48:50.139
<v Upol>And in one of those Julie scenes, I one of the who's right?

791
00:48:50.140 --> 00:48:54.069
<v Upol>Maybe imagine this is Bob, and Bob is a veteran in the

792
00:48:54.070 --> 00:48:59.199
<v Upol>industry. He's like a level three analyst, and he says,

793
00:48:59.200 --> 00:49:02.319
<v Upol>Oh, these are backup site reports in law.

794
00:49:02.320 --> 00:49:05.529
<v Upol>Mm. Right? So who did what?

795
00:49:05.530 --> 00:49:07.359
<v Upol>Right? When?

796
00:49:07.360 --> 00:49:09.669
<v Upol>Maybe, let's say, three months ago?

797
00:49:09.670 --> 00:49:11.799
<v Upol>And why? So the why is the reasoning right?

798
00:49:11.800 --> 00:49:15.999
<v Upol>Like these are backup center reports ignored

799
00:49:16.000 --> 00:49:20.119
<v Upol>by situating this extra piece of information that is actually capturing?

800
00:49:20.120 --> 00:49:23.319
<v Upol>You know, one might argue, Hey, people like that seems like a bad problem.

801
00:49:23.320 --> 00:49:26.529
<v Upol>They should've just added back to the data center, right?

802
00:49:26.530 --> 00:49:30.369
<v Upol>That's not good. And that is where I think the critical insight lies.

803
00:49:30.370 --> 00:49:34.539
<v Upol>There is not enough things you can add in the dataset.

804
00:49:34.540 --> 00:49:37.149
<v Upol>It's like a golden goose chase

805
00:49:37.150 --> 00:49:39.129
<v Andrey>because it's all inside the model, right?

806
00:49:39.130 --> 00:49:42.129
<v Upol>Exactly. And sometimes things happen dynamically.

807
00:49:42.130 --> 00:49:45.579
<v Upol>Remember, data sets are basically snapshots of the past.

808
00:49:46.700 --> 00:49:50.839
<v Upol>And work norms actually change over time due

809
00:49:50.840 --> 00:49:55.549
<v Upol>to the sensitive nature of certain cybersecurity explanation institutions.

810
00:49:55.550 --> 00:49:59.109
<v Upol>You do not want certain things to be coded into a data set.

811
00:49:59.110 --> 00:50:02.929
<v Upol>Right. Because what if that gets hacked, then all your secrets are out.

812
00:50:02.930 --> 00:50:07.249
<v Upol>So there will always be elements that are not

813
00:50:07.250 --> 00:50:11.329
<v Upol>quantifiable, that are not acceptable in a cleanly named

814
00:50:11.330 --> 00:50:15.289
<v Upol>dataset. In those cases, those very things that are

815
00:50:15.290 --> 00:50:19.279
<v Upol>hard to quantify, hard to incorporate often can be the difference

816
00:50:19.280 --> 00:50:23.329
<v Upol>maker between right and wrong decisions where they are.

817
00:50:23.330 --> 00:50:27.559
<v Upol>So by adding this social transparency, you are able to inform

818
00:50:27.560 --> 00:50:31.849
<v Upol>someone to know when to trust the AI versus

819
00:50:31.850 --> 00:50:32.850
<v Upol>not.

820
00:50:33.130 --> 00:50:37.509
<v Andrey>Mm hmm. Yeah, exactly, and to dig a bit deeper.

821
00:50:37.510 --> 00:50:42.759
<v Andrey>I would love to hear how did this scenario based design

822
00:50:42.760 --> 00:50:46.839
<v Andrey>process work? I think figure one of your work is really interesting is

823
00:50:46.840 --> 00:50:48.220
<v Andrey>that the scenario used.

824
00:50:50.020 --> 00:50:54.189
<v Upol>So you know, in this scenario, we asked our participants

825
00:50:54.190 --> 00:50:58.299
<v Upol>to kind of envision being in using a AI powered

826
00:50:58.300 --> 00:51:02.529
<v Upol>pricing tool to price and access management

827
00:51:02.530 --> 00:51:05.419
<v Upol>product to a customer called Scout.

828
00:51:05.420 --> 00:51:09.409
<v Upol>Right. So the AI kind of does its analysis and recommends that, hey, you

829
00:51:09.410 --> 00:51:12.939
<v Upol>got to sell it at 100 bucks per month per account.

830
00:51:12.940 --> 00:51:17.289
<v Upol>And it did also share some post-rock explanations and kind of justifies,

831
00:51:17.290 --> 00:51:21.339
<v Upol>White said, what it's saying and that the model, the technical transparency

832
00:51:21.340 --> 00:51:25.419
<v Upol>pieces like the item, the quotable goes off a salesperson into account.

833
00:51:25.420 --> 00:51:29.499
<v Upol>It did a comparative pricing of what similar customers pray, and also it gave

834
00:51:29.500 --> 00:51:33.249
<v Upol>you the floor. So what is the cost price for doing this product?

835
00:51:33.250 --> 00:51:37.209
<v Upol>So these are so imagine that's the first letter and today, right?

836
00:51:37.210 --> 00:51:40.779
<v Upol>That's the state of the art. Nothing is better than that, right?

837
00:51:40.780 --> 00:51:44.709
<v Upol>We don't have the social transparency that we kind of envision in this

838
00:51:44.710 --> 00:51:47.859
<v Upol>paper, but that is where the state of the art was.

839
00:51:47.860 --> 00:51:49.569
<v Upol>So that was our grounding moment.

840
00:51:49.570 --> 00:51:53.499
<v Upol>So we would ask our people as they went through the walk to what would you

841
00:51:53.500 --> 00:51:56.499
<v Upol>do right now? Do you think this is, you know, before we showed them any social

842
00:51:56.500 --> 00:52:00.639
<v Upol>transparency? Right? And we will see that most people agree

843
00:52:00.640 --> 00:52:02.469
<v Upol>with that. Yeah, this seems like a decent.

844
00:52:02.470 --> 00:52:06.009
<v Upol>We also kind of calibrated the price point by asking experts.

845
00:52:06.010 --> 00:52:08.919
<v Upol>So we kind of grounded a lot of his data, even though it's a scenario.

846
00:52:08.920 --> 00:52:13.629
<v Upol>If it's fictional, the fiction is grounded in reality, our version of reality.

847
00:52:13.630 --> 00:52:17.629
<v Upol>And then we told them, like, now imagine what have you found out?

848
00:52:17.630 --> 00:52:21.789
<v Upol>And that only one out of 10 people sold this product

849
00:52:21.790 --> 00:52:24.759
<v Upol>and the recommended price? What would you do?

850
00:52:24.760 --> 00:52:27.609
<v Upol>And you could see our participants kind of get very interested, like those

851
00:52:27.610 --> 00:52:31.659
<v Upol>like, oh, that's really interesting information that

852
00:52:31.660 --> 00:52:34.029
<v Upol>helps me calibrate what to do.

853
00:52:34.030 --> 00:52:38.139
<v Upol>So then we kind of dug deeper, which is like the bullet points three four five

854
00:52:38.140 --> 00:52:42.159
<v Upol>to share three examples of past colleagues who have

855
00:52:42.160 --> 00:52:47.079
<v Upol>dealt with the same customer scout on similar products.

856
00:52:47.080 --> 00:52:51.609
<v Upol>And then one of the most important comments were made by Jessica

857
00:52:51.610 --> 00:52:54.459
<v Upol>or Jess, who's a sales director.

858
00:52:54.460 --> 00:52:58.479
<v Upol>And it turns out Jess had rejected the recommendation, but

859
00:52:58.480 --> 00:53:02.439
<v Upol>the sale did happen, and the comment was the most important where they

860
00:53:02.440 --> 00:53:04.599
<v Upol>say that, Hey, is COVID 19.

861
00:53:04.600 --> 00:53:06.310
<v Upol>And this was done at the height of the pandemic.

862
00:53:08.080 --> 00:53:11.049
<v Upol>I can't lose a long term, profitable customer.

863
00:53:11.050 --> 00:53:14.559
<v Upol>So they offered 10 percent below the cost price.

864
00:53:14.560 --> 00:53:16.599
<v Upol>And that's an important part, right?

865
00:53:16.600 --> 00:53:20.889
<v Upol>That not only did they give you a discount, but just the director

866
00:53:20.890 --> 00:53:25.209
<v Upol>had given them below the cost price and that social

867
00:53:25.210 --> 00:53:29.169
<v Upol>context of what was going on that was outside of the algorithm, right?

868
00:53:29.170 --> 00:53:33.129
<v Upol>Very much inform how people acted on it because remember, without

869
00:53:33.130 --> 00:53:35.229
<v Upol>any of this context, they fell.

870
00:53:35.230 --> 00:53:37.719
<v Upol>The price was fair. It was done the right way.

871
00:53:37.720 --> 00:53:41.769
<v Upol>You know, the justifications were right, but very few

872
00:53:41.770 --> 00:53:45.759
<v Upol>people actually, you know, offered the

873
00:53:45.760 --> 00:53:50.199
<v Upol>same price when they knew what others had done, especially when a director

874
00:53:50.200 --> 00:53:52.659
<v Upol>level person had done it before.

875
00:53:52.660 --> 00:53:56.789
<v Andrey>Yes. So in a sense, I think going back to something you mentioned,

876
00:53:56.790 --> 00:54:00.219
<v Andrey>it's letting you know what the model doesn't know.

877
00:54:00.220 --> 00:54:04.239
<v Andrey>Right? It doesn't know about COVID and these four

878
00:54:04.240 --> 00:54:08.949
<v Andrey>W's. The social scenario doesn't really explain the mottoes decisions,

879
00:54:08.950 --> 00:54:13.299
<v Andrey>but it does like to understand via a system

880
00:54:13.300 --> 00:54:17.499
<v Andrey>better in the sense of like the AI system is situated within

881
00:54:17.500 --> 00:54:21.819
<v Andrey>the organization. And so you get to know more its weaknesses

882
00:54:21.820 --> 00:54:25.929
<v Andrey>and where and when to follow it. Maybe when not, which

883
00:54:25.930 --> 00:54:29.899
<v Andrey>I think would be a lot harder about seeing like, OK, this first person accepted

884
00:54:29.900 --> 00:54:32.110
<v Andrey>discrimination. This person didn't.

885
00:54:33.460 --> 00:54:36.819
<v Andrey>And this figure, I think, illustrates that really well.

886
00:54:36.820 --> 00:54:40.359
<v Upol>And I think it's kind of asking the question like, what are the eyes blind

887
00:54:40.360 --> 00:54:44.349
<v Upol>spots and can other humans who have interacted with this system

888
00:54:44.350 --> 00:54:46.239
<v Upol>in the past and address it?

889
00:54:46.240 --> 00:54:50.949
<v Upol>So like, for instance, I am now currently working with radiation oncologists

890
00:54:50.950 --> 00:54:54.610
<v Upol>on a very similar project and in radiation oncology,

891
00:54:55.990 --> 00:55:00.219
<v Upol>just like in other fields, there is no absolute ground

892
00:55:00.220 --> 00:55:03.969
<v Upol>truth. With 80 senators, we are so comfortable with the terminology of ground

893
00:55:03.970 --> 00:55:07.989
<v Upol>truth, right? But when it comes to using radiation to treat

894
00:55:07.990 --> 00:55:10.539
<v Upol>cancers, they're established practices.

895
00:55:10.540 --> 00:55:14.709
<v Upol>There is no like absolute gold thing that everyone must do

896
00:55:14.710 --> 00:55:16.869
<v Upol>because each patient is different.

897
00:55:16.870 --> 00:55:19.239
<v Upol>Each treatment facility is different.

898
00:55:19.240 --> 00:55:20.590
<v Upol>So in that case?

899
00:55:21.840 --> 00:55:25.949
<v Upol>Knowing when to trust these recommendations, foreign by saying,

900
00:55:25.950 --> 00:55:30.179
<v Upol>you know, give this much radiation to the patient's left optic nerve.

901
00:55:30.180 --> 00:55:33.449
<v Upol>Right. That's a very high stakes decision, right?

902
00:55:33.450 --> 00:55:36.269
<v Upol>Because if you do it the wrong way, you can blast away my left optic there and

903
00:55:36.270 --> 00:55:38.369
<v Upol>take away my vision. Right?

904
00:55:38.370 --> 00:55:42.419
<v Upol>But guess what? What the AI system might not have known is

905
00:55:42.420 --> 00:55:45.059
<v Upol>that the patient is blind in the right die.

906
00:55:45.060 --> 00:55:48.629
<v Upol>So all of the calculus goes away because we know there's no like central

907
00:55:48.630 --> 00:55:51.519
<v Upol>blindness data in randomized controlled trials.

908
00:55:51.520 --> 00:55:52.229
<v Upol>Right?

909
00:55:52.230 --> 00:55:56.519
<v Upol>So just knowing that extra piece can help you calibrate

910
00:55:56.520 --> 00:56:00.659
<v Upol>how much treatment you want to give it and knowing what your peers done

911
00:56:00.660 --> 00:56:04.649
<v Upol>right, because in these kind of communities of practices is very much

912
00:56:04.650 --> 00:56:08.759
<v Upol>community driven, right? Like the radiation oncologist kind of

913
00:56:08.760 --> 00:56:13.199
<v Upol>have these standards that they co-developed together are two studies.

914
00:56:13.200 --> 00:56:17.219
<v Upol>So this social transparency starts mattering extremely

915
00:56:17.220 --> 00:56:21.179
<v Upol>when the cost of failure is also very high, right?

916
00:56:21.180 --> 00:56:24.669
<v Upol>Like, you know, blasting someone's optic nerve nerve out there, the radiation

917
00:56:24.670 --> 00:56:29.969
<v Upol>is a pretty high cost rather than, you know, missing a song recommendation.

918
00:56:29.970 --> 00:56:32.669
<v Upol>And I think that's the other part like you don't think I don't.

919
00:56:32.670 --> 00:56:36.839
<v Upol>Social transparency is not really helpful when the stakes are low

920
00:56:36.840 --> 00:56:40.529
<v Upol>or when the nature of the job is not very collaborative, right?

921
00:56:40.530 --> 00:56:44.429
<v Upol>But the more the stakes are high, the more collaboration is needed.

922
00:56:44.430 --> 00:56:48.389
<v Upol>Social transparency becomes important because what then becomes very social.

923
00:56:49.770 --> 00:56:53.100
<v Andrey>Yeah, it's just makes me feel like you can almost

924
00:56:54.660 --> 00:56:59.279
<v Andrey>consider this like what if the AI model is

925
00:56:59.280 --> 00:57:01.409
<v Andrey>in some sense, a coworker, right?

926
00:57:01.410 --> 00:57:05.369
<v Andrey>When you work with people, some people you trust more and less

927
00:57:05.370 --> 00:57:08.459
<v Andrey>and when you do decision making, it is sort of collaborative.

928
00:57:08.460 --> 00:57:11.639
<v Andrey>You know, you might debate, you might ask whatever you consider, address, if

929
00:57:11.640 --> 00:57:15.749
<v Andrey>you consider that that's not something that you can do with

930
00:57:15.750 --> 00:57:19.709
<v Andrey>any AI system, at least for now, you can't say, well, you know, have you taken

931
00:57:19.710 --> 00:57:21.449
<v Andrey>this into that into account?

932
00:57:21.450 --> 00:57:25.649
<v Andrey>But seeing the social context, it seems to me, was

933
00:57:25.650 --> 00:57:30.209
<v Andrey>what people might have realized that to me, this comment and now, you know,

934
00:57:30.210 --> 00:57:32.039
<v Andrey>didn't take into account the COVID thing.

935
00:57:33.390 --> 00:57:37.379
<v Andrey>So, yeah, she gets interesting in the sense of like you get to know the

936
00:57:37.380 --> 00:57:41.609
<v Andrey>system as another entity you work with

937
00:57:41.610 --> 00:57:43.049
<v Andrey>almost.

938
00:57:43.050 --> 00:57:46.709
<v Upol>Yeah, yeah, exactly. And I think that's kind of changes the way we think of

939
00:57:46.710 --> 00:57:49.439
<v Upol>human collaboration, right?

940
00:57:49.440 --> 00:57:53.009
<v Upol>Because you are now like, I often think about it, it's like, you know, Avatar

941
00:57:53.010 --> 00:57:55.179
<v Upol>The Last Airbender. I don't know.

942
00:57:55.180 --> 00:57:55.769
<v Andrey>Yeah, yeah.

943
00:57:55.770 --> 00:57:59.699
<v Upol>So it's like, what does Avatar do? And new faces around there, like Avatar and

944
00:57:59.700 --> 00:58:04.589
<v Upol>right? Like when he faces some difficult choices, he kind of seeks the counsel

945
00:58:04.590 --> 00:58:08.429
<v Upol>of past avatars who had come before him.

946
00:58:08.430 --> 00:58:12.479
<v Upol>And so the social transparency is in a weird way of capturing

947
00:58:12.480 --> 00:58:16.619
<v Upol>that historical context in a system in

948
00:58:16.620 --> 00:58:20.849
<v Upol>situ that really makes your decision making in that moment

949
00:58:20.850 --> 00:58:22.529
<v Upol>much more informed.

950
00:58:22.530 --> 00:58:25.979
<v Upol>Because at the end of the day, we have to ask ourselves why these explanations

951
00:58:25.980 --> 00:58:30.509
<v Upol>aren't there. They're there to make things actionable.

952
00:58:30.510 --> 00:58:34.649
<v Upol>If you if someone cannot do something without explanation, then there might

953
00:58:34.650 --> 00:58:37.149
<v Upol>not be any explanation, right?

954
00:58:37.150 --> 00:58:40.409
<v Upol>Like if the machine is explaining itself and I cannot do anything with it,

955
00:58:40.410 --> 00:58:44.609
<v Upol>that's very difficult. Like, I don't know what purpose of serving other

956
00:58:44.610 --> 00:58:47.729
<v Upol>than just understanding. But if I understand and cannot do anything with

957
00:58:47.730 --> 00:58:50.069
<v Upol>understanding, what is it there for anyway?

958
00:58:50.070 --> 00:58:54.279
<v Upol>So social transparency can make things more actionable,

959
00:58:54.280 --> 00:58:57.199
<v Upol>even if right, even if.

960
00:58:57.200 --> 00:59:01.369
<v Upol>The participants are saying no to the system,

961
00:59:01.370 --> 00:59:06.379
<v Upol>and that's the crucial part. I think it changes how we formulate trust

962
00:59:06.380 --> 00:59:10.369
<v Upol>because a lot of the work around trust that we see is around user

963
00:59:10.370 --> 00:59:14.669
<v Upol>acceptance. I want my user to like the I want my user to accept

964
00:59:14.670 --> 00:59:19.609
<v Upol>me. But I think what we are seeing is it's not about just mindlessly

965
00:59:19.610 --> 00:59:21.379
<v Upol>fostering trust.

966
00:59:21.380 --> 00:59:25.489
<v Upol>It's about mindfully calibrating trust.

967
00:59:25.490 --> 00:59:28.699
<v Upol>You don't want people to over trust your system because then there are

968
00:59:28.700 --> 00:59:29.700
<v Upol>liability issues.

969
00:59:30.780 --> 00:59:34.739
<v Andrey>Yeah, exactly. And, yeah, so in terms

970
00:59:34.740 --> 00:59:38.879
<v Andrey>of this process, you started these four workshops you,

971
00:59:38.880 --> 00:59:42.869
<v Andrey>I think, carried out this idea of the four W's what

972
00:59:42.870 --> 00:59:46.829
<v Andrey>who why when. And if I understand correctly after the workshops, you

973
00:59:46.830 --> 00:59:51.449
<v Andrey>then had sort of a more controlled study where

974
00:59:51.450 --> 00:59:53.879
<v Andrey>29 participants. Is that right?

975
00:59:53.880 --> 00:59:57.839
<v Upol>Yeah, yeah. So then then we really did this once we built the scenario, right?

976
00:59:57.840 --> 01:00:01.469
<v Upol>Then you kind of when made people go through the scenario of the study.

977
01:00:01.470 --> 01:00:04.499
<v Upol>So we would walk them through the scenario just the way I kind of described a

978
01:00:04.500 --> 01:00:06.359
<v Upol>few minutes ago.

979
01:00:06.360 --> 01:00:10.229
<v Upol>And we will start seeing that how they start thinking through this and this is

980
01:00:10.230 --> 01:00:12.359
<v Upol>the beauty of scenario, these design, right?

981
01:00:12.360 --> 01:00:14.639
<v Upol>You can think of this as a probe.

982
01:00:14.640 --> 01:00:18.719
<v Upol>So you are probing for reactions about an air power

983
01:00:18.720 --> 01:00:23.069
<v Upol>system without really needing to invest the severe infrastructure

984
01:00:23.070 --> 01:00:26.579
<v Upol>that is needed to make a like a full fledged AI system.

985
01:00:26.580 --> 01:00:30.869
<v Upol>But you're getting very good design elements out of this

986
01:00:30.870 --> 01:00:32.639
<v Upol>for a lot less cost.

987
01:00:32.640 --> 01:00:36.209
<v Upol>It does not mean that you don't build a system, of course you do.

988
01:00:36.210 --> 01:00:40.169
<v Upol>So, for instance, in the cybersecurity case, once we

989
01:00:40.170 --> 01:00:44.159
<v Upol>did very similar studies with them, with scenarios, we went and we

990
01:00:44.160 --> 01:00:46.639
<v Upol>built out, this takes. And guess what, right?

991
01:00:46.640 --> 01:00:51.509
<v Upol>Like two years into the project with them, that company

992
01:00:51.510 --> 01:00:55.709
<v Upol>actually now lives in a socially transparent world where all

993
01:00:55.710 --> 01:00:59.879
<v Upol>their decisions are actually automatically situated with prior

994
01:00:59.880 --> 01:01:03.959
<v Upol>history. And they are actually you able to use

995
01:01:03.960 --> 01:01:08.009
<v Upol>the four W's as training to retrain their

996
01:01:08.010 --> 01:01:12.119
<v Upol>models so that the decision is not only just algorithmically situated but

997
01:01:12.120 --> 01:01:14.099
<v Upol>also socially situated, right?

998
01:01:14.100 --> 01:01:18.749
<v Andrey>So it becomes of this sort of feature space to model is informed by

999
01:01:18.750 --> 01:01:19.829
<v Andrey>it seems.

1000
01:01:19.830 --> 01:01:22.059
<v Upol>Absolutely. And then think about it that way, right?

1001
01:01:22.060 --> 01:01:25.800
<v Upol>Like you are also getting a corpus without actually building a corpus.

1002
01:01:26.970 --> 01:01:29.189
<v Upol>Right. Because over time, what is going to happen?

1003
01:01:29.190 --> 01:01:33.179
<v Upol>These four W's are going to get enough density depending on who knows, right

1004
01:01:33.180 --> 01:01:37.109
<v Upol>where they become large enough that you can feed back into the model.

1005
01:01:37.110 --> 01:01:41.129
<v Upol>But the cool part is from day one, they're giving

1006
01:01:41.130 --> 01:01:42.959
<v Upol>value to the user.

1007
01:01:42.960 --> 01:01:47.039
<v Upol>Right? So they're not like being a grunt

1008
01:01:47.040 --> 01:01:49.019
<v Upol>work of a data set building task.

1009
01:01:49.020 --> 01:01:52.679
<v Upol>That is the one thing that a lot of my cybersecurity analyst stakeholders would

1010
01:01:52.680 --> 01:01:55.769
<v Upol>counter that like, Hey, this is actually useful.

1011
01:01:55.770 --> 01:01:59.339
<v Upol>I like doing this because it doesn't make me feel like I'm building a stupid

1012
01:01:59.340 --> 01:02:01.739
<v Upol>dataset that I might not ever see.

1013
01:02:01.740 --> 01:02:05.699
<v Upol>So they're actually building a corpus while getting value

1014
01:02:05.700 --> 01:02:09.989
<v Upol>from it, which is very hard to achieve in any kind of dataset building tasks.

1015
01:02:09.990 --> 01:02:12.329
<v Andrey>Mm hmm. Yeah, exactly.

1016
01:02:12.330 --> 01:02:16.349
<v Andrey>And it's interesting to hear that they are more into

1017
01:02:16.350 --> 01:02:20.519
<v Andrey>it. And I think it's also funny that, you know, having done a study

1018
01:02:20.520 --> 01:02:24.509
<v Andrey>in the paper, you are able to not just give you on

1019
01:02:24.510 --> 01:02:29.269
<v Andrey>tape, but actually quotes the study participants and really,

1020
01:02:29.270 --> 01:02:33.779
<v Andrey>you know, showing their words very concretely

1021
01:02:33.780 --> 01:02:36.119
<v Andrey>how you came to your conclusion.

1022
01:02:36.120 --> 01:02:40.379
<v Andrey>So for instance, one quote that I think is really relevant is

1023
01:02:40.380 --> 01:02:44.309
<v Andrey>I hate how it just gives me a confidence level in gibberish to engineers will

1024
01:02:44.310 --> 01:02:47.129
<v Andrey>understand for zero context, right?

1025
01:02:47.130 --> 01:02:51.269
<v Andrey>It's a very human reaction that really tells you, Well,

1026
01:02:51.270 --> 01:02:53.939
<v Andrey>you know, this person, what's the context, right?

1027
01:02:55.230 --> 01:02:59.759
<v Andrey>So then, you know, we've talked through somebody resembles your study,

1028
01:02:59.760 --> 01:03:03.059
<v Andrey>and now I think we can dove in a little more.

1029
01:03:03.060 --> 01:03:05.729
<v Andrey>So we've talked about social concerns.

1030
01:03:05.730 --> 01:03:10.559
<v Andrey>And I think in the paper, you also break down a little bit what exactly

1031
01:03:10.560 --> 01:03:14.229
<v Andrey>is made visible, what you want to make visible.

1032
01:03:14.230 --> 01:03:18.179
<v Andrey>So the decision making context and the organizational context.

1033
01:03:18.180 --> 01:03:22.199
<v Andrey>So yeah, what is involved in these things that people really

1034
01:03:22.200 --> 01:03:23.699
<v Andrey>need to understand?

1035
01:03:23.700 --> 01:03:26.459
<v Upol>Yeah. So, you know, through the four W's.

1036
01:03:26.460 --> 01:03:30.149
<v Upol>So these are the kind of like that you can think of them, the vehicles that

1037
01:03:30.150 --> 01:03:32.249
<v Upol>carry this context, right?

1038
01:03:32.250 --> 01:03:36.299
<v Upol>Mm hmm. So the four is the first thing, and I think we kind

1039
01:03:36.300 --> 01:03:40.769
<v Upol>of shared a framework that sees how it makes context

1040
01:03:40.770 --> 01:03:44.759
<v Upol>visible at three levels, which do the technical, the decision

1041
01:03:44.760 --> 01:03:47.669
<v Upol>making and the organizational right.

1042
01:03:47.670 --> 01:03:51.749
<v Upol>So but with the umbrella

1043
01:03:51.750 --> 01:03:55.676
<v Upol>of all three is the first to use what we call crew

1044
01:03:55.677 --> 01:04:00.569
<v Upol>knowledge, right? So crew knowledge is really an important part

1045
01:04:00.570 --> 01:04:04.409
<v Upol>of these informal knowledge that is acquired through hands on experience.

1046
01:04:04.410 --> 01:04:08.369
<v Upol>It's part, and it's tacit of every job that anyone has

1047
01:04:08.370 --> 01:04:12.439
<v Upol>ever done. Right? And it's often situated locally in

1048
01:04:12.440 --> 01:04:16.469
<v Upol>a in a tight knit community of practice, sort of like an aggregated set of

1049
01:04:16.470 --> 01:04:18.359
<v Upol>Milhouse. Right?

1050
01:04:18.360 --> 01:04:22.659
<v Upol>So the Y right, the Y is actually giving

1051
01:04:22.660 --> 01:04:24.899
<v Upol>insight into that knowledge.

1052
01:04:24.900 --> 01:04:28.289
<v Upol>These are the variables that would be important for decision making, but

1053
01:04:28.290 --> 01:04:32.119
<v Upol>sometimes are not captured in the eyes kind of feature space.

1054
01:04:32.120 --> 01:04:36.479
<v Upol>Right? The other part is like social transparency can support analogical

1055
01:04:36.480 --> 01:04:40.469
<v Upol>reasoning in terms of like, OK, if someone has made the decision in the

1056
01:04:40.470 --> 01:04:44.489
<v Upol>past, like you remember, just I just gave a discount for the

1057
01:04:44.490 --> 01:04:48.719
<v Upol>COVID case. So that means I too can give the discount

1058
01:04:48.720 --> 01:04:51.059
<v Upol>on the Kovic case. Right?

1059
01:04:51.060 --> 01:04:52.949
<v Upol>So it's aiming.

1060
01:04:52.950 --> 01:04:56.909
<v Upol>So at the at the technical level, it helps you calibrate the trust on the

1061
01:04:56.910 --> 01:05:00.179
<v Upol>air right and the decision making level.

1062
01:05:00.180 --> 01:05:05.099
<v Upol>It can foster a sense of confidence and a decision making resilience

1063
01:05:05.100 --> 01:05:06.839
<v Upol>that you know. How good are you?

1064
01:05:06.840 --> 01:05:10.859
<v Upol>Can you trust AI? Can you not trust and self confidence, right?

1065
01:05:10.860 --> 01:05:15.059
<v Upol>Yes, these difference between like, do I trust the AI versus do I trust

1066
01:05:15.060 --> 01:05:17.109
<v Upol>myself to act on that?

1067
01:05:17.110 --> 01:05:22.139
<v Upol>And I think those two are slightly different constructs and organizationally

1068
01:05:22.140 --> 01:05:22.679
<v Upol>leases.

1069
01:05:22.680 --> 01:05:27.509
<v Upol>So for them to use is capturing these tacit knowledge and the meta knowledge

1070
01:05:27.510 --> 01:05:31.409
<v Upol>of how an organization will work. So you get an understanding of norms and

1071
01:05:31.410 --> 01:05:33.329
<v Upol>values, right?

1072
01:05:33.330 --> 01:05:37.769
<v Upol>It kind of encodes a level of institutional memory, and

1073
01:05:37.770 --> 01:05:41.129
<v Upol>it promotes accountability because you can audit it, right?

1074
01:05:41.130 --> 01:05:45.420
<v Upol>If you know who did what, when and why you can go back and audit things.

1075
01:05:46.470 --> 01:05:50.639
<v Upol>So those are some of the things that we got out of this that are helpful

1076
01:05:50.640 --> 01:05:53.969
<v Upol>when it comes to making the AI powered decision making.

1077
01:05:53.970 --> 01:05:58.139
<v Andrey>Yeah, it's quite interesting this notion of decision making and organizational

1078
01:05:58.140 --> 01:06:02.189
<v Andrey>context. I think you define decision as sort of,

1079
01:06:02.190 --> 01:06:04.559
<v Andrey>you know, localized to a decision.

1080
01:06:04.560 --> 01:06:08.489
<v Andrey>So like, you know, you choose a price quarter, you you think about similar

1081
01:06:08.490 --> 01:06:12.429
<v Andrey>price quotas. Organization context is something that's

1082
01:06:12.430 --> 01:06:16.409
<v Andrey>easier to forget, but it's sort of, you know, what do we

1083
01:06:16.410 --> 01:06:20.049
<v Andrey>stand for? You know, how aggressive are we?

1084
01:06:20.050 --> 01:06:25.259
<v Andrey>You know, these sorts of things that are or in general and

1085
01:06:25.260 --> 01:06:29.549
<v Andrey>yeah, pretty interesting to. I think and I guess you came to

1086
01:06:29.550 --> 01:06:34.139
<v Andrey>understanding this sort of split by just seeing what people

1087
01:06:34.140 --> 01:06:37.739
<v Andrey>used or included in their four W's.

1088
01:06:37.740 --> 01:06:40.349
<v Upol>Yes, I think this came back from those workshops.

1089
01:06:40.350 --> 01:06:43.109
<v Upol>I think where we kind of understand like, OK, because, you know, we were

1090
01:06:43.110 --> 01:06:47.219
<v Upol>thinking, maybe there is an h like how maybe there is another W

1091
01:06:47.220 --> 01:06:50.400
<v Upol>like where. So we were trying to understand

1092
01:06:51.450 --> 01:06:55.499
<v Upol>what would be the minimum viable product, so to speak about in

1093
01:06:55.500 --> 01:06:58.379
<v Upol>the in the social transparency, because we didn't also want to overwhelm

1094
01:06:58.380 --> 01:07:03.329
<v Upol>people. So the way we kind of understood like what they were doing

1095
01:07:03.330 --> 01:07:06.449
<v Upol>is actually through the case studies that we have done in addition to this

1096
01:07:06.450 --> 01:07:08.789
<v Upol>study. Right. So we were trying.

1097
01:07:08.790 --> 01:07:11.429
<v Upol>We were inspired by that aspect.

1098
01:07:11.430 --> 01:07:15.629
<v Upol>And that's why, like, you know, in the in table three, we kind of talk about,

1099
01:07:15.630 --> 01:07:17.829
<v Upol>you know what? What was it?

1100
01:07:17.830 --> 01:07:21.749
<v Upol>So it's an action taken on the API, the decision outcome.

1101
01:07:21.750 --> 01:07:25.679
<v Upol>Why is like the comments with the rationale that justifies

1102
01:07:25.680 --> 01:07:29.309
<v Upol>the decision because of what and why is always linked?

1103
01:07:29.310 --> 01:07:33.479
<v Upol>And then who the name the organizational role, because sometimes

1104
01:07:33.480 --> 01:07:38.039
<v Upol>seniority starts playing a role like it was a director.

1105
01:07:38.040 --> 01:07:42.659
<v Upol>You kind of take their their view a little more than others.

1106
01:07:42.660 --> 01:07:44.939
<v Upol>And then when is the time of decision?

1107
01:07:44.940 --> 01:07:48.209
<v Upol>And that's important because sometimes something some decisions are not

1108
01:07:48.210 --> 01:07:52.349
<v Upol>relevant, right? So think about like, you know, pre-COVID, decisions

1109
01:07:52.350 --> 01:07:54.779
<v Upol>do not become very relevant during COVID.

1110
01:07:54.780 --> 01:07:58.859
<v Upol>So those are some of the things that we found, not just by our workshops,

1111
01:07:58.860 --> 01:08:02.589
<v Upol>but also analyzing the data, the qualitative data through the interviews and

1112
01:08:02.590 --> 01:08:03.929
<v Upol>the walk throughs that we had.

1113
01:08:05.280 --> 01:08:09.509
<v Andrey>Yeah. And then you found, you know, the what is really important, the why

1114
01:08:09.510 --> 01:08:13.059
<v Andrey>is important, the when, you know, sometimes, but you know.

1115
01:08:13.060 --> 01:08:17.219
<v Andrey>Yeah. And then also, I think probably informed the UI, sort

1116
01:08:17.220 --> 01:08:19.559
<v Andrey>of how you present things.

1117
01:08:19.560 --> 01:08:21.629
<v Upol>We actually asked our participants at the end of it.

1118
01:08:21.630 --> 01:08:23.969
<v Upol>I'm like, Can you rank it and tell me why?

1119
01:08:23.970 --> 01:08:27.929
<v Upol>Right? So we would make them rank the them like, tell me what you

1120
01:08:27.930 --> 01:08:32.068
<v Upol>cannot live without and everyone saying, I can't live without the what.

1121
01:08:32.069 --> 01:08:34.259
<v Upol>And then I said, OK, imagine that I can give you one more.

1122
01:08:34.260 --> 01:08:37.108
<v Upol>What would that be like? Oh, I need another wide.

1123
01:08:37.109 --> 01:08:39.398
<v Upol>And then I said, Now imagine I give you one more.

1124
01:08:39.399 --> 01:08:41.099
<v Upol>That's I need to know the whole.

1125
01:08:41.100 --> 01:08:45.028
<v Upol>So that's how we kind of made them do this ranking task and then

1126
01:08:45.029 --> 01:08:48.869
<v Upol>get a sense of importance, because sometimes many companies might not have all

1127
01:08:48.870 --> 01:08:52.889
<v Upol>before that. There might be privacy concerns that prevent

1128
01:08:52.890 --> 01:08:54.629
<v Upol>the HU from being shot.

1129
01:08:54.630 --> 01:08:58.589
<v Upol>Right. Because you can also see, like, you know, biases creep up, like

1130
01:08:58.590 --> 01:09:02.699
<v Upol>if I show the profile picture and you know, if you can guess the person's race

1131
01:09:02.700 --> 01:09:06.989
<v Upol>or gender from the profile picture, it can create certain biased

1132
01:09:06.990 --> 01:09:09.778
<v Upol>viewpoints or even the location, right?

1133
01:09:09.779 --> 01:09:13.769
<v Upol>Because certain companies are multinational and it could be that, you

1134
01:09:13.770 --> 01:09:17.938
<v Upol>know, certain locations are not often looked positively enough.

1135
01:09:17.939 --> 01:09:21.898
<v Upol>And that's my bias, the receiver's perception.

1136
01:09:21.899 --> 01:09:22.899
<v Upol>Mm hmm.

1137
01:09:23.490 --> 01:09:27.569
<v Andrey>Yeah, exactly. And I think again, it's interesting here,

1138
01:09:28.620 --> 01:09:32.639
<v Andrey>just reading the paper, which I think is, is, you know, I would recommend it.

1139
01:09:32.640 --> 01:09:34.739
<v Andrey>I think it's quite approachable.

1140
01:09:34.740 --> 01:09:39.209
<v Andrey>Is again, you have these quotes from the study participants

1141
01:09:39.210 --> 01:09:41.278
<v Andrey>that make it very concrete.

1142
01:09:41.279 --> 01:09:45.449
<v Andrey>One of them is the outcome should be a tldr the why

1143
01:09:45.450 --> 01:09:49.648
<v Andrey>is there if I'm interested, then there's also

1144
01:09:49.649 --> 01:09:53.339
<v Andrey>the issue. Someone said if I knew for to reach out to, I could find out the

1145
01:09:53.340 --> 01:09:55.109
<v Andrey>rest of the story and so on.

1146
01:09:55.110 --> 01:09:59.729
<v Andrey>So again, it's it's really giving you a sense of how

1147
01:09:59.730 --> 01:10:03.719
<v Andrey>your study and interaction with people led you to

1148
01:10:03.720 --> 01:10:07.470
<v Andrey>your conclusions, which I really enjoyed in reading the paper.

1149
01:10:08.960 --> 01:10:12.799
<v Upol>No, you know, thank you so much for the kind words we put a lot of love into

1150
01:10:12.800 --> 01:10:13.800
<v Upol>this paper.

1151
01:10:14.440 --> 01:10:18.789
<v Andrey>Yeah. And, yeah, you know, that's what you need

1152
01:10:18.790 --> 01:10:22.149
<v Andrey>to make a paper really enjoyable, so your work pay off.

1153
01:10:23.420 --> 01:10:27.429
<v Andrey>Now I think we can touch on a lot of it and it went

1154
01:10:27.430 --> 01:10:31.059
<v Andrey>through, I think hopefully the most this stuff in terms of the study elements

1155
01:10:31.060 --> 01:10:35.709
<v Andrey>and the four W's and make clear a social transparency is

1156
01:10:35.710 --> 01:10:39.219
<v Andrey>now on to a couple final things.

1157
01:10:39.220 --> 01:10:43.359
<v Andrey>So we've said, you know, it's

1158
01:10:43.360 --> 01:10:47.709
<v Andrey>good to have this on top of what is already there.

1159
01:10:47.710 --> 01:10:49.569
<v Andrey>I'll go to make sure it's fancy.

1160
01:10:49.570 --> 01:10:53.589
<v Andrey>So you need to add the social transparency and one question there

1161
01:10:53.590 --> 01:10:57.699
<v Andrey>as well is it easy or is it a very challenge is in place

1162
01:10:57.700 --> 01:11:00.969
<v Andrey>that would make it harder to do that?

1163
01:11:00.970 --> 01:11:04.539
<v Upol>Yeah, that's a that's a good point. I think, you know, as as with everything,

1164
01:11:04.540 --> 01:11:08.739
<v Upol>there has to be the infrastructure that is supported, right?

1165
01:11:09.940 --> 01:11:12.549
<v Upol>And there are challenges like privacy.

1166
01:11:12.550 --> 01:11:16.689
<v Upol>There are challenges like biases or information overload,

1167
01:11:16.690 --> 01:11:20.889
<v Upol>as well as incentives like if you want to engage in a socially

1168
01:11:20.890 --> 01:11:25.389
<v Upol>transparent system that has to be incentive for people to

1169
01:11:25.390 --> 01:11:28.699
<v Upol>engage with it like, you know, give those four W's as they're working, that is

1170
01:11:28.700 --> 01:11:31.239
<v Upol>a burden that is added, right? Like no fees, lunches.

1171
01:11:32.290 --> 01:11:36.849
<v Upol>And so that means we have to be very mindful of that.

1172
01:11:36.850 --> 01:11:39.609
<v Upol>And you know, we can, you know, with the Ford family, you could also kind of

1173
01:11:39.610 --> 01:11:43.419
<v Upol>promote groupthink, right? Imagine in a company culture where you're not

1174
01:11:43.420 --> 01:11:46.299
<v Upol>allowed to go against your boss and you see a comment from your boss

1175
01:11:46.300 --> 01:11:49.919
<v Upol>previously. So so we have to be careful.

1176
01:11:49.920 --> 01:11:51.189
<v Upol>You know, it's not a golden bullet.

1177
01:11:52.240 --> 01:11:56.949
<v Upol>So we have to be very careful when we operationalize the social transparency

1178
01:11:56.950 --> 01:12:01.269
<v Upol>that we are trying to be very mindful of some of these challenges,

1179
01:12:01.270 --> 01:12:05.289
<v Upol>like, you know, do we really want to see all the four W's at every single time?

1180
01:12:05.290 --> 01:12:07.509
<v Upol>No, there are ways to summarize it.

1181
01:12:07.510 --> 01:12:11.349
<v Upol>And we have done that in my project with the cyber security people, we have

1182
01:12:11.350 --> 01:12:15.219
<v Upol>been able to figure out how to summarize these aspects at a level of detail

1183
01:12:15.220 --> 01:12:16.220
<v Upol>that is actionable.

1184
01:12:17.810 --> 01:12:21.759
<v Andrey>Yeah. And so speaking of cyber security,

1185
01:12:21.760 --> 01:12:26.049
<v Andrey>people say, take it outside the study, I was interacting

1186
01:12:26.050 --> 01:12:28.839
<v Andrey>with these participants and,

1187
01:12:30.130 --> 01:12:33.039
<v Andrey>you know, figuring out the of context.

1188
01:12:33.040 --> 01:12:37.029
<v Andrey>You also took this for context to an actual

1189
01:12:37.030 --> 01:12:38.979
<v Andrey>organization and then tried it out.

1190
01:12:38.980 --> 01:12:43.449
<v Upol>Is that right? Yeah. So like if you remember just from a timeline perspective,

1191
01:12:43.450 --> 01:12:47.949
<v Upol>right? So by the time I think we wrote the paper we already

1192
01:12:47.950 --> 01:12:50.019
<v Upol>had. This is obviously the study.

1193
01:12:50.020 --> 01:12:51.799
<v Upol>So there was an empirical study that was done.

1194
01:12:51.800 --> 01:12:56.019
<v Upol>Separate from this in parallel was the cyber security project

1195
01:12:56.020 --> 01:12:58.329
<v Upol>that I was running for a long, long time.

1196
01:12:58.330 --> 01:13:02.289
<v Upol>And what I had the, I guess, the luxury of knowing the future to some

1197
01:13:02.290 --> 01:13:06.369
<v Upol>extent is we were able to incorporate a lot of these four ws into

1198
01:13:06.370 --> 01:13:10.329
<v Upol>their system and they lived in a socially transparent world

1199
01:13:10.330 --> 01:13:12.219
<v Upol>when we wrote this paper.

1200
01:13:12.220 --> 01:13:16.899
<v Upol>So that's why we were able to talk a lot about these transfer cases challenges

1201
01:13:16.900 --> 01:13:20.229
<v Upol>because those are some of the challenges we faced in the real world when we

1202
01:13:20.230 --> 01:13:24.969
<v Upol>were trying to implement this in an enterprise setting that is multinational.

1203
01:13:24.970 --> 01:13:29.019
<v Andrey>I see. So when you presented this and sort of said, we should do

1204
01:13:29.020 --> 01:13:33.129
<v Andrey>this, you know how receptive our people today sort of get

1205
01:13:33.130 --> 01:13:34.539
<v Andrey>it right away or

1206
01:13:34.540 --> 01:13:38.529
<v Upol>initially there was a little bit of like hesitation, I think,

1207
01:13:38.530 --> 01:13:41.109
<v Upol>because someone said, like, how is this explainability,

1208
01:13:43.180 --> 01:13:47.649
<v Upol>right? Because there is this and there is a very powerful like AI developer

1209
01:13:47.650 --> 01:13:50.499
<v Upol>like this is not explainability. And I think that's kind of like the idea of

1210
01:13:50.500 --> 01:13:53.289
<v Upol>the paper kind of came to light.

1211
01:13:53.290 --> 01:13:57.549
<v Upol>Our idea of explainability is so narrow that

1212
01:13:57.550 --> 01:14:01.629
<v Upol>we have a hard time kind of even envisioning more than

1213
01:14:01.630 --> 01:14:05.919
<v Upol>that. So what we actually did to kind of address those kind of concerns

1214
01:14:05.920 --> 01:14:09.399
<v Upol>is, you know, as we see as you saw also on the paper in this empirical study

1215
01:14:09.400 --> 01:14:13.479
<v Upol>that we had concrete like

1216
01:14:13.480 --> 01:14:17.619
<v Upol>directly from the stakeholder information about how these

1217
01:14:17.620 --> 01:14:21.939
<v Upol>additional context help them understand the system, right?

1218
01:14:21.940 --> 01:14:25.419
<v Upol>And then if we go back to our initial definition of explainability rights,

1219
01:14:25.420 --> 01:14:28.359
<v Upol>things that helped me understand the AI systems, right?

1220
01:14:28.360 --> 01:14:30.969
<v Upol>And in this case, the AI systems are not algorithm.

1221
01:14:30.970 --> 01:14:32.919
<v Upol>These are human AI assemblages.

1222
01:14:32.920 --> 01:14:36.549
<v Upol>Right? So and they're socio technically situated.

1223
01:14:36.550 --> 01:14:40.689
<v Upol>So there you go. So initially, there was a lot of

1224
01:14:40.690 --> 01:14:43.989
<v Upol>pushback, but what the proof is often in the pudding.

1225
01:14:43.990 --> 01:14:48.339
<v Upol>So when we added social transparency, the engagement went from like

1226
01:14:48.340 --> 01:14:50.639
<v Upol>two percent to ninety six percent.

1227
01:14:50.640 --> 01:14:53.549
<v Upol>Right? That you can't ignore.

1228
01:14:53.550 --> 01:14:57.399
<v Upol>Right. And so so those are some of the things that helped a lot of the

1229
01:14:57.400 --> 01:15:01.329
<v Upol>stakeholders have more buy in and get a sense of,

1230
01:15:01.330 --> 01:15:05.259
<v Upol>OK, now this is important. This might not look algorithmic, but it

1231
01:15:05.260 --> 01:15:08.069
<v Upol>has everything to do with the algorithm, right?

1232
01:15:09.130 --> 01:15:12.429
<v Andrey>Yeah, I guess it harkens back to the title of a work, right?

1233
01:15:12.430 --> 01:15:16.419
<v Andrey>Expanding explainability, you know, as person said, how is

1234
01:15:16.420 --> 01:15:19.989
<v Andrey>this explainbaility while you've pointed out, then you sort of make the

1235
01:15:19.990 --> 01:15:24.849
<v Andrey>argument that this should be part of expandability, and

1236
01:15:24.850 --> 01:15:29.829
<v Andrey>by adding it, you get sort of a more holistic, full understanding.

1237
01:15:29.830 --> 01:15:32.199
<v Andrey>Is that kind of a fair characterization?

1238
01:15:32.200 --> 01:15:36.399
<v Upol>Yeah, yeah. And I think, you know, sometimes the simplicity is kind

1239
01:15:36.400 --> 01:15:38.709
<v Upol>of elusive and deceptive.

1240
01:15:38.710 --> 01:15:42.789
<v Upol>But, you know, we also have to understand that sometimes very powerful

1241
01:15:42.790 --> 01:15:44.739
<v Upol>ideas and also very simple ideals.

1242
01:15:44.740 --> 01:15:48.669
<v Upol>And I think within AI, we have to kind of go back to those roots at

1243
01:15:48.670 --> 01:15:51.579
<v Upol>some point, like not everything that is complex is good.

1244
01:15:51.580 --> 01:15:54.639
<v Upol>Neither is not everything that is simple is bad.

1245
01:15:54.640 --> 01:15:57.219
<v Upol>You can have very good ideas that are very simple.

1246
01:15:57.220 --> 01:15:59.919
<v Andrey>Yeah, exactly. Simple ideas can be very powerful.

1247
01:15:59.920 --> 01:16:03.849
<v Andrey>And I guess one of the key insights here is social transparency

1248
01:16:03.850 --> 01:16:08.289
<v Andrey>as a concept and as something that needs to be part of expandability.

1249
01:16:08.290 --> 01:16:12.759
<v Andrey>So just to go back and situate within the XAI

1250
01:16:12.760 --> 01:16:15.789
<v Andrey>research field, you know,

1251
01:16:16.810 --> 01:16:20.559
<v Andrey>I don't know too much about the context of that field and what is going on

1252
01:16:20.560 --> 01:16:25.119
<v Andrey>there. So what do you think could be hopefully, I guess, the impact

1253
01:16:25.120 --> 01:16:30.039
<v Andrey>and what this could enable as far as future research?

1254
01:16:30.040 --> 01:16:34.059
<v Upol>First of all, I think it makes this very nebulous topic of socio

1255
01:16:34.060 --> 01:16:37.099
<v Upol>organizational context tractable, right?

1256
01:16:37.100 --> 01:16:41.679
<v Upol>Like for concrete things to go for, and that's a good starting point.

1257
01:16:41.680 --> 01:16:44.709
<v Upol>It gives people to grasp on to that and build on it.

1258
01:16:44.710 --> 01:16:49.269
<v Upol>And I think that's what we actually invite people to do right is

1259
01:16:49.270 --> 01:16:52.659
<v Upol>now that we have at least started the conversation.

1260
01:16:52.660 --> 01:16:57.279
<v Upol>That explainability is beyond algorithmic transparency

1261
01:16:57.280 --> 01:17:01.239
<v Upol>and given the community one way of capturing

1262
01:17:01.240 --> 01:17:05.560
<v Upol>the socio organizational context, I think now it starts to seed

1263
01:17:06.880 --> 01:17:10.689
<v Upol>more ideas. And I think there is some fascinating paper that I've seen after

1264
01:17:10.690 --> 01:17:14.949
<v Upol>that around and ideas actually

1265
01:17:14.950 --> 01:17:15.859
<v Upol>that talk.

1266
01:17:15.860 --> 01:17:19.909
<v Upol>Using this notion of social transparency talked about end to end lifecycle

1267
01:17:19.910 --> 01:17:23.959
<v Upol>perspectives within explainability, like who needs to know what, when

1268
01:17:23.960 --> 01:17:27.709
<v Upol>and why, like sheep and ocher and Christine Wolfe and others have kind of

1269
01:17:27.710 --> 01:17:29.499
<v Upol>written about it. So I didn't get it.

1270
01:17:29.500 --> 01:17:33.709
<v Upol>It gives us bedrock for future work to kind of build

1271
01:17:33.710 --> 01:17:38.359
<v Upol>on it, and I hope it does, and I'll work within explainability

1272
01:17:38.360 --> 01:17:41.239
<v Upol>takes far beyond social transparency.

1273
01:17:41.240 --> 01:17:45.019
<v Upol>There are other things that are outside the box that also need to be included.

1274
01:17:45.020 --> 01:17:48.829
<v Upol>And how do we encode that? I hope people use this kind of scenario, these

1275
01:17:48.830 --> 01:17:52.729
<v Upol>design techniques, and it is also not shy away from the fact that if something

1276
01:17:52.730 --> 01:17:57.049
<v Upol>as simple, right, as long as powerful, that's still a valid

1277
01:17:57.050 --> 01:17:58.130
<v Upol>and good contribution.

1278
01:17:59.140 --> 01:18:02.919
<v Andrey>Yeah, I guess in a sense, that's how you want research to work, someone reads a

1279
01:18:02.920 --> 01:18:07.419
<v Andrey>paper and it's like, Wow, this is cool, but what if he did this or

1280
01:18:07.420 --> 01:18:10.539
<v Andrey>this thing doesn't work? You know, I have this idea.

1281
01:18:10.540 --> 01:18:12.639
<v Andrey>So that makes a lot of sense.

1282
01:18:12.640 --> 01:18:17.049
<v Andrey>And also to that notion of sort of the context and the field itself,

1283
01:18:17.050 --> 01:18:21.249
<v Andrey>we talked about on a bit of a push back, it got at the

1284
01:18:21.250 --> 01:18:24.489
<v Andrey>industry level within the research community.

1285
01:18:24.490 --> 01:18:28.419
<v Andrey>You know, when you submitted it, when you got reviews, when you presented it,

1286
01:18:28.420 --> 01:18:31.149
<v Andrey>what was the reception of your colleagues?

1287
01:18:32.230 --> 01:18:33.999
<v Upol>I think it was surprising to us.

1288
01:18:34.000 --> 01:18:37.779
<v Upol>We always thought when we wrote the paper that people either hate it or they

1289
01:18:37.780 --> 01:18:41.709
<v Upol>will love it. I don't think anyone who's going to be neutral to

1290
01:18:41.710 --> 01:18:45.309
<v Upol>it because it was making a very provocative argument.

1291
01:18:45.310 --> 01:18:49.419
<v Upol>It was making the argument that explainability is not transgressive.

1292
01:18:49.420 --> 01:18:52.869
<v Upol>It is more than that. And it's not just saying that it's like this one way of

1293
01:18:52.870 --> 01:18:57.729
<v Upol>doing it. So and clearly it was well-received,

1294
01:18:57.730 --> 01:19:01.270
<v Upol>and the presentation at Chi went very well.

1295
01:19:02.830 --> 01:19:06.459
<v Upol>And, you know, we were very lucky to receive this paper, honorable mention on

1296
01:19:06.460 --> 01:19:07.460
<v Upol>it as well.

1297
01:19:08.170 --> 01:19:13.009
<v Upol>So I think overall, it went better than we expected it, to be honest.

1298
01:19:13.010 --> 01:19:16.989
<v Andrey>Yeah, it's good to hear that given again, this

1299
01:19:16.990 --> 01:19:20.210
<v Andrey>was it looks to be quite the effort.

1300
01:19:20.211 --> 01:19:22.269
<v Andrey>CHI is pretty big, right?

1301
01:19:22.270 --> 01:19:24.969
<v Upol>Yeah, it is the premier HCI conference.

1302
01:19:24.970 --> 01:19:28.659
<v Upol>So like, not like nervous for now because Neurips at a different scale, but

1303
01:19:28.660 --> 01:19:30.849
<v Upol>like in terms of like the premiere venue.

1304
01:19:30.850 --> 01:19:34.989
<v Upol>Right CHI, is that for HCI, what NEURIPS is like for ML.no,

1305
01:19:34.990 --> 01:19:36.969
<v Upol>I guess that's a different way of looking at it.

1306
01:19:36.970 --> 01:19:39.309
<v Andrey>Well, so yeah, that's really cool.

1307
01:19:39.310 --> 01:19:43.239
<v Andrey>And we'll have a link to that paper again and a description

1308
01:19:43.240 --> 01:19:47.379
<v Andrey>and our Substack. So if you do want to get more into it,

1309
01:19:47.380 --> 01:19:49.569
<v Andrey>you can just click and read it.

1310
01:19:49.570 --> 01:19:53.709
<v Andrey>And that's just to touch on a bit what has happened

1311
01:19:53.710 --> 01:19:58.659
<v Andrey>since. In your research, you had actually a couple of weeks.

1312
01:19:58.660 --> 01:20:03.639
<v Andrey>So first up, you have the WHO and explainable AI

1313
01:20:03.640 --> 01:20:07.179
<v Andrey>how AI background shapes perceptions of AI explanations.

1314
01:20:07.180 --> 01:20:10.869
<v Andrey>How does that relate to your prior work and endless work?

1315
01:20:10.870 --> 01:20:13.779
<v Andrey>And sort of what what was what is it?

1316
01:20:13.780 --> 01:20:17.169
<v Upol>Absolutely. So I mean, this is directly related to a human centered,

1317
01:20:17.170 --> 01:20:21.369
<v Upol>explainable way. I kind of work in the sense that not all

1318
01:20:21.370 --> 01:20:25.599
<v Upol>humans are the same when it comes to interacting with the AI system.

1319
01:20:25.600 --> 01:20:28.929
<v Upol>I don't think anyone will challenge that observation.

1320
01:20:28.930 --> 01:20:33.969
<v Upol>Right. But then the question becomes, OK, who are these people?

1321
01:20:33.970 --> 01:20:38.469
<v Upol>How do their different views or characteristics impact how they interpret

1322
01:20:38.470 --> 01:20:42.639
<v Upol>explanations? So in this paper, it's just something

1323
01:20:42.640 --> 01:20:46.509
<v Upol>that we looked at like a very critical dimension, which is any AI background.

1324
01:20:46.510 --> 01:20:50.349
<v Upol>Like if you think about consumers of AI technology versus creators of AI

1325
01:20:50.350 --> 01:20:54.789
<v Upol>technology, oftentimes consumers don't have the level of AI background

1326
01:20:54.790 --> 01:20:57.219
<v Upol>that the creators have, right?

1327
01:20:57.220 --> 01:21:01.269
<v Upol>So given that this background is a consequential dimension, but also the fact

1328
01:21:01.270 --> 01:21:05.469
<v Upol>that it might be absent in the users of systems that we build.

1329
01:21:05.470 --> 01:21:10.119
<v Upol>How does that background actually impact the perceptions

1330
01:21:10.120 --> 01:21:14.089
<v Upol>of these AI explanations, right? Because again, we're making the explanations

1331
01:21:14.090 --> 01:21:18.489
<v Upol>also for the receiver explaining that and then they explain that

1332
01:21:18.490 --> 01:21:21.819
<v Upol>so that this is the paper that is, I think, the first paper that kind of

1333
01:21:21.820 --> 01:21:25.749
<v Upol>explores the AI background as

1334
01:21:25.750 --> 01:21:29.739
<v Upol>there's a dimension to to see like, well, how does that impact

1335
01:21:29.740 --> 01:21:32.469
<v Upol>like we see humans, humans, but who are these humans?

1336
01:21:32.470 --> 01:21:36.639
<v Upol>Well, let's look at two two groups of humans like people with

1337
01:21:36.640 --> 01:21:41.199
<v Upol>and people without. So this paper kind of presents a study

1338
01:21:41.200 --> 01:21:45.579
<v Upol>based largely actually on the Frogger work now way back when

1339
01:21:45.580 --> 01:21:47.499
<v Upol>to kind of get at these questions.

1340
01:21:48.640 --> 01:21:53.049
<v Andrey>Yeah, it makes me think also, aside from like,

1341
01:21:53.050 --> 01:21:57.159
<v Andrey>you know, I develop or not add it all up, or even just like programmer

1342
01:21:57.160 --> 01:22:00.559
<v Andrey>who were and resources a person in sales.

1343
01:22:00.560 --> 01:22:05.319
<v Andrey>You might interact with the AI system differently, so it seems

1344
01:22:05.320 --> 01:22:07.449
<v Andrey>no good to take into account, for sure.

1345
01:22:07.450 --> 01:22:08.450
<v Andrey>Yeah.

1346
01:22:09.190 --> 01:22:13.509
<v Andrey>And then I think also you had this

1347
01:22:13.510 --> 01:22:17.679
<v Andrey>elevating explainability pitfalls beyond dark patterns

1348
01:22:17.680 --> 01:22:21.579
<v Andrey>and explainable AI, which sounds a little bit exciting.

1349
01:22:22.590 --> 01:22:24.339
<v Andrey>So, yeah, what's that about?

1350
01:22:24.340 --> 01:22:28.509
<v Upol>So this paper is actually related to the WHO in my paper, because one

1351
01:22:28.510 --> 01:22:32.859
<v Upol>of the findings in WHO and say that we got was we're both

1352
01:22:32.860 --> 01:22:37.269
<v Upol>groups. The group with AI and NONYE backgrounds

1353
01:22:37.270 --> 01:22:41.259
<v Upol>had exhibited unwarranted faith in

1354
01:22:41.260 --> 01:22:45.549
<v Upol>numerical based explanations that had

1355
01:22:45.550 --> 01:22:49.569
<v Upol>no meaning

1356
01:22:49.570 --> 01:22:52.989
<v Upol>behind them, so to speak. But even if people did not understand what the

1357
01:22:52.990 --> 01:22:57.219
<v Upol>numbers meant, there was a level of over trust in them.

1358
01:22:57.220 --> 01:23:01.149
<v Upol>So based on. That observation, what is interesting is like we were not trying

1359
01:23:01.150 --> 01:23:04.629
<v Upol>to trick anyone, right? Like that's the importance of this finding that in the

1360
01:23:04.630 --> 01:23:06.379
<v Upol>study, we were not trying to trick anyone.

1361
01:23:06.380 --> 01:23:09.399
<v Upol>We just use the numerical explanations as a baseline.

1362
01:23:09.400 --> 01:23:13.750
<v Upol>Our main instrument was the textual explanations, the actual rationale.

1363
01:23:15.130 --> 01:23:19.149
<v Upol>And while trying to examine that, we were like, Oh my God, why are people

1364
01:23:19.150 --> 01:23:23.349
<v Upol>like so in love with these numbers, then that they don't understand?

1365
01:23:23.350 --> 01:23:27.879
<v Upol>Because we have qualitative data where they tell us, I don't understand it now,

1366
01:23:27.880 --> 01:23:30.039
<v Upol>but I can understand it later.

1367
01:23:30.040 --> 01:23:34.059
<v Upol>And what is interesting is that people with AI background

1368
01:23:34.060 --> 01:23:38.319
<v Upol>and those without. Have different results

1369
01:23:38.320 --> 01:23:41.349
<v Upol>for over trusting the AI, right?

1370
01:23:41.350 --> 01:23:43.659
<v Upol>So over trusting the numbers. Excuse me.

1371
01:23:43.660 --> 01:23:45.849
<v Upol>Yeah. So we started asking the questions.

1372
01:23:45.850 --> 01:23:50.199
<v Upol>All right. There are many times where harmful effects

1373
01:23:50.200 --> 01:23:54.189
<v Upol>can happen, like over trust, even when

1374
01:23:54.190 --> 01:23:57.969
<v Upol>best of intentions are there, like in our case.

1375
01:23:57.970 --> 01:24:02.139
<v Upol>Right. A lot of harmful work and explainable AI is couched

1376
01:24:02.140 --> 01:24:06.519
<v Upol>under this term called dark patterns, which are basically deceptive practices.

1377
01:24:06.520 --> 01:24:09.819
<v Upol>It's easiest to explain it from the other side, like if you think about like,

1378
01:24:09.820 --> 01:24:14.079
<v Upol>you know, in certain websites, they have all these like like transparent

1379
01:24:14.080 --> 01:24:18.099
<v Upol>like ads. And when you're trying to click the play button like 10000

1380
01:24:18.100 --> 01:24:21.639
<v Upol>windows, open up, right? And you have to take them 10000 politicians to get it.

1381
01:24:21.640 --> 01:24:26.859
<v Upol>So there's a dark side that kind of drives clicks by tricking the user,

1382
01:24:26.860 --> 01:24:31.119
<v Upol>you know, not all harm patterns like harmful patterns are created equal.

1383
01:24:31.120 --> 01:24:35.859
<v Upol>So what happens when harmful effects emerge,

1384
01:24:35.860 --> 01:24:38.769
<v Upol>when there is no bad intention behind it?

1385
01:24:38.770 --> 01:24:42.849
<v Upol>Right, right? So to answer that question, we wrote another kind of conceptual

1386
01:24:42.850 --> 01:24:47.319
<v Upol>paper, and we call these things explainable the pitfalls.

1387
01:24:47.320 --> 01:24:51.939
<v Upol>Right? So these pitfalls are certain things that you might not intend

1388
01:24:51.940 --> 01:24:55.689
<v Upol>for bad things to happen, but like a pitfall in a real piece of like in the

1389
01:24:55.690 --> 01:24:58.659
<v Upol>real world, you might inadvertently fall into it.

1390
01:24:58.660 --> 01:25:01.479
<v Upol>Right? Because, you know, it's not like pitfalls have there to like trap

1391
01:25:01.480 --> 01:25:05.439
<v Upol>people. Sometimes the pitfalls emerge in nature, in jungles and

1392
01:25:05.440 --> 01:25:09.549
<v Upol>other places by the construction site, and that you might

1393
01:25:09.550 --> 01:25:13.509
<v Upol>inadvertently fall into it. So this paper is kind of trying to articulate

1394
01:25:13.510 --> 01:25:15.819
<v Upol>what are explainability pitfalls?

1395
01:25:15.820 --> 01:25:17.559
<v Upol>How do you address them?

1396
01:25:17.560 --> 01:25:19.869
<v Upol>What are some of the strategies to mitigate them?

1397
01:25:19.870 --> 01:25:23.709
<v Upol>So this is more of another kind of a conceptual paper situated with a case

1398
01:25:23.710 --> 01:25:27.649
<v Upol>study, and it recently got into the human centered

1399
01:25:27.650 --> 01:25:31.119
<v Upol>AI workshop at in Europe. So this year, so we are looking forward to sharing it

1400
01:25:31.120 --> 01:25:32.509
<v Upol>with the community as well.

1401
01:25:32.510 --> 01:25:36.389
<v Andrey>Oh, it's exciting. Yeah, that's roughly in a moment, right?

1402
01:25:36.390 --> 01:25:39.529
<v Andrey>Yeah, yeah. Yeah, that's that's interesting.

1403
01:25:39.530 --> 01:25:43.759
<v Andrey>This concept of sheer is something you should avoid doing

1404
01:25:43.760 --> 01:25:48.489
<v Andrey>seems like a good idea, almost publishing negative results,

1405
01:25:48.490 --> 01:25:50.229
<v Andrey>which is which is fun.

1406
01:25:51.820 --> 01:25:56.290
<v Andrey>Well, we went for a lot of your work and then almost traced

1407
01:25:57.310 --> 01:25:59.049
<v Andrey>from the beginning to the present.

1408
01:25:59.050 --> 01:26:03.099
<v Andrey>But of course, it's also important again, to mention, as you have done before,

1409
01:26:03.100 --> 01:26:07.479
<v Andrey>that this was, you know, a lot of this was done with many collaborators

1410
01:26:07.480 --> 01:26:11.499
<v Andrey>and you built on a lot of prior research, obviously in many

1411
01:26:11.500 --> 01:26:16.030
<v Andrey>fields. This is true of any research job because you were present,

1412
01:26:18.100 --> 01:26:21.159
<v Andrey>maybe beyond your papers.

1413
01:26:21.160 --> 01:26:25.329
<v Andrey>What kind of is the situation when it comes

1414
01:26:25.330 --> 01:26:29.519
<v Andrey>to community working on XAI, Nick's family

1415
01:26:29.520 --> 01:26:33.869
<v Andrey>AI and also human centered XAI, you know, is

1416
01:26:33.870 --> 01:26:37.359
<v Andrey>is is your being human centered or socio technical?

1417
01:26:37.360 --> 01:26:41.409
<v Andrey>Is that becoming more popular or are more people

1418
01:26:41.410 --> 01:26:43.839
<v Andrey>so aware of it, that sort of thing?

1419
01:26:43.840 --> 01:26:47.259
<v Upol>No, you're absolutely right. I think, you know, I stand in the shoulder of

1420
01:26:47.260 --> 01:26:51.249
<v Upol>giants, right? There's no two ways about it without the fantastic

1421
01:26:51.250 --> 01:26:55.180
<v Upol>people I work with. None of this work becomes reality

1422
01:26:56.330 --> 01:27:00.189
<v Upol>and the communities, and it's something that I care deeply about.

1423
01:27:00.190 --> 01:27:03.189
<v Upol>So we have been very lucky in this context.

1424
01:27:03.190 --> 01:27:07.209
<v Upol>And by 2020 one, we were able to host the

1425
01:27:07.210 --> 01:27:10.299
<v Upol>first human centered explainable AI workshop.

1426
01:27:10.300 --> 01:27:15.249
<v Upol>It was actually one of the largest attended workshops and trials during

1427
01:27:15.250 --> 01:27:19.209
<v Upol>more than 100 people came over 14 countries.

1428
01:27:20.470 --> 01:27:25.389
<v Upol>So we had a stellar group of papers.

1429
01:27:25.390 --> 01:27:30.279
<v Upol>We had a keynote from Tim Miller, an expert panel discussions.

1430
01:27:30.280 --> 01:27:32.769
<v Upol>So I think that community is still going on.

1431
01:27:32.770 --> 01:27:36.309
<v Upol>And actually, we did just propose to host the second workshop

1432
01:27:37.420 --> 01:27:41.409
<v Upol>at Chi. And I think after this, we want to take it beyond.

1433
01:27:41.410 --> 01:27:45.369
<v Upol>We want to take it down. Europe's who want to take it to triple AI, to

1434
01:27:45.370 --> 01:27:50.409
<v Upol>try to see how more can we intersect with more

1435
01:27:50.410 --> 01:27:54.729
<v Upol>other communities around HCI, like other relevant social groups, right?

1436
01:27:54.730 --> 01:27:57.559
<v Upol>The computer vision people and this people.

1437
01:27:57.560 --> 01:28:01.759
<v Upol>So. These are some things that we deeply care about, and that is something

1438
01:28:01.760 --> 01:28:06.139
<v Upol>that I would that I'm kind of like

1439
01:28:06.140 --> 01:28:07.140
<v Upol>looking forward.

1440
01:28:07.960 --> 01:28:09.939
<v Andrey>Yeah, definitely so.

1441
01:28:09.940 --> 01:28:13.929
<v Andrey>And just to get it a bit more into that, you know, what's

1442
01:28:13.930 --> 01:28:18.129
<v Andrey>next for you both in terms of this community aspect of, you know, having

1443
01:28:18.130 --> 01:28:22.059
<v Andrey>various events to let people know about

1444
01:28:22.060 --> 01:28:25.809
<v Andrey>to see you and also in terms of, I guess, where your research is headed.

1445
01:28:25.810 --> 01:28:29.679
<v Upol>Yeah, I think for me, I as I share, if there's a project that I'm doing with

1446
01:28:29.680 --> 01:28:33.549
<v Upol>radiation oncology, it's actually exploring social transparency in their world

1447
01:28:33.550 --> 01:28:36.789
<v Upol>and this has been actually a value long term engagement.

1448
01:28:36.790 --> 01:28:39.939
<v Upol>I've been working with them for more than two years now.

1449
01:28:39.940 --> 01:28:44.169
<v Upol>I've also kind of been working with the Data and Society Institute

1450
01:28:44.170 --> 01:28:47.739
<v Upol>on Algorithmic Justice Issues around the Global South.

1451
01:28:47.740 --> 01:28:51.879
<v Upol>So you know what happens when we all talk a lot about

1452
01:28:51.880 --> 01:28:56.229
<v Upol>algorithmic deployment right before deployment?

1453
01:28:56.230 --> 01:29:00.919
<v Upol>Dataset creation? But what happens when algorithms get taken out

1454
01:29:00.920 --> 01:29:04.569
<v Upol>and what happens, then what happens when they're no longer used?

1455
01:29:04.570 --> 01:29:08.619
<v Upol>So there is a project that I'm running that has explainability component,

1456
01:29:08.620 --> 01:29:13.269
<v Upol>as well as algorithmic justice component around being creating

1457
01:29:13.270 --> 01:29:17.949
<v Upol>the algorithmic trading of the DC exams,

1458
01:29:17.950 --> 01:29:22.329
<v Upol>which are like basically international exams administered by Ofqual and

1459
01:29:22.330 --> 01:29:24.459
<v Upol>UK governing boards.

1460
01:29:24.460 --> 01:29:27.519
<v Upol>But these exams are actually administered in over one hundred and sixty

1461
01:29:27.520 --> 01:29:31.629
<v Upol>countries. So you might recall that in August of twenty

1462
01:29:31.630 --> 01:29:36.389
<v Upol>twenty, there were protests around an algorithm grading a lot of students

1463
01:29:36.390 --> 01:29:38.589
<v Upol>know. While the reporting was great.

1464
01:29:38.590 --> 01:29:40.299
<v Upol>It only focused on the U.K.

1465
01:29:40.300 --> 01:29:43.959
<v Upol>we really don't know what happened in the other one hundred and sixty countries

1466
01:29:43.960 --> 01:29:46.059
<v Upol>where these exams were administered.

1467
01:29:46.060 --> 01:29:50.409
<v Upol>So, you know, beyond, you know, as I say, denies you kindly shared my bio,

1468
01:29:50.410 --> 01:29:53.749
<v Upol>right? What happens to the people who are not on the table?

1469
01:29:53.750 --> 01:29:57.799
<v Upol>And I think if you don't amplify people's voices,

1470
01:29:57.800 --> 01:30:01.149
<v Upol>we're not at the table, they often end up on the menu.

1471
01:30:01.150 --> 01:30:05.229
<v Upol>So I think coming for the circle like that, something that I'm deeply curious

1472
01:30:05.230 --> 01:30:09.329
<v Upol>about, so that's roughly like, you know what things are and if

1473
01:30:09.330 --> 01:30:12.969
<v Upol>I have the privilege of giving a keynote at the World Usability Day actually

1474
01:30:12.970 --> 01:30:17.139
<v Upol>tomorrow on November 11th, I have some invited talks lined up at the University

1475
01:30:17.140 --> 01:30:21.309
<v Upol>of Buffalo on the 30th and then an expert panel discussion

1476
01:30:21.310 --> 01:30:25.419
<v Upol>actually at the university's medical school, the Stanford Medical

1477
01:30:25.420 --> 01:30:27.909
<v Upol>School, to the conference.

1478
01:30:27.910 --> 01:30:31.899
<v Upol>So that's that's pretty much like like a ramp up to the end of the year.

1479
01:30:31.900 --> 01:30:35.919
<v Andrey>Cool, yeah. Sadly, will release as five guests

1480
01:30:35.920 --> 01:30:37.029
<v Andrey>pass through 11.

1481
01:30:38.230 --> 01:30:42.069
<v Andrey>But will these talks be recorded or public?

1482
01:30:42.070 --> 01:30:42.769
<v Andrey>Could be.

1483
01:30:42.770 --> 01:30:45.159
<v Upol>That's a very good point. Thank you so much for asking.

1484
01:30:45.160 --> 01:30:46.979
<v Upol>So I am going to check it.

1485
01:30:46.980 --> 01:30:50.439
<v Upol>I wonder what I would recommend if the listeners are there.

1486
01:30:50.440 --> 01:30:54.549
<v Upol>If you check out my Twitter, if they are public, I will be sure

1487
01:30:54.550 --> 01:30:58.599
<v Upol>to make sure that they are published and shared widely.

1488
01:30:58.600 --> 01:31:02.650
<v Upol>So as of now, I'm not sure which of these would be public versus not.

1489
01:31:03.670 --> 01:31:07.089
<v Upol>But if they are, I will publish them on my Twitter.

1490
01:31:07.090 --> 01:31:10.689
<v Upol>So if people are interested and I think we can also add links to them

1491
01:31:11.800 --> 01:31:12.870
<v Upol>after the podcast.

1492
01:31:13.930 --> 01:31:16.449
<v Andrey>Exactly. Yeah. So you can look down that description.

1493
01:31:16.450 --> 01:31:20.829
<v Andrey>We'll figure it out and we'll have links to

1494
01:31:20.830 --> 01:31:22.929
<v Andrey>this and all papers and everything.

1495
01:31:24.100 --> 01:31:26.379
<v Andrey>All right. So that's cool.

1496
01:31:26.380 --> 01:31:30.309
<v Andrey>And then as I like to wrap up, after all this intense

1497
01:31:30.310 --> 01:31:34.749
<v Andrey>discussion of research and ideas and studies, just,

1498
01:31:34.750 --> 01:31:38.679
<v Andrey>you know, a little bit about you and not your research.

1499
01:31:38.680 --> 01:31:40.839
<v Andrey>What do you do these days?

1500
01:31:40.840 --> 01:31:44.979
<v Andrey>Or, you know, in general, beyond

1501
01:31:44.980 --> 01:31:46.929
<v Andrey>research, what are your main hobbies?

1502
01:31:46.930 --> 01:31:48.699
<v Andrey>What are your main interests?

1503
01:31:48.700 --> 01:31:52.569
<v Upol>Yeah, I guess I'm, you know, I've been I love to cook.

1504
01:31:52.570 --> 01:31:56.499
<v Upol>I think that is something that has been during the

1505
01:31:56.500 --> 01:32:00.069
<v Upol>stay at home and pandemic mode has been a blessing.

1506
01:32:01.180 --> 01:32:05.959
<v Upol>I absolutely love European football or soccer.

1507
01:32:05.960 --> 01:32:07.959
<v Upol>All my team is not doing very well.

1508
01:32:07.960 --> 01:32:11.229
<v Upol>Manchester United right now, but I tend to.

1509
01:32:11.230 --> 01:32:15.519
<v Upol>That is my escape and I also play this game called

1510
01:32:15.520 --> 01:32:17.319
<v Upol>Football Manager.

1511
01:32:17.320 --> 01:32:21.489
<v Upol>I have not like fantasy football, but it's like kind of like that

1512
01:32:21.490 --> 01:32:23.259
<v Upol>where it's a very data driven engine.

1513
01:32:24.850 --> 01:32:27.579
<v Upol>And that's how it comes up to

1514
01:32:28.960 --> 01:32:31.959
<v Upol>like this game engine that kind of predicts the future.

1515
01:32:31.960 --> 01:32:33.759
<v Upol>I'm going to simulate games.

1516
01:32:33.760 --> 01:32:38.979
<v Upol>That is my escape in terms of all the things in reality.

1517
01:32:38.980 --> 01:32:43.289
<v Upol>But I absolutely a big fan of old school hip hop.

1518
01:32:43.290 --> 01:32:45.279
<v Upol>So I listen to a lot of music.

1519
01:32:45.280 --> 01:32:49.779
<v Upol>I, whenever I get some time, I do mix beats

1520
01:32:51.430 --> 01:32:55.119
<v Upol>on my own time for my own enjoyment.

1521
01:32:55.120 --> 01:32:59.499
<v Upol>I don't think I have a song called Account or anything now, but those are my

1522
01:32:59.500 --> 01:33:01.779
<v Upol>ways of keeping sane.

1523
01:33:01.780 --> 01:33:06.219
<v Upol>But most importantly, one of the most cherished things that I do

1524
01:33:06.220 --> 01:33:10.479
<v Upol>is mentoring young researchers, especially

1525
01:33:10.480 --> 01:33:15.399
<v Upol>who are underrepresented, especially who are from the global south.

1526
01:33:15.400 --> 01:33:19.569
<v Upol>So I'm very proud of all the mentees that have taught

1527
01:33:19.570 --> 01:33:23.649
<v Upol>me so much throughout the years, like ever since 2000.

1528
01:33:23.650 --> 01:33:27.579
<v Upol>I think 11 12, I've had the privilege of mentoring

1529
01:33:27.580 --> 01:33:32.469
<v Upol>around 100 hundred people from many different countries in Asia and Africa and

1530
01:33:32.470 --> 01:33:35.349
<v Upol>kind of guiding them through high school and those.

1531
01:33:35.350 --> 01:33:39.429
<v Upol>That is something that like gives me a lot of joy actually, like whenever

1532
01:33:39.430 --> 01:33:41.109
<v Upol>I get free time. That's actually what I do.

1533
01:33:41.110 --> 01:33:44.679
<v Upol>And during application season, it's usually gets tough because we have a lot of

1534
01:33:44.680 --> 01:33:48.849
<v Upol>requests to review applications because, you know, sometimes

1535
01:33:48.850 --> 01:33:51.279
<v Upol>as you can imagine, life like the application.

1536
01:33:51.280 --> 01:33:54.329
<v Upol>The statement of purpose is often a black box, right?

1537
01:33:54.330 --> 01:33:58.299
<v Upol>And you don't know what to write. So that is one thing that I get a lot

1538
01:33:58.300 --> 01:33:59.859
<v Upol>of joy from.

1539
01:33:59.860 --> 01:34:01.549
<v Andrey>Yeah, that's that's fantastic.

1540
01:34:01.550 --> 01:34:06.419
<v Andrey>I think we all guys share what a good deal of mentorship

1541
01:34:06.420 --> 01:34:10.899
<v Andrey>as Diaz's adviser for a reason, you know, as an assigned mentor.

1542
01:34:10.900 --> 01:34:15.279
<v Andrey>So it's it does feel nice to give back, and I have always enjoyed

1543
01:34:15.280 --> 01:34:19.449
<v Andrey>being a teaching assistant and these various things are always pretty

1544
01:34:19.450 --> 01:34:20.949
<v Andrey>rewarding for me.

1545
01:34:22.330 --> 01:34:24.999
<v Andrey>Well, that was a really fun interview.

1546
01:34:25.000 --> 01:34:29.019
<v Andrey>It was great to see or hear about

1547
01:34:29.020 --> 01:34:33.099
<v Andrey>this human centered AI as a researcher who

1548
01:34:33.100 --> 01:34:37.689
<v Andrey>talks of robots refreshing to think about people for once.

1549
01:34:37.690 --> 01:34:41.769
<v Andrey>Thank you so much for being on the podcast.

1550
01:34:41.770 --> 01:34:45.669
<v Upol>My pleasure. Thank you, Andrey. I so appreciate the opportunity to talk to you

1551
01:34:45.670 --> 01:34:48.159
<v Upol>and an animal in a way to you.

1552
01:34:48.160 --> 01:34:50.829
<v Upol>Talk to the listeners. Thank you.

1553
01:34:50.830 --> 01:34:55.269
<v Andrey>Absolutely. And once again, this is The Gradient podcast.

1554
01:34:55.270 --> 01:34:59.829
<v Andrey>Check out our magazine website at The Gradient dot

1555
01:34:59.830 --> 01:35:03.809
<v Andrey>com. To you, Earl. And our newsletter and actually this podcast at

1556
01:35:03.810 --> 01:35:08.009
<v Andrey>The Gradient pub that Substack dot com, you can support us there

1557
01:35:08.010 --> 01:35:11.999
<v Andrey>by subscribing and also share all of this review on this

1558
01:35:12.000 --> 01:35:14.879
<v Andrey>Apple and all these kinds of things.

1559
01:35:14.880 --> 01:35:18.479
<v Andrey>So if you dig this stuff, we would appreciate your support.

1560
01:35:18.480 --> 01:35:22.890
<v Andrey>Thank you so much for listening and be sure to tune into our future episodes.

