WEBVTT

1
00:00:05.400 --> 00:00:08.159
<v Andrey> Hello and welcome to the 18th episode

2
00:00:08.160 --> 00:00:10.469
<v Andrey> of The Gradient podcast,.

3
00:00:10.470 --> 00:00:13.259
<v Andrey> The Gradient is a digital magazine that aims to

4
00:00:13.260 --> 00:00:15.539
<v Andrey> be a place for discussion about research and

5
00:00:15.540 --> 00:00:18.179
<v Andrey> trends in artificial intelligence and

6
00:00:18.180 --> 00:00:20.729
<v Andrey> machine learning, we interview various people in

7
00:00:20.730 --> 00:00:23.309
<v Andrey> A.I such as engineers, researchers,

8
00:00:23.310 --> 00:00:25.079
<v Andrey> artists and more.

9
00:00:25.080 --> 00:00:27.539
<v Andrey> I'm your host Andrey Kurenkov.

10
00:00:27.540 --> 00:00:29.519
<v Andrey> In this episode, I'm excited to be interviewing

11
00:00:31.530 --> 00:00:33.779
<v Andrey> Upol Ehsan. Upol Cares about people first

12
00:00:33.780 --> 00:00:35.669
<v Andrey> technology second.

13
00:00:35.670 --> 00:00:38.159
<v Andrey> He's a doctoral candidate in the School of

14
00:00:38.160 --> 00:00:40.679
<v Andrey> Interactive Computing at Georgia Tech and

15
00:00:40.680 --> 00:00:43.469
<v Andrey> an affiliate at the Data and Society Research

16
00:00:43.470 --> 00:00:44.669
<v Andrey> Institute.

17
00:00:44.670 --> 00:00:46.889
<v Andrey> Combining his expertize in A.I.

18
00:00:46.890 --> 00:00:49.649
<v Andrey> and background and philosophy as work and

19
00:00:49.650 --> 00:00:52.649
<v Andrey> explainable, A.I., or SETI,

20
00:00:52.650 --> 00:00:55.439
<v Andrey> aims to foster a future where anyone, regardless

21
00:00:55.440 --> 00:00:58.319
<v Andrey> of our background, can use air power technology

22
00:00:58.320 --> 00:00:59.849
<v Andrey> with dignity.

23
00:00:59.850 --> 00:01:02.519
<v Andrey> Putting the human first and focusing on how our

24
00:01:02.520 --> 00:01:06.029
<v Andrey> values shape the use and abuse of technology.

25
00:01:06.030 --> 00:01:09.179
<v Andrey> His work has coined the term human centered,

26
00:01:09.180 --> 00:01:12.179
<v Andrey> explainable A.I., which is subfield

27
00:01:12.180 --> 00:01:13.829
<v Andrey> of expandable A.I.

28
00:01:13.830 --> 00:01:16.079
<v Andrey> and a charted its visions.

29
00:01:16.080 --> 00:01:18.749
<v Andrey> Actively publishing and top peer reviewed venues

30
00:01:18.750 --> 00:01:21.479
<v Andrey> like his work, has received multiple awards

31
00:01:21.480 --> 00:01:24.809
<v Andrey> and been covered in major media outlets

32
00:01:24.810 --> 00:01:26.729
<v Andrey> bringing industry academia.

33
00:01:26.730 --> 00:01:29.339
<v Andrey> He serves on multiple program committees

34
00:01:29.340 --> 00:01:32.519
<v Andrey> and A.I. and AI conferences such as

35
00:01:32.520 --> 00:01:35.369
<v Andrey> Europe's NDIS, and equally connects

36
00:01:35.370 --> 00:01:38.039
<v Andrey> these communities by

37
00:01:38.040 --> 00:01:39.809
<v Andrey> promoting equity and ethics and AI.

38
00:01:39.810 --> 00:01:42.329
<v Andrey> He wants to ensure stakeholders who aren't

39
00:01:42.330 --> 00:01:45.269
<v Andrey> at the table do not end up on

40
00:01:45.270 --> 00:01:46.270
<v Andrey> the menu.

41
00:01:46.860 --> 00:01:49.769
<v Andrey> Outside research He is an advisor for

42
00:01:49.770 --> 00:01:52.019
<v Andrey> Ilar Asia and Educational Institute.

43
00:01:52.020 --> 00:01:55.139
<v Andrey> He started for underprivileged children subjected

44
00:01:55.140 --> 00:01:57.569
<v Andrey> to child labor and Twitter.

45
00:01:57.570 --> 00:01:59.589
<v Andrey> You can follow him at at.

46
00:01:59.590 --> 00:02:02.579
<v Andrey> Upul Assan New pop

47
00:02:02.580 --> 00:02:06.389
<v Andrey> up O L E H s n,

48
00:02:06.390 --> 00:02:07.860
<v Andrey> so I'm very excited for this.

49
00:02:08.940 --> 00:02:10.499
<v Andrey> Paul has written to The Guardian before, and I

50
00:02:10.500 --> 00:02:12.509
<v Andrey> think his work is super cool.

51
00:02:12.510 --> 00:02:15.539
<v Andrey> Welcome to the podcast, Supo.

52
00:02:15.540 --> 00:02:17.129
<v Upol> Thank you for having me on.

53
00:02:17.130 --> 00:02:18.210
<v Upol> It's pleasure to be here.

54
00:02:19.290 --> 00:02:21.959
<v Andrey> Definitely. So as we usually do

55
00:02:21.960 --> 00:02:25.229
<v Andrey> when these episodes before diving into your work

56
00:02:25.230 --> 00:02:28.109
<v Andrey> a bit on your sort of background, I'm curious,

57
00:02:28.110 --> 00:02:30.809
<v Andrey> how did you get into working

58
00:02:30.810 --> 00:02:33.359
<v Andrey> on? I think your trajectory

59
00:02:33.360 --> 00:02:34.949
<v Andrey> might be interesting or your background or

60
00:02:34.950 --> 00:02:36.989
<v Andrey> philosophy as well.

61
00:02:36.990 --> 00:02:40.289
<v Upol> Yes, I think I have Isaac Asimov

62
00:02:40.290 --> 00:02:43.679
<v Upol> to kind of attribute that credit to.

63
00:02:43.680 --> 00:02:46.199
<v Upol> When I was very young, I got hooked

64
00:02:46.200 --> 00:02:48.479
<v Upol> into his books. I have read forty seven of his

65
00:02:48.480 --> 00:02:51.039
<v Upol> books, not just the science fiction that he

66
00:02:51.040 --> 00:02:51.749
<v Upol> a lot.

67
00:02:51.750 --> 00:02:52.439
<v Andrey> Yeah, yeah,

68
00:02:52.440 --> 00:02:55.199
<v Upol> I mean, the maestro is is someone who's near

69
00:02:55.200 --> 00:02:57.389
<v Upol> and dear to my heart, which makes watching

70
00:02:57.390 --> 00:03:00.149
<v Upol> foundation and Apple TV right now a very scary

71
00:03:00.150 --> 00:03:02.759
<v Upol> prospect. Oh yeah, because I remember those

72
00:03:02.760 --> 00:03:06.359
<v Upol> things, but I think Asimov

73
00:03:06.360 --> 00:03:09.479
<v Upol> pushed me to think about artificial intelligence

74
00:03:09.480 --> 00:03:12.029
<v Upol> in ways that I don't think I would have

75
00:03:12.030 --> 00:03:14.759
<v Upol> thought of, because all of his books,

76
00:03:14.760 --> 00:03:17.279
<v Upol> if you think about it, it's about how

77
00:03:17.280 --> 00:03:20.429
<v Upol> does how can we find

78
00:03:20.430 --> 00:03:23.519
<v Upol> flaws in the three laws of robotics that

79
00:03:23.520 --> 00:03:25.229
<v Upol> kind of he proposed, right?

80
00:03:26.400 --> 00:03:28.919
<v Upol> And in college, I

81
00:03:28.920 --> 00:03:31.589
<v Upol> was very so. I grew up in a philosophy department

82
00:03:31.590 --> 00:03:34.469
<v Upol> that had a lot of cognitive scientists in them,

83
00:03:34.470 --> 00:03:36.659
<v Upol> but who were teaching analytic philosophy.

84
00:03:37.680 --> 00:03:39.329
<v Upol> And that's where I actually got into.

85
00:03:39.330 --> 00:03:41.189
<v Upol> I got hooked into it.

86
00:03:41.190 --> 00:03:43.949
<v Upol> I was like, OK, and maybe initially

87
00:03:43.950 --> 00:03:46.469
<v Upol> I had more ambitious goals of

88
00:03:46.470 --> 00:03:49.019
<v Upol> creating something like AGI, so to

89
00:03:49.020 --> 00:03:51.419
<v Upol> speak. But then over time, I started getting more

90
00:03:51.420 --> 00:03:53.039
<v Upol> practical about it.

91
00:03:53.040 --> 00:03:55.649
<v Upol> And after graduating, I actually

92
00:03:55.650 --> 00:03:58.049
<v Upol> spent a lot of time doing management, consulting

93
00:03:58.050 --> 00:03:59.729
<v Upol> and then then a startup.

94
00:03:59.730 --> 00:04:02.309
<v Upol> And in those experiences, I was

95
00:04:02.310 --> 00:04:04.889
<v Upol> dealing with innovative applications,

96
00:04:04.890 --> 00:04:07.499
<v Upol> but mostly on the consumer

97
00:04:07.500 --> 00:04:10.139
<v Upol> side. So I had clients who are really using

98
00:04:10.140 --> 00:04:12.659
<v Upol> this at the enterprise level, and I was

99
00:04:12.660 --> 00:04:15.239
<v Upol> seeing how sometimes despite

100
00:04:15.240 --> 00:04:18.268
<v Upol> best intentions, the

101
00:04:18.269 --> 00:04:21.059
<v Upol> real use of these systems were suffering.

102
00:04:21.060 --> 00:04:23.849
<v Upol> So that's one way when I got into the

103
00:04:23.850 --> 00:04:26.879
<v Upol> Ph.D. journey, I started thinking of

104
00:04:26.880 --> 00:04:29.429
<v Upol> artificial intelligence, but from the human

105
00:04:29.430 --> 00:04:30.430
<v Upol> side.

106
00:04:31.200 --> 00:04:33.899
<v Andrey> Right, and this was roughly when

107
00:04:33.900 --> 00:04:34.829
<v Andrey> what year?

108
00:04:34.830 --> 00:04:37.499
<v Upol> Yeah, so I had like

109
00:04:37.500 --> 00:04:40.139
<v Upol> so it was like I started the peace Deep Learning

110
00:04:40.140 --> 00:04:44.279
<v Upol> roughly around 20 or 15 16.

111
00:04:44.280 --> 00:04:46.919
<v Upol> But the work that I had done before that was

112
00:04:46.920 --> 00:04:49.199
<v Upol> like the last four years before that, that's

113
00:04:49.200 --> 00:04:51.010
<v Upol> around like 2012 13.

114
00:04:52.230 --> 00:04:54.519
<v Upol> So that's like the industry experience very much

115
00:04:54.520 --> 00:04:58.139
<v Upol> drives a lot of my insights into the work today,

116
00:04:58.140 --> 00:05:01.229
<v Upol> especially seeing people and I do consult

117
00:05:01.230 --> 00:05:02.729
<v Upol> even now.

118
00:05:02.730 --> 00:05:05.579
<v Upol> So I'm very much into the applied setting of these

119
00:05:05.580 --> 00:05:08.339
<v Upol> research discussions, which help me kind of bridge

120
00:05:08.340 --> 00:05:11.069
<v Upol> too. That's why you'll see, even in my work,

121
00:05:11.070 --> 00:05:13.439
<v Upol> I do tend to have a more applied kind of a

122
00:05:13.440 --> 00:05:14.440
<v Upol> connotation.

123
00:05:15.150 --> 00:05:18.239
<v Andrey> Yeah, yeah. I was just wondering because I think,

124
00:05:18.240 --> 00:05:20.429
<v Andrey> you know, obviously there's been a huge boom.

125
00:05:20.430 --> 00:05:22.979
<v Andrey> I have a past decade and explainable

126
00:05:22.980 --> 00:05:25.559
<v Andrey> AI, which you know your work with and

127
00:05:25.560 --> 00:05:28.859
<v Andrey> has been more and more an

128
00:05:28.860 --> 00:05:31.469
<v Andrey> area study. But I think it took

129
00:05:31.470 --> 00:05:34.709
<v Andrey> it a little while it of catching up in some sense

130
00:05:34.710 --> 00:05:37.019
<v Andrey> as as I was getting deployed.

131
00:05:37.020 --> 00:05:39.809
<v Andrey> Yeah. And then so you started your journey in

132
00:05:39.810 --> 00:05:42.359
<v Andrey> 2015. Did you go to expand aid right

133
00:05:42.360 --> 00:05:44.939
<v Andrey> away or did it sort of did you

134
00:05:44.940 --> 00:05:47.459
<v Andrey> find your way there a bit later?

135
00:05:47.460 --> 00:05:48.719
<v Upol> That's a really great question.

136
00:05:48.720 --> 00:05:51.599
<v Upol> No, I actually started my journey doing

137
00:05:51.600 --> 00:05:54.179
<v Upol> affective computing, so I was very much

138
00:05:54.180 --> 00:05:57.149
<v Upol> interested in helping children with autism,

139
00:05:57.150 --> 00:06:00.209
<v Upol> learn about non-verbal

140
00:06:00.210 --> 00:06:02.759
<v Upol> communication to head up displays, and

141
00:06:02.760 --> 00:06:04.739
<v Upol> Google Glass was very hot back then.

142
00:06:04.740 --> 00:06:05.939
<v Upol> Oh yeah.

143
00:06:05.940 --> 00:06:07.919
<v Upol> So I was trying to develop algorithms trying to

144
00:06:07.920 --> 00:06:10.889
<v Upol> help people who had

145
00:06:10.890 --> 00:06:14.219
<v Upol> had difficulties processing social signals

146
00:06:14.220 --> 00:06:16.799
<v Upol> to use some kind of a prosthetic

147
00:06:16.800 --> 00:06:18.909
<v Upol> to kind of in that social interaction.

148
00:06:18.910 --> 00:06:20.759
<v Upol> So that's how I actually started.

149
00:06:20.760 --> 00:06:23.609
<v Upol> And then after that, I am originally

150
00:06:23.610 --> 00:06:26.189
<v Upol> from Bangladesh. So I, the global south

151
00:06:26.190 --> 00:06:28.859
<v Upol> has been very much and is still

152
00:06:28.860 --> 00:06:32.009
<v Upol> very much a core part of my existence.

153
00:06:32.010 --> 00:06:34.109
<v Upol> So after that, I started looking at how do these

154
00:06:34.110 --> 00:06:36.629
<v Upol> technologies kind of behave

155
00:06:36.630 --> 00:06:39.359
<v Upol> in the global south, where the technology

156
00:06:39.360 --> 00:06:41.100
<v Upol> is not necessarily made in?

157
00:06:42.900 --> 00:06:45.629
<v Upol> After that, I think it was

158
00:06:45.630 --> 00:06:48.509
<v Upol> in two thousand sixteen or seventeen where DARPA

159
00:06:48.510 --> 00:06:49.669
<v Upol> had that essay.

160
00:06:49.670 --> 00:06:52.859
<v Upol> I grant and

161
00:06:52.860 --> 00:06:55.379
<v Upol> that was the first time where

162
00:06:55.380 --> 00:06:56.699
<v Upol> because it's interesting, right?

163
00:06:56.700 --> 00:06:58.349
<v Upol> Like explainability of A.I.

164
00:06:58.350 --> 00:06:59.459
<v Upol> is not real.

165
00:06:59.460 --> 00:07:02.459
<v Upol> If you look at the literature in the 80s,

166
00:07:02.460 --> 00:07:04.499
<v Upol> there is a lot of work, in fact, that comics label

167
00:07:04.500 --> 00:07:06.929
<v Upol> and I was coined back in the 80s of the 90s.

168
00:07:08.130 --> 00:07:10.319
<v Upol> This was based on the knowledge, you know, the

169
00:07:10.320 --> 00:07:12.719
<v Upol> knowledge based systems like we had a second

170
00:07:12.720 --> 00:07:15.369
<v Upol> there. But with the advent of Deep

171
00:07:15.370 --> 00:07:18.029
<v Upol> Learning and Deep Learning becoming kind

172
00:07:18.030 --> 00:07:20.909
<v Upol> of enterprise level almost

173
00:07:20.910 --> 00:07:24.389
<v Upol> like coming of age, you see,

174
00:07:24.390 --> 00:07:26.939
<v Upol> then there is this need to hold these

175
00:07:26.940 --> 00:07:28.919
<v Upol> systems accountable.

176
00:07:28.920 --> 00:07:31.889
<v Upol> So I actually had

177
00:07:31.890 --> 00:07:34.619
<v Upol> walked into my advisor's office at that time

178
00:07:34.620 --> 00:07:37.139
<v Upol> and I was asking, you know, what kind

179
00:07:37.140 --> 00:07:39.479
<v Upol> of projects do we have to work on?

180
00:07:39.480 --> 00:07:42.089
<v Upol> And he said, And my advisor

181
00:07:42.090 --> 00:07:43.979
<v Upol> is fantastic microneedle.

182
00:07:43.980 --> 00:07:46.529
<v Upol> And Mark kind of said that, hey, there

183
00:07:46.530 --> 00:07:49.829
<v Upol> is this other project that no one really

184
00:07:49.830 --> 00:07:52.259
<v Upol> has taken upon themselves because we don't really

185
00:07:52.260 --> 00:07:54.899
<v Upol> know what it would look like.

186
00:07:54.900 --> 00:07:56.949
<v Upol> And I said, What is it? Is this explainable?

187
00:07:56.950 --> 00:07:59.639
<v Upol> I think until at that time, like I had not

188
00:07:59.640 --> 00:08:02.219
<v Upol> heard about the term, I was like, This sounds

189
00:08:02.220 --> 00:08:05.519
<v Upol> like interesting. And I think upon reflection,

190
00:08:05.520 --> 00:08:08.669
<v Upol> what I realized about myself is I do very well

191
00:08:08.670 --> 00:08:11.549
<v Upol> when it's so empty slate and I get to paint

192
00:08:11.550 --> 00:08:14.099
<v Upol> my own picture rather than very well

193
00:08:14.100 --> 00:08:16.679
<v Upol> form. So I was very lucky

194
00:08:16.680 --> 00:08:19.529
<v Upol> to get into that debate very

195
00:08:19.530 --> 00:08:22.679
<v Upol> early on. In the second resurgence, I would argue,

196
00:08:22.680 --> 00:08:25.379
<v Upol> because the second life I has had is, I think,

197
00:08:25.380 --> 00:08:28.024
<v Upol> much more longer t.

198
00:08:28.025 --> 00:08:30.509
<v Upol> Han the first life it had because it was there and

199
00:08:30.510 --> 00:08:33.179
<v Upol> but it also wasn't there in the early 1980s.

200
00:08:34.830 --> 00:08:37.499
<v Upol> So then I started looking

201
00:08:37.500 --> 00:08:38.999
<v Upol> into it.

202
00:08:39.000 --> 00:08:41.619
<v Upol> I started on the algorithmic side, frankly, and

203
00:08:41.620 --> 00:08:43.558
<v Upol> then trying to work with algorithms.

204
00:08:43.559 --> 00:08:46.709
<v Upol> And then over time, I got on my Human side

205
00:08:46.710 --> 00:08:48.210
<v Upol> and you are right. I think

206
00:08:49.290 --> 00:08:51.869
<v Upol> explainable AI is very much

207
00:08:51.870 --> 00:08:52.979
<v Upol> in flux.

208
00:08:52.980 --> 00:08:55.799
<v Upol> That's how I would talk about it.

209
00:08:55.800 --> 00:08:58.529
<v Upol> I think we as a community, we are still trying

210
00:08:58.530 --> 00:09:02.039
<v Upol> to figure out how to navigate

211
00:09:02.040 --> 00:09:05.099
<v Upol> this field, being consistent

212
00:09:05.100 --> 00:09:07.769
<v Upol> in our terminology in the way

213
00:09:07.770 --> 00:09:09.329
<v Upol> we do our work.

214
00:09:09.330 --> 00:09:12.359
<v Upol> But there is also a certain level of beauty

215
00:09:12.360 --> 00:09:13.439
<v Upol> in that.

216
00:09:13.440 --> 00:09:16.079
<v Upol> And in that case, I'm kind of drawn

217
00:09:16.080 --> 00:09:19.109
<v Upol> by the social construction of technology

218
00:09:19.110 --> 00:09:21.749
<v Upol> lenses, something pioneered by way,

219
00:09:21.750 --> 00:09:24.689
<v Upol> a biker, and he talked about relevant

220
00:09:24.690 --> 00:09:25.889
<v Upol> social groups.

221
00:09:25.890 --> 00:09:27.719
<v Upol> So in any piece of technology, you will have

222
00:09:27.720 --> 00:09:30.299
<v Upol> relevant social groups in that.

223
00:09:30.300 --> 00:09:32.819
<v Upol> Is why they was talking about bicycles,

224
00:09:32.820 --> 00:09:35.909
<v Upol> so bicycles have very other social groups,

225
00:09:35.910 --> 00:09:38.489
<v Upol> and each relevant social

226
00:09:38.490 --> 00:09:40.259
<v Upol> groups are these are stakeholders who have skin in

227
00:09:40.260 --> 00:09:42.809
<v Upol> the game actually give meaning to

228
00:09:42.810 --> 00:09:45.179
<v Upol> the technology as much as the technology gives

229
00:09:45.180 --> 00:09:46.379
<v Upol> meaning to the rights of.

230
00:09:46.380 --> 00:09:49.169
<v Upol> If you think about the mountain bikes and BMX

231
00:09:49.170 --> 00:09:52.259
<v Upol> bikes now, you know, like racing bikes

232
00:09:52.260 --> 00:09:53.909
<v Upol> on different bikes.

233
00:09:53.910 --> 00:09:55.859
<v Upol> And it's because of the stakeholders.

234
00:09:55.860 --> 00:09:57.629
<v Upol> They get very different, meaning all of them are

235
00:09:57.630 --> 00:09:59.759
<v Upol> bicycles, but they look very different.

236
00:09:59.760 --> 00:10:02.639
<v Upol> And I think within explainability we have people

237
00:10:02.640 --> 00:10:05.189
<v Upol> from the algorithmic side,

238
00:10:05.190 --> 00:10:07.859
<v Upol> basically the items from the

239
00:10:07.860 --> 00:10:09.479
<v Upol> HCI side.

240
00:10:09.480 --> 00:10:11.609
<v Upol> And now we are having stakeholders in the public

241
00:10:11.610 --> 00:10:14.339
<v Upol> policy side, in the regulation side,

242
00:10:14.340 --> 00:10:15.839
<v Upol> in the auditing side.

243
00:10:15.840 --> 00:10:18.059
<v Upol> So I think each of these stakeholders are also

244
00:10:18.060 --> 00:10:21.209
<v Upol> adding their own lenses to what is explainable

245
00:10:21.210 --> 00:10:23.729
<v Upol> and which is why you will see a lot

246
00:10:23.730 --> 00:10:24.730
<v Upol> of flux.

247
00:10:25.440 --> 00:10:28.319
<v Andrey> Yeah, it's super interesting seeing this field

248
00:10:28.320 --> 00:10:30.839
<v Andrey> kind of grow, and there's so much area

249
00:10:30.840 --> 00:10:32.999
<v Andrey> to cover that.

250
00:10:33.000 --> 00:10:35.789
<v Andrey> I think, you know, maybe compared

251
00:10:35.790 --> 00:10:36.869
<v Andrey> to selling computer here.

252
00:10:36.870 --> 00:10:39.629
<v Andrey> And you know, I think there's a lot more

253
00:10:39.630 --> 00:10:42.209
<v Andrey> kind of maybe foundational or at

254
00:10:42.210 --> 00:10:44.880
<v Andrey> least a conceptually

255
00:10:46.650 --> 00:10:48.569
<v Andrey> important work. And then we'll get into what I

256
00:10:48.570 --> 00:10:51.179
<v Andrey> think are yours and they could be could be called

257
00:10:51.180 --> 00:10:53.789
<v Andrey> that. Yeah, your journey is really interesting.

258
00:10:53.790 --> 00:10:56.309
<v Andrey> It's always fun to hear about how people bring

259
00:10:56.310 --> 00:10:59.019
<v Andrey> in their experience before repeatedly

260
00:10:59.020 --> 00:11:01.769
<v Andrey> and how that sort of guides their

261
00:11:01.770 --> 00:11:03.329
<v Andrey> their direction.

262
00:11:03.330 --> 00:11:05.549
<v Andrey> In my case, I started in robotics, in high school

263
00:11:05.550 --> 00:11:07.169
<v Andrey> and then, you know, I did it in college.

264
00:11:07.170 --> 00:11:09.689
<v Andrey> And then, you know, even when I went in some of

265
00:11:09.690 --> 00:11:12.299
<v Andrey> the interactions and I came back to it.

266
00:11:12.300 --> 00:11:14.969
<v Andrey> So it's it's always interesting to

267
00:11:14.970 --> 00:11:16.199
<v Andrey> see how it happens.

268
00:11:16.200 --> 00:11:18.509
<v Upol> I love that story because it's weird, right?

269
00:11:18.510 --> 00:11:21.239
<v Upol> Because I have an undergrad, I have a

270
00:11:21.240 --> 00:11:24.029
<v Upol> like a B.S. in electrical engineering and a B.A.

271
00:11:24.030 --> 00:11:25.289
<v Upol> in philosophy, right?

272
00:11:25.290 --> 00:11:27.539
<v Upol> And I never thought I would use that philosophy

273
00:11:27.540 --> 00:11:30.419
<v Upol> degree on a daily basis

274
00:11:30.420 --> 00:11:33.049
<v Upol> as much as I use it today.

275
00:11:33.050 --> 00:11:35.749
<v Upol> In fact, my edge in

276
00:11:35.750 --> 00:11:38.299
<v Upol> explainable air actually comes from my philosophy

277
00:11:38.300 --> 00:11:41.059
<v Upol> training because I can't

278
00:11:41.060 --> 00:11:43.729
<v Upol> access the writing that is coming

279
00:11:43.730 --> 00:11:45.499
<v Upol> from the air because even as academics are part of

280
00:11:45.500 --> 00:11:48.169
<v Upol> our training is how to read a certain body

281
00:11:48.170 --> 00:11:49.399
<v Upol> of work.

282
00:11:49.400 --> 00:11:51.319
<v Upol> But then when you're also trained in computer

283
00:11:51.320 --> 00:11:53.259
<v Upol> science, you can bridge it.

284
00:11:53.260 --> 00:11:55.999
<v Upol> Mm-Hmm. And I think there is something to be said

285
00:11:56.000 --> 00:11:58.249
<v Upol> there, especially for PhD student or other

286
00:11:58.250 --> 00:11:59.539
<v Upol> practitioners and researchers.

287
00:11:59.540 --> 00:12:02.389
<v Upol> Listening is I have been my mentors

288
00:12:02.390 --> 00:12:03.859
<v Upol> have always said like, you know, if you really

289
00:12:03.860 --> 00:12:06.559
<v Upol> want to make a name, pick an area and pick

290
00:12:06.560 --> 00:12:09.799
<v Upol> an Area B and then intersect them

291
00:12:09.800 --> 00:12:12.769
<v Upol> and you might actually get a C that is

292
00:12:12.770 --> 00:12:15.469
<v Upol> has a has an interesting angle to it that makes

293
00:12:15.470 --> 00:12:18.199
<v Upol> your work more relevant, more impactful.

294
00:12:18.200 --> 00:12:20.719
<v Upol> So I love also your story about robotics and how

295
00:12:20.720 --> 00:12:22.009
<v Upol> your back full circle.

296
00:12:22.010 --> 00:12:24.949
<v Upol> I think many of us in some ways are at the other

297
00:12:24.950 --> 00:12:28.459
<v Upol> end up where our interest kind of started.

298
00:12:28.460 --> 00:12:30.919
<v Andrey> Yeah, for sure. It's it's quite interesting.

299
00:12:30.920 --> 00:12:33.619
<v Andrey> You know, I worked in robotics a lot in undergrad

300
00:12:33.620 --> 00:12:36.339
<v Andrey> and I worked a lot and then were kind of classical

301
00:12:36.340 --> 00:12:38.779
<v Andrey> robotic algorithms not knowing you all nets.

302
00:12:38.780 --> 00:12:41.449
<v Andrey> And then that definitely informed by

303
00:12:41.450 --> 00:12:43.579
<v Andrey> understanding and my ability to get into it.

304
00:12:43.580 --> 00:12:46.370
<v Andrey> So always, always call to see how that happens.

305
00:12:47.750 --> 00:12:50.719
<v Andrey> So that's kind of my

306
00:12:50.720 --> 00:12:53.149
<v Andrey> introduction to how you got here out of a way.

307
00:12:53.150 --> 00:12:55.879
<v Andrey> Let's start diving into your work

308
00:12:55.880 --> 00:12:59.089
<v Andrey> will be focusing a lot on a particular

309
00:12:59.090 --> 00:13:02.179
<v Andrey> paper that I think is very cool.

310
00:13:02.180 --> 00:13:05.149
<v Andrey> But before that, let's just give the listeners

311
00:13:05.150 --> 00:13:07.789
<v Andrey> a bit of a conceptual kind of

312
00:13:07.790 --> 00:13:10.339
<v Andrey> introduction to a field, I suppose, and manage

313
00:13:10.340 --> 00:13:11.569
<v Andrey> our work.

314
00:13:11.570 --> 00:13:14.389
<v Andrey> So just

315
00:13:14.390 --> 00:13:17.449
<v Andrey> common basics, you know, quick introduction

316
00:13:17.450 --> 00:13:20.539
<v Andrey> to explain what explainability is,

317
00:13:20.540 --> 00:13:23.179
<v Andrey> maybe, you know, and that's a pretty flat surface

318
00:13:23.180 --> 00:13:25.399
<v Andrey> level and why it's important.

319
00:13:25.400 --> 00:13:27.739
<v Upol> Yeah. So let's start with why it's important and

320
00:13:27.740 --> 00:13:29.719
<v Upol> then I'll share why what it is and I think the

321
00:13:29.720 --> 00:13:31.669
<v Upol> importance of drives what it is.

322
00:13:31.670 --> 00:13:34.519
<v Upol> So with with with today, like the AI

323
00:13:34.520 --> 00:13:37.609
<v Upol> powered decision making is everywhere

324
00:13:37.610 --> 00:13:40.459
<v Upol> from radiation radiologists

325
00:13:40.460 --> 00:13:43.039
<v Upol> using AI powered decision

326
00:13:43.040 --> 00:13:45.769
<v Upol> support systems to diagnose chest COVID

327
00:13:45.770 --> 00:13:49.099
<v Upol> pneumonia on chest x rays right to

328
00:13:49.100 --> 00:13:51.979
<v Upol> loan officers using algorithms to determine

329
00:13:51.980 --> 00:13:53.510
<v Upol> if you are loan worthy or not.

330
00:13:54.590 --> 00:13:57.289
<v Upol> Do you know the recidivism cases,

331
00:13:57.290 --> 00:14:00.139
<v Upol> right? So as

332
00:14:00.140 --> 00:14:03.229
<v Upol> we go on, more and more consequential

333
00:14:03.230 --> 00:14:05.809
<v Upol> decisions that we are making are

334
00:14:05.810 --> 00:14:08.359
<v Upol> either powered through AI

335
00:14:08.360 --> 00:14:10.309
<v Upol> or automated by.

336
00:14:10.310 --> 00:14:13.669
<v Upol> So this actually creates a need

337
00:14:13.670 --> 00:14:16.549
<v Upol> for AI to be held accountable.

338
00:14:16.550 --> 00:14:18.439
<v Upol> Like if something is doing something

339
00:14:18.440 --> 00:14:21.139
<v Upol> consequential, I need to be able

340
00:14:21.140 --> 00:14:23.629
<v Upol> to ask why?

341
00:14:23.630 --> 00:14:26.689
<v Upol> Hmm. And the answer to that question

342
00:14:26.690 --> 00:14:28.820
<v Upol> is where explainable AI comes in.

343
00:14:29.830 --> 00:14:32.229
<v Upol> Broadly speaking, and many people have many

344
00:14:32.230 --> 00:14:34.729
<v Upol> different definitions of it, at least the

345
00:14:34.730 --> 00:14:37.509
<v Upol> way our lab and I have conceptualized

346
00:14:37.510 --> 00:14:39.729
<v Upol> it in the years of work we have done is

347
00:14:39.730 --> 00:14:42.429
<v Upol> explainable. AI refers to

348
00:14:42.430 --> 00:14:45.189
<v Upol> the techniques, the strategies, the

349
00:14:45.190 --> 00:14:47.769
<v Upol> philosophies that can

350
00:14:47.770 --> 00:14:51.639
<v Upol> help us as stakeholders

351
00:14:51.640 --> 00:14:53.889
<v Upol> within the AI system so it could be end users,

352
00:14:53.890 --> 00:14:58.269
<v Upol> developers, data scientists understand

353
00:14:58.270 --> 00:15:01.659
<v Upol> why the the system

354
00:15:01.660 --> 00:15:02.799
<v Upol> did what it did.

355
00:15:04.090 --> 00:15:06.849
<v Upol> And again, this is why it's also human

356
00:15:06.850 --> 00:15:09.129
<v Upol> centered in the sense that it's not just the

357
00:15:09.130 --> 00:15:11.079
<v Upol> algorithm, right? There's a human at the end of it

358
00:15:11.080 --> 00:15:13.779
<v Upol> trying to understand it so it

359
00:15:13.780 --> 00:15:15.819
<v Upol> can take many forms.

360
00:15:15.820 --> 00:15:18.399
<v Upol> Sometimes these explanations can be in the

361
00:15:18.400 --> 00:15:20.949
<v Upol> form of natural language, plain English,

362
00:15:20.950 --> 00:15:24.069
<v Upol> for instance, explanations like textual.

363
00:15:24.070 --> 00:15:26.889
<v Upol> Sometimes these explanations can be in the form

364
00:15:26.890 --> 00:15:28.090
<v Upol> of visualizations.

365
00:15:29.170 --> 00:15:31.779
<v Upol> Sometimes these explanations can be in the form

366
00:15:31.780 --> 00:15:34.449
<v Upol> of data structures, so they have the guts

367
00:15:34.450 --> 00:15:36.999
<v Upol> of a neural net where you are trying to figure

368
00:15:37.000 --> 00:15:39.639
<v Upol> out which layer is what's important.

369
00:15:39.640 --> 00:15:42.669
<v Upol> So these explanations and explainable?

370
00:15:42.670 --> 00:15:45.099
<v Upol> I think the takeaway is very pluralistic.

371
00:15:45.100 --> 00:15:46.419
<v Upol> It's not monolithic.

372
00:15:46.420 --> 00:15:48.699
<v Upol> It's not. There's not one little thing that fits

373
00:15:48.700 --> 00:15:51.429
<v Upol> all. But at the core of it, it's

374
00:15:51.430 --> 00:15:54.249
<v Upol> about understanding the decision making

375
00:15:54.250 --> 00:15:56.799
<v Upol> in a way that makes sense to the user in a

376
00:15:56.800 --> 00:15:59.019
<v Upol> way that makes sense for the person interpreting

377
00:15:59.020 --> 00:15:59.799
<v Upol> it.

378
00:15:59.800 --> 00:16:01.699
<v Andrey> Does that help? Yeah, no.

379
00:16:01.700 --> 00:16:03.459
<v Andrey> That explains it. I think quite well.

380
00:16:03.460 --> 00:16:06.099
<v Andrey> And I guess it's

381
00:16:06.100 --> 00:16:08.649
<v Andrey> worth noting that this is especially

382
00:16:08.650 --> 00:16:11.319
<v Andrey> difficult these days because we are working

383
00:16:11.320 --> 00:16:13.419
<v Andrey> a lot to a Deep Learning.

384
00:16:13.420 --> 00:16:16.079
<v Andrey> The way that works is you have a huge model

385
00:16:16.080 --> 00:16:17.289
<v Andrey> of what awaits you.

386
00:16:17.290 --> 00:16:18.639
<v Andrey> You trained on that a set.

387
00:16:18.640 --> 00:16:20.589
<v Andrey> And then what you get is a phase where you can fry

388
00:16:20.590 --> 00:16:22.689
<v Andrey> an endpoint and get an output right of the

389
00:16:22.690 --> 00:16:24.039
<v Andrey> challenges.

390
00:16:24.040 --> 00:16:26.619
<v Andrey> Now explain why it's doing what

391
00:16:26.620 --> 00:16:27.729
<v Andrey> it's doing, right?

392
00:16:27.730 --> 00:16:29.189
<v Upol> Absolutely. Yeah.

393
00:16:29.190 --> 00:16:31.689
<v Upol> And actually, now that brings to another

394
00:16:31.690 --> 00:16:34.329
<v Upol> point, you know, there are many ways and then you

395
00:16:34.330 --> 00:16:37.619
<v Upol> hear different words being kind of used.

396
00:16:37.620 --> 00:16:41.139
<v Upol> In my view, I kind of split explainability

397
00:16:41.140 --> 00:16:43.749
<v Upol> into like transparency,

398
00:16:43.750 --> 00:16:46.659
<v Upol> interpretability kind of

399
00:16:46.660 --> 00:16:49.359
<v Upol> branches and then post hoc explainability.

400
00:16:49.360 --> 00:16:51.549
<v Upol> So I'll cover all eight of these.

401
00:16:51.550 --> 00:16:54.309
<v Upol> So transparency would be almost like clear boxing

402
00:16:54.310 --> 00:16:56.979
<v Upol> it so like instead of like black boxing it could

403
00:16:56.980 --> 00:16:59.109
<v Upol> you just make the model just completely

404
00:16:59.110 --> 00:17:01.629
<v Upol> transparent, like that's just one of the ideal to

405
00:17:01.630 --> 00:17:03.969
<v Andrey> understand the model itself.

406
00:17:03.970 --> 00:17:06.578
<v Upol> Then interpretability involves, I add

407
00:17:06.579 --> 00:17:09.098
<v Upol> in my view, the able to scrutinize

408
00:17:09.099 --> 00:17:10.868
<v Upol> an algorithm. So in other words, like like a

409
00:17:10.869 --> 00:17:13.449
<v Upol> decision tree life, like the infrastructure or the

410
00:17:13.450 --> 00:17:16.269
<v Upol> architecture of forms,

411
00:17:16.270 --> 00:17:19.088
<v Upol> the fact that I can poke and prod

412
00:17:19.089 --> 00:17:22.179
<v Upol> and I can get a good understanding

413
00:17:22.180 --> 00:17:25.118
<v Upol> and I can interpret what the model is doing right.

414
00:17:25.119 --> 00:17:27.669
<v Upol> But that also requires a level of expertize

415
00:17:27.670 --> 00:17:29.739
<v Upol> like you need to have the training to interpret a

416
00:17:29.740 --> 00:17:31.719
<v Upol> decision tree. You cannot just, you know, you

417
00:17:31.720 --> 00:17:33.429
<v Upol> can't just give anyone on the street like, Hey,

418
00:17:33.430 --> 00:17:35.679
<v Upol> who's a decision tree? Interpret it, right?

419
00:17:35.680 --> 00:17:37.899
<v Upol> So there's this level of interpretation that comes

420
00:17:37.900 --> 00:17:41.079
<v Upol> in, but the architecture of the model

421
00:17:41.080 --> 00:17:43.389
<v Upol> should also be able to support it.

422
00:17:43.390 --> 00:17:45.909
<v Upol> Not as you seem like deep learning algorithms are

423
00:17:45.910 --> 00:17:48.279
<v Upol> not really interpretive or by their architecture,

424
00:17:48.280 --> 00:17:50.529
<v Upol> right? Like they're not very friendly on their

425
00:17:50.530 --> 00:17:53.139
<v Upol> side. So recently, there

426
00:17:53.140 --> 00:17:55.869
<v Upol> has been a very big push towards what we call post

427
00:17:55.870 --> 00:17:57.099
<v Upol> hoc explanations.

428
00:17:57.100 --> 00:17:59.769
<v Upol> Right? So adding a layer, a

429
00:17:59.770 --> 00:18:02.289
<v Upol> model on top of the black box, so to speak, and

430
00:18:02.290 --> 00:18:04.149
<v Upol> make it somewhat transparent.

431
00:18:04.150 --> 00:18:06.459
<v Upol> So in other words, can I generate the explanation

432
00:18:06.460 --> 00:18:09.399
<v Upol> after the decision has been made?

433
00:18:09.400 --> 00:18:12.009
<v Upol> So those are the three main branches you see

434
00:18:12.010 --> 00:18:14.799
<v Upol> work within explainable AI these days,

435
00:18:14.800 --> 00:18:17.679
<v Upol> and a lot of people do use the word explainability

436
00:18:17.680 --> 00:18:20.559
<v Upol> and interpretability interchangeably.

437
00:18:20.560 --> 00:18:23.289
<v Upol> I don't. I tend to see explainability

438
00:18:23.290 --> 00:18:26.169
<v Upol> as a larger umbrella that

439
00:18:26.170 --> 00:18:28.329
<v Upol> can house, but doesn't mean I'm right to be

440
00:18:28.330 --> 00:18:30.999
<v Upol> honest, like it's being very precise about

441
00:18:31.000 --> 00:18:32.500
<v Upol> what you're saying when you're saying it.

442
00:18:33.520 --> 00:18:36.099
<v Upol> Does that help like kind of give the demarcation

443
00:18:36.100 --> 00:18:38.739
<v Upol> of the landscape as well the area in the work?

444
00:18:38.740 --> 00:18:41.199
<v Andrey> Yeah, yeah. Of course it's it's interesting that

445
00:18:41.200 --> 00:18:43.359
<v Andrey> at least you can think of it in these different

446
00:18:43.360 --> 00:18:46.359
<v Andrey> dimensions, and I think that also helps understand

447
00:18:46.360 --> 00:18:48.909
<v Andrey> sort of the ways

448
00:18:48.910 --> 00:18:50.289
<v Andrey> you might approach it.

449
00:18:50.290 --> 00:18:53.349
<v Andrey> And speaking of that, as you introduced

450
00:18:53.350 --> 00:18:56.139
<v Andrey> in the intro, your work focuses

451
00:18:56.140 --> 00:18:59.229
<v Andrey> in particular on human centered XIII,

452
00:18:59.230 --> 00:19:02.139
<v Andrey> which is in some ways in contrast to algorithm

453
00:19:02.140 --> 00:19:03.809
<v Andrey> centered XIII.

454
00:19:03.810 --> 00:19:06.489
<v Andrey> So what is human centered

455
00:19:06.490 --> 00:19:08.229
<v Andrey> XIII, in your view?

456
00:19:08.230 --> 00:19:10.239
<v Andrey> Again, as kind of a surface level?

457
00:19:10.240 --> 00:19:12.759
<v Upol> Yeah, it's about, I

458
00:19:12.760 --> 00:19:14.589
<v Upol> guess, the way to kind of think about Incentive

459
00:19:14.590 --> 00:19:17.139
<v Upol> XIII is the following like, there is a myth often

460
00:19:17.140 --> 00:19:19.869
<v Upol> in explainable AI, where we tend

461
00:19:19.870 --> 00:19:22.389
<v Upol> to think that if we could just open the

462
00:19:22.390 --> 00:19:25.179
<v Upol> black box, everything will be fine.

463
00:19:25.180 --> 00:19:26.180
<v Upol> Right?

464
00:19:26.770 --> 00:19:29.299
<v Upol> And my my my response to the myth is also.

465
00:19:29.300 --> 00:19:32.089
<v Upol> And not everything that matters

466
00:19:32.090 --> 00:19:34.339
<v Upol> actually is inside the box.

467
00:19:34.340 --> 00:19:37.289
<v Upol> Why? Because humans don't live inside

468
00:19:37.290 --> 00:19:39.769
<v Upol> the black box office, they're outside and around

469
00:19:39.770 --> 00:19:40.909
<v Upol> it.

470
00:19:40.910 --> 00:19:42.170
<v Upol> And given,

471
00:19:43.520 --> 00:19:46.399
<v Upol> you know, humans are so instrumental

472
00:19:46.400 --> 00:19:48.259
<v Upol> in this ecosystem, right?

473
00:19:50.030 --> 00:19:52.699
<v Upol> It might not be a bad idea to start looking

474
00:19:52.700 --> 00:19:55.339
<v Upol> around the box to understand what

475
00:19:55.340 --> 00:19:56.539
<v Upol> are these value systems?

476
00:19:56.540 --> 00:19:59.119
<v Upol> What are people's ways of thinking that

477
00:19:59.120 --> 00:20:01.639
<v Upol> can ultimately aid

478
00:20:01.640 --> 00:20:03.439
<v Upol> that understanding ability that is so

479
00:20:03.440 --> 00:20:05.959
<v Upol> instrumental, explainable and so

480
00:20:05.960 --> 00:20:07.369
<v Upol> human centered, explainable AI?

481
00:20:07.370 --> 00:20:10.009
<v Upol> What it does is it fundamentally shifts the

482
00:20:10.010 --> 00:20:12.559
<v Upol> attention, and it doesn't say that

483
00:20:12.560 --> 00:20:15.699
<v Upol> algorithm centered work is bad by any means.

484
00:20:15.700 --> 00:20:18.229
<v Upol> It's not saying that what we're saying is we

485
00:20:18.230 --> 00:20:21.619
<v Upol> need to put just as much attention

486
00:20:21.620 --> 00:20:24.199
<v Upol> on the human on who is

487
00:20:24.200 --> 00:20:27.079
<v Upol> opening the box as much as opening

488
00:20:27.080 --> 00:20:27.889
<v Upol> the box.

489
00:20:27.890 --> 00:20:30.409
<v Andrey> Right? Do you need to sort of pay

490
00:20:30.410 --> 00:20:33.139
<v Andrey> attention, care about the human aspect and not

491
00:20:33.140 --> 00:20:34.999
<v Andrey> just think about the model?

492
00:20:35.000 --> 00:20:38.659
<v Andrey> And then, you know, maybe we humans

493
00:20:38.660 --> 00:20:41.089
<v Andrey> take with you, develop a model later and they can

494
00:20:41.090 --> 00:20:43.609
<v Andrey> figure it out. That makes a lot of sense,

495
00:20:43.610 --> 00:20:46.279
<v Andrey> and you have a great motivating example

496
00:20:46.280 --> 00:20:49.759
<v Andrey> of this in your Great Again article

497
00:20:49.760 --> 00:20:52.989
<v Andrey> having to do with this Fire Wall management

498
00:20:52.990 --> 00:20:56.209
<v Andrey> thing and why human centered aspect was necessary.

499
00:20:56.210 --> 00:20:58.279
<v Andrey> So, yeah, I find that very cool.

500
00:20:58.280 --> 00:21:00.079
<v Andrey> Can you go ahead?

501
00:21:00.080 --> 00:21:02.689
<v Upol> Yeah. So this was a

502
00:21:02.690 --> 00:21:05.029
<v Upol> this was a consulting project, but I had the

503
00:21:05.030 --> 00:21:07.609
<v Upol> privilege of kind of helping out with.

504
00:21:07.610 --> 00:21:10.249
<v Upol> They had a cybersecurity company, had hired me

505
00:21:10.250 --> 00:21:12.889
<v Upol> to address a very interesting issue of

506
00:21:12.890 --> 00:21:15.519
<v Upol> this firewall management system.

507
00:21:15.520 --> 00:21:18.109
<v Upol> And in that environment, one

508
00:21:18.110 --> 00:21:20.959
<v Upol> thing that happens is the problem was that bloat.

509
00:21:20.960 --> 00:21:22.279
<v Upol> So what is it? Bloat?

510
00:21:22.280 --> 00:21:25.129
<v Upol> Bloat is what happens when people open course

511
00:21:25.130 --> 00:21:27.499
<v Upol> on a firewall and forget to close them.

512
00:21:27.500 --> 00:21:30.859
<v Upol> So over time, you get a bunch of stuff

513
00:21:30.860 --> 00:21:33.379
<v Upol> that is open. But then what happens is at an

514
00:21:33.380 --> 00:21:36.139
<v Upol> enterprise scale, there is so many

515
00:21:36.140 --> 00:21:38.749
<v Upol> open course that is humanly impossible to go

516
00:21:38.750 --> 00:21:40.579
<v Upol> to every one of them and check.

517
00:21:40.580 --> 00:21:41.959
<v Upol> Oh, wow. Right.

518
00:21:41.960 --> 00:21:44.509
<v Upol> So they had a system that would analyze all these

519
00:21:44.510 --> 00:21:47.209
<v Upol> ports and suggest which

520
00:21:47.210 --> 00:21:49.969
<v Upol> ones do remain closed versus which ones do remain

521
00:21:49.970 --> 00:21:51.289
<v Upol> open.

522
00:21:51.290 --> 00:21:53.929
<v Upol> The problem was the problem here was

523
00:21:53.930 --> 00:21:55.729
<v Upol> rather tricky.

524
00:21:55.730 --> 00:21:58.309
<v Upol> The system was actually performing rather

525
00:21:58.310 --> 00:22:00.439
<v Upol> well around the 90 percent accuracy.

526
00:22:00.440 --> 00:22:04.039
<v Upol> It had really good algorithmic transparency.

527
00:22:04.040 --> 00:22:05.299
<v Upol> But the problem was.

528
00:22:06.610 --> 00:22:09.549
<v Upol> Less than two percent of the workforce

529
00:22:09.550 --> 00:22:12.399
<v Upol> was actually engaging with it and using

530
00:22:12.400 --> 00:22:13.400
<v Upol> it.

531
00:22:14.090 --> 00:22:17.059
<v Andrey> Yeah, and that's not what you want.

532
00:22:17.060 --> 00:22:18.979
<v Andrey> Yeah, and then what was that?

533
00:22:18.980 --> 00:22:19.699
<v Andrey> Yeah.

534
00:22:19.700 --> 00:22:23.029
<v Upol> So and you know, I was brought in with the

535
00:22:23.030 --> 00:22:25.609
<v Upol> task of fixing this and the assumption

536
00:22:25.610 --> 00:22:27.859
<v Upol> was still back then and this was before we kind of

537
00:22:27.860 --> 00:22:29.659
<v Upol> coined the term in the Senate, say I.

538
00:22:29.660 --> 00:22:32.179
<v Upol> And this is the project that actually drives a lot

539
00:22:32.180 --> 00:22:33.709
<v Upol> of that thinking.

540
00:22:33.710 --> 00:22:35.719
<v Upol> And the assumption was, you know, maybe the

541
00:22:35.720 --> 00:22:37.909
<v Upol> solution is within the algorithm, just fix the

542
00:22:37.910 --> 00:22:40.709
<v Upol> algorithm, maybe make it explain better,

543
00:22:40.710 --> 00:22:43.369
<v Upol> maybe open the box differently, so to speak.

544
00:22:44.900 --> 00:22:47.569
<v Upol> And what I found at the end of the day just

545
00:22:47.570 --> 00:22:48.679
<v Upol> to give a cut.

546
00:22:48.680 --> 00:22:51.650
<v Upol> The long story short, I guess, is.

547
00:22:52.670 --> 00:22:55.129
<v Upol> There was nothing that was wrong with the

548
00:22:55.130 --> 00:22:56.509
<v Upol> algorithm.

549
00:22:56.510 --> 00:22:59.239
<v Upol> The explainability that this company was

550
00:22:59.240 --> 00:23:02.119
<v Upol> looking for was at the intersection

551
00:23:02.120 --> 00:23:04.669
<v Upol> of the human and the machine not included in

552
00:23:04.670 --> 00:23:05.670
<v Upol> the machine.

553
00:23:06.290 --> 00:23:08.989
<v Upol> So what we found in this project

554
00:23:08.990 --> 00:23:11.539
<v Upol> presumption was still that something must be wrong

555
00:23:11.540 --> 00:23:13.699
<v Upol> with the algorithm. This was before we had coined

556
00:23:13.700 --> 00:23:15.679
<v Upol> the term Human-Centered XIII.

557
00:23:15.680 --> 00:23:18.319
<v Upol> A lot of the work here actually drove

558
00:23:18.320 --> 00:23:19.490
<v Upol> the philosophy behind it.

559
00:23:20.630 --> 00:23:23.929
<v Upol> And one thing that that came up was

560
00:23:23.930 --> 00:23:26.749
<v Upol> nothing was actually like we couldn't

561
00:23:26.750 --> 00:23:30.079
<v Upol> do much at the algorithmic level that helped

562
00:23:30.080 --> 00:23:32.659
<v Upol> the explainability of the system, the

563
00:23:32.660 --> 00:23:35.209
<v Upol> changes that had to be done, which actually I

564
00:23:35.210 --> 00:23:37.879
<v Upol> think we'll get into when we discuss the expanding

565
00:23:37.880 --> 00:23:40.459
<v Upol> explainability paper is at

566
00:23:40.460 --> 00:23:42.469
<v Upol> the social level.

567
00:23:42.470 --> 00:23:45.589
<v Upol> So what was the problem here was people

568
00:23:45.590 --> 00:23:48.469
<v Upol> had no idea how to calibrate

569
00:23:48.470 --> 00:23:51.649
<v Upol> their trust on this system

570
00:23:51.650 --> 00:23:54.499
<v Upol> that without really understanding

571
00:23:54.500 --> 00:23:57.019
<v Upol> how others are also interacting

572
00:23:57.020 --> 00:23:58.129
<v Upol> with the system.

573
00:23:58.130 --> 00:24:00.409
<v Upol> Right. So for instance, if I'm faced with a new

574
00:24:00.410 --> 00:24:02.669
<v Upol> system and there is no notion of the ground truth,

575
00:24:02.670 --> 00:24:03.949
<v Upol> right?

576
00:24:03.950 --> 00:24:06.889
<v Upol> And the easiest example to share here was

577
00:24:06.890 --> 00:24:09.229
<v Upol> there was a young analyst and I'm using pseudonyms

578
00:24:09.230 --> 00:24:11.810
<v Upol> like Julie and Julie

579
00:24:13.010 --> 00:24:14.749
<v Upol> had a recommendation from the A.I.

580
00:24:14.750 --> 00:24:17.359
<v Upol> system and to close a few

581
00:24:17.360 --> 00:24:20.089
<v Upol> ports. And on paper, the recommendation

582
00:24:20.090 --> 00:24:21.090
<v Upol> was not wrong.

583
00:24:22.070 --> 00:24:24.589
<v Upol> I have suggested that, hey, you know, if

584
00:24:24.590 --> 00:24:26.539
<v Upol> you close these ports because they have been open

585
00:24:26.540 --> 00:24:28.069
<v Upol> for a long time, they have not been used.

586
00:24:28.070 --> 00:24:31.279
<v Upol> So technically, these are not bad suggestions.

587
00:24:31.280 --> 00:24:33.769
<v Upol> Julie, not knowing a lot of the institutional

588
00:24:33.770 --> 00:24:36.619
<v Upol> history and how things are done accepted

589
00:24:36.620 --> 00:24:37.640
<v Upol> this decision.

590
00:24:38.700 --> 00:24:41.219
<v Upol> Two weeks later, the company faced

591
00:24:41.220 --> 00:24:42.220
<v Upol> a breach.

592
00:24:43.240 --> 00:24:45.819
<v Upol> And then lost around $2 billion

593
00:24:45.820 --> 00:24:46.820
<v Upol> in one.

594
00:24:47.810 --> 00:24:50.569
<v Upol> What had happened was Julie

595
00:24:50.570 --> 00:24:53.419
<v Upol> had accidentally closed following

596
00:24:53.420 --> 00:24:56.419
<v Upol> the A's recommendation, the backup center reports.

597
00:24:56.420 --> 00:24:59.179
<v Upol> Right. So because their backups are reports,

598
00:24:59.180 --> 00:25:01.219
<v Upol> of course, it's good that they have not been used,

599
00:25:01.220 --> 00:25:04.219
<v Upol> right? It is also good that they're open.

600
00:25:04.220 --> 00:25:06.529
<v Upol> So this kind of highlights a very interesting

601
00:25:06.530 --> 00:25:09.379
<v Upol> tension here that even though the air

602
00:25:09.380 --> 00:25:12.049
<v Upol> system was technically not right,

603
00:25:12.050 --> 00:25:14.299
<v Upol> Julie actually got fired.

604
00:25:14.300 --> 00:25:16.169
<v Upol> Oh, that's that's a shame.

605
00:25:16.170 --> 00:25:18.649
<v Upol> Yeah, yeah. So the accountability is squarely

606
00:25:18.650 --> 00:25:21.319
<v Upol> light on the human user, even though

607
00:25:21.320 --> 00:25:23.209
<v Upol> the human user in this case, they are not data

608
00:25:23.210 --> 00:25:25.399
<v Upol> scientist, either cybersecurity analysts, they

609
00:25:25.400 --> 00:25:28.639
<v Upol> shouldn't have to know how this guy is working.

610
00:25:28.640 --> 00:25:31.519
<v Upol> So it's very hard in real world situations

611
00:25:31.520 --> 00:25:34.099
<v Upol> to answer the following question one

612
00:25:34.100 --> 00:25:35.509
<v Upol> does this A.I.

613
00:25:35.510 --> 00:25:36.529
<v Upol> not know,

614
00:25:38.240 --> 00:25:41.419
<v Upol> right? And to address that question

615
00:25:41.420 --> 00:25:43.160
<v Upol> is almost an unknown, unknown, right?

616
00:25:44.210 --> 00:25:46.609
<v Upol> You need and in this case, in this case study,

617
00:25:46.610 --> 00:25:48.649
<v Upol> they needed this thing.

618
00:25:48.650 --> 00:25:51.619
<v Upol> What the socio organizational context

619
00:25:51.620 --> 00:25:53.539
<v Upol> to help them understand how are other people

620
00:25:53.540 --> 00:25:56.179
<v Upol> dealing with it and and watching how

621
00:25:56.180 --> 00:25:57.829
<v Upol> others are acting with it, they were able to

622
00:25:57.830 --> 00:26:00.469
<v Upol> develop more robust mental models

623
00:26:00.470 --> 00:26:03.079
<v Upol> of how to calibrate that trust on the

624
00:26:03.080 --> 00:26:05.929
<v Upol> system. In other words, which are the situations

625
00:26:05.930 --> 00:26:08.599
<v Upol> that I want to see really well and which

626
00:26:08.600 --> 00:26:10.669
<v Upol> are the situations that A.I. does not perform

627
00:26:10.670 --> 00:26:12.739
<v Upol> really well because even if the performance is not

628
00:26:12.740 --> 00:26:15.349
<v Upol> uniform, that's the other reality in these

629
00:26:15.350 --> 00:26:17.179
<v Upol> real world systems.

630
00:26:17.180 --> 00:26:18.889
<v Upol> So that's just, you know, just a quick

631
00:26:18.890 --> 00:26:21.589
<v Upol> summarization of this out of that

632
00:26:21.590 --> 00:26:24.169
<v Upol> case study, which kind of showed me that

633
00:26:24.170 --> 00:26:27.109
<v Upol> there were elements outside the black box

634
00:26:27.110 --> 00:26:29.629
<v Upol> that we really needed to incorporate in the

635
00:26:29.630 --> 00:26:32.209
<v Upol> decision making to help

636
00:26:32.210 --> 00:26:34.789
<v Upol> decision makers do it right

637
00:26:34.790 --> 00:26:37.789
<v Upol> and to make sure accountability was shared

638
00:26:37.790 --> 00:26:40.699
<v Upol> rather than be inappropriately placed

639
00:26:40.700 --> 00:26:43.879
<v Upol> all on the human and nothing on the machine.

640
00:26:43.880 --> 00:26:45.529
<v Andrey> Yeah, yeah, it's interesting.

641
00:26:45.530 --> 00:26:49.099
<v Andrey> I think a lot of listeners might now appreciate

642
00:26:49.100 --> 00:26:52.359
<v Andrey> the importance of this kind of work in terms of,

643
00:26:52.360 --> 00:26:53.719
<v Andrey> you know, outcome come here.

644
00:26:53.720 --> 00:26:56.599
<v Andrey> And I think we'll dig in a bit more into

645
00:26:56.600 --> 00:26:59.119
<v Andrey> where you want are in terms of

646
00:26:59.120 --> 00:27:01.279
<v Andrey> how you do it, which was really interesting.

647
00:27:02.390 --> 00:27:04.909
<v Andrey> Now, with a lot of these concepts laid

648
00:27:04.910 --> 00:27:07.459
<v Andrey> out before we get into kind of our

649
00:27:07.460 --> 00:27:10.159
<v Andrey> main focus, I try to be fun to walk

650
00:27:10.160 --> 00:27:12.979
<v Andrey> through kind of your

651
00:27:12.980 --> 00:27:15.589
<v Andrey> journey in some sense of your

652
00:27:15.590 --> 00:27:18.259
<v Andrey> trajectory, starting out less

653
00:27:18.260 --> 00:27:20.659
<v Andrey> human centered and then sort of discovering that

654
00:27:20.660 --> 00:27:23.209
<v Andrey> and more and more coming closer to where you

655
00:27:23.210 --> 00:27:24.499
<v Andrey> are now.

656
00:27:24.500 --> 00:27:27.049
<v Andrey> So first, you had kind

657
00:27:27.050 --> 00:27:29.749
<v Andrey> of, let's say, a more traditional maybe

658
00:27:29.750 --> 00:27:32.479
<v Andrey> X hardware called rationalization and

659
00:27:32.480 --> 00:27:35.599
<v Andrey> neural machine translation approach to generating

660
00:27:35.600 --> 00:27:38.269
<v Andrey> natural language explanations.

661
00:27:38.270 --> 00:27:40.909
<v Andrey> So just in brief, you know, what was

662
00:27:40.910 --> 00:27:44.629
<v Andrey> this paper and sort of what was the

663
00:27:44.630 --> 00:27:46.039
<v Andrey> contribution there?

664
00:27:46.040 --> 00:27:47.329
<v Upol> No, thank you for asking that.

665
00:27:47.330 --> 00:27:49.909
<v Upol> I think this is the phase in my dissertation that

666
00:27:49.910 --> 00:27:51.829
<v Upol> I call turn to the machine.

667
00:27:51.830 --> 00:27:54.439
<v Upol> Mm hmm. I've kind of takes a few turns in

668
00:27:54.440 --> 00:27:57.319
<v Upol> this turn to the machine mark

669
00:27:57.320 --> 00:27:59.149
<v Upol> and I kind of end Brandt.

670
00:27:59.150 --> 00:28:01.009
<v Upol> So I just when I thought on my coauthors like

671
00:28:01.010 --> 00:28:02.389
<v Upol> Brant Harrison, who is at the University of

672
00:28:02.390 --> 00:28:05.299
<v Upol> Kentucky, my fiddle, obviously his tech

673
00:28:06.470 --> 00:28:09.049
<v Upol> and per now is also now I think it's

674
00:28:09.050 --> 00:28:11.959
<v Upol> eastern at Georgia Tech and Larry Chen,

675
00:28:11.960 --> 00:28:14.359
<v Upol> who is now graduated from Georgia Tech.

676
00:28:14.360 --> 00:28:17.390
<v Upol> We kind of started thinking that, you know,

677
00:28:18.530 --> 00:28:21.199
<v Upol> wouldn't it be nice if

678
00:28:21.200 --> 00:28:23.749
<v Upol> the AI system talk to you or

679
00:28:23.750 --> 00:28:26.269
<v Upol> thought out loud in plain English?

680
00:28:26.270 --> 00:28:28.789
<v Upol> Mm hmm. And the reason why we kind of

681
00:28:28.790 --> 00:28:32.359
<v Upol> thought about that was, Hey, I'm

682
00:28:32.360 --> 00:28:35.509
<v Upol> not everyone has the background to interpret

683
00:28:35.510 --> 00:28:37.159
<v Upol> models, right?

684
00:28:37.160 --> 00:28:39.739
<v Upol> And our a lot of our end users are not

685
00:28:39.740 --> 00:28:42.359
<v Upol> A.I. experts, but everyone,

686
00:28:42.360 --> 00:28:44.599
<v Upol> if they can speak and read and write in English,

687
00:28:44.600 --> 00:28:46.009
<v Upol> could understand English, right?

688
00:28:46.010 --> 00:28:48.499
<v Upol> In fact, that's how we even communicate.

689
00:28:48.500 --> 00:28:51.499
<v Upol> So I in this paper actually do a lot

690
00:28:51.500 --> 00:28:54.469
<v Upol> of inspiration from philosophy

691
00:28:54.470 --> 00:28:57.199
<v Upol> of language, namely the work of Jerry Fodor

692
00:28:58.430 --> 00:29:00.949
<v Upol> to kind of and work with Brant to kind of

693
00:29:00.950 --> 00:29:03.469
<v Upol> develop the algorithmic infrastructure

694
00:29:03.470 --> 00:29:05.929
<v Upol> to answer the following question.

695
00:29:05.930 --> 00:29:08.659
<v Upol> And then this is the question that is asked me

696
00:29:08.660 --> 00:29:10.319
<v Upol> in this paper Can we?

697
00:29:10.320 --> 00:29:12.379
<v Upol> This is almost like an existence proof, like can

698
00:29:12.380 --> 00:29:15.169
<v Upol> we generate rationales

699
00:29:15.170 --> 00:29:18.829
<v Upol> from using a neural machine translation approach?

700
00:29:18.830 --> 00:29:20.809
<v Upol> And this was the first one.

701
00:29:20.810 --> 00:29:23.309
<v Upol> Yeah, to our knowledge, that uses an

702
00:29:23.310 --> 00:29:24.859
<v Upol> NMT mechanism.

703
00:29:24.860 --> 00:29:27.859
<v Upol> Instead of translating from like

704
00:29:27.860 --> 00:29:30.289
<v Upol> English to Bengali, like natural language to

705
00:29:30.290 --> 00:29:33.199
<v Upol> natural language, we we felt what if we replace

706
00:29:33.200 --> 00:29:34.759
<v Upol> one of the natural languages with some data

707
00:29:34.760 --> 00:29:36.589
<v Upol> structures? Right, right.

708
00:29:36.590 --> 00:29:38.659
<v Upol> And that's the insight in this case.

709
00:29:38.660 --> 00:29:41.359
<v Upol> And the innovation was we were able to back

710
00:29:41.360 --> 00:29:43.219
<v Upol> in the day, like when this paper was published

711
00:29:43.220 --> 00:29:45.799
<v Upol> back in 2017 18, there was a lot

712
00:29:45.800 --> 00:29:47.329
<v Upol> of work going on automated image.

713
00:29:47.330 --> 00:29:49.939
<v Upol> Captioning and stuff like that, but very little

714
00:29:49.940 --> 00:29:52.789
<v Upol> work was done on sequential

715
00:29:52.790 --> 00:29:54.679
<v Upol> decision making, right?

716
00:29:54.680 --> 00:29:56.779
<v Upol> So like, you know, if you can think of robotics,

717
00:29:56.780 --> 00:29:59.519
<v Upol> right, like getting a robot from one point

718
00:29:59.520 --> 00:30:02.089
<v Upol> in the kitchen to be in the kitchen is

719
00:30:02.090 --> 00:30:04.069
<v Upol> a sequential decision making task.

720
00:30:04.070 --> 00:30:06.769
<v Upol> So we actually took a sequential decision making

721
00:30:06.770 --> 00:30:09.709
<v Upol> environment and need an agent

722
00:30:09.710 --> 00:30:12.349
<v Upol> navigate it while being

723
00:30:12.350 --> 00:30:15.019
<v Upol> able to think out loud in plain English.

724
00:30:15.020 --> 00:30:17.299
<v Andrey> If I remember correctly, this was like a game.

725
00:30:17.300 --> 00:30:18.929
<v Andrey> Frogger? Yes.

726
00:30:18.930 --> 00:30:20.419
<v Upol> Yes, yes.

727
00:30:20.420 --> 00:30:23.029
<v Upol> Yes. So that that was an homage to a lot

728
00:30:23.030 --> 00:30:25.669
<v Upol> of the game work that goes at the entertainment

729
00:30:25.670 --> 00:30:28.069
<v Upol> intelligent and Human-Centered AI Lab at Georgia

730
00:30:28.070 --> 00:30:30.349
<v Upol> Tech. So we kind of leveraged a lot of our game

731
00:30:30.350 --> 00:30:32.959
<v Upol> A.I. history, which I know, you know,

732
00:30:32.960 --> 00:30:34.909
<v Upol> I know you were at Georgia Tech for undergrad, so

733
00:30:34.910 --> 00:30:37.099
<v Upol> I think you might also be familiar with a bit of

734
00:30:37.100 --> 00:30:37.729
<v Upol> that.

735
00:30:37.730 --> 00:30:38.889
<v Andrey> Oh, yeah, yeah, yeah.

736
00:30:38.890 --> 00:30:41.689
<v Andrey> And yeah, Frontier is a fine example because it's

737
00:30:41.690 --> 00:30:43.479
<v Andrey> pretty intuitive, right?

738
00:30:43.480 --> 00:30:45.709
<v Andrey> You know, why do you want to jump forward well as

739
00:30:45.710 --> 00:30:48.379
<v Andrey> a car racing towards Minnesota to

740
00:30:48.380 --> 00:30:49.380
<v Andrey> avoid it?

741
00:30:50.420 --> 00:30:53.179
<v Andrey> Yeah, but that was a cool start and

742
00:30:53.180 --> 00:30:55.699
<v Andrey> certainly interesting. But since when you

743
00:30:55.700 --> 00:30:58.399
<v Andrey> have moved more towards the humor center

744
00:30:58.400 --> 00:31:00.979
<v Andrey> aspect, so that's going to the next

745
00:31:00.980 --> 00:31:04.159
<v Andrey> step, I suppose return to a human,

746
00:31:04.160 --> 00:31:06.259
<v Andrey> which I think started in the way of this other

747
00:31:06.260 --> 00:31:08.719
<v Andrey> paper automated rationale generation kind of

748
00:31:08.720 --> 00:31:11.299
<v Andrey> extending this, but then a technique

749
00:31:11.300 --> 00:31:13.579
<v Andrey> for explainable AI and its effects on human

750
00:31:13.580 --> 00:31:14.580
<v Andrey> perception.

751
00:31:15.650 --> 00:31:17.929
<v Andrey> So how did that come about?

752
00:31:17.930 --> 00:31:20.569
<v Upol> So, yeah, so this one, so after we

753
00:31:20.570 --> 00:31:22.489
<v Upol> ask the question, can we generate?

754
00:31:22.490 --> 00:31:24.289
<v Upol> And the answer was yes.

755
00:31:24.290 --> 00:31:26.899
<v Upol> Now we ask the question, OK.

756
00:31:26.900 --> 00:31:30.349
<v Upol> These generated rationales, are they any good,

757
00:31:30.350 --> 00:31:32.899
<v Upol> right? Like because back then, if you think about

758
00:31:32.900 --> 00:31:35.749
<v Upol> how we used to evaluate these generative

759
00:31:35.750 --> 00:31:38.299
<v Upol> systems, you know, blue score or other

760
00:31:38.300 --> 00:31:40.819
<v Upol> procedural techniques are

761
00:31:40.820 --> 00:31:43.579
<v Upol> good, but we don't really get a sense of how good

762
00:31:43.580 --> 00:31:45.139
<v Upol> they are to human beings.

763
00:31:45.140 --> 00:31:47.779
<v Upol> Right? Like, do people actually find these

764
00:31:47.780 --> 00:31:48.979
<v Upol> plausible?

765
00:31:48.980 --> 00:31:51.979
<v Upol> So in this paper, ours are like kind of starts

766
00:31:51.980 --> 00:31:53.479
<v Upol> to turn to the human.

767
00:31:53.480 --> 00:31:56.029
<v Upol> We presented the first work

768
00:31:56.030 --> 00:31:58.789
<v Upol> that gave a robust human

769
00:31:58.790 --> 00:32:01.969
<v Upol> centered use our study along certain dimensions

770
00:32:01.970 --> 00:32:05.359
<v Upol> of user perceptions to evaluate

771
00:32:05.360 --> 00:32:08.389
<v Upol> these rationale generating systems.

772
00:32:08.390 --> 00:32:11.239
<v Upol> And what we found was that

773
00:32:11.240 --> 00:32:12.499
<v Upol> we bridged a lot of work.

774
00:32:12.500 --> 00:32:15.139
<v Upol> So I took all of these measures and adapted

775
00:32:15.140 --> 00:32:17.779
<v Upol> it from work in HCI human robot

776
00:32:17.780 --> 00:32:20.329
<v Upol> interaction, as well as the technology

777
00:32:20.330 --> 00:32:22.909
<v Upol> acceptance models from back in the 90s when

778
00:32:22.910 --> 00:32:24.890
<v Upol> automation was becoming hot.

779
00:32:25.970 --> 00:32:28.969
<v Upol> And we found fascinating things around, not just

780
00:32:28.970 --> 00:32:31.579
<v Upol> the fact that these were plausible in this paper.

781
00:32:31.580 --> 00:32:34.369
<v Upol> We just didn't make the Frogger kind of say

782
00:32:34.370 --> 00:32:37.069
<v Upol> things. In one way, we were able to tweak

783
00:32:37.070 --> 00:32:39.949
<v Upol> the network in a way that I could make

784
00:32:39.950 --> 00:32:42.589
<v Upol> Frogger cough more in detail

785
00:32:42.590 --> 00:32:45.379
<v Upol> versus, say things more shortly

786
00:32:45.380 --> 00:32:46.400
<v Upol> in its rationales.

787
00:32:47.540 --> 00:32:50.209
<v Upol> And we found that the level of detail

788
00:32:50.210 --> 00:32:52.939
<v Upol> also had a lot of interesting

789
00:32:52.940 --> 00:32:55.489
<v Upol> interweaving effects on people's

790
00:32:55.490 --> 00:32:58.039
<v Upol> trust, people's confidence,

791
00:32:58.040 --> 00:33:00.589
<v Upol> how tolerant, where they when the robots like

792
00:33:00.590 --> 00:33:03.019
<v Upol> that further failed, right?

793
00:33:03.020 --> 00:33:05.479
<v Upol> So this was a really interesting deep dove.

794
00:33:05.480 --> 00:33:07.909
<v Upol> And we just not only did the quality quantitative

795
00:33:07.910 --> 00:33:10.339
<v Upol> part, we did a really good qualitative part as

796
00:33:10.340 --> 00:33:11.509
<v Upol> well.

797
00:33:11.510 --> 00:33:13.369
<v Upol> These are the crowd workers.

798
00:33:13.370 --> 00:33:16.129
<v Upol> And, you know, getting Amazon Mechanical

799
00:33:16.130 --> 00:33:18.799
<v Upol> Turk first to take a forty five minute

800
00:33:18.800 --> 00:33:21.499
<v Upol> task is not easy, I

801
00:33:21.500 --> 00:33:24.499
<v Upol> think. And so we were liking the methodology

802
00:33:24.500 --> 00:33:26.839
<v Upol> part. I think we were very happy with it, and I'm

803
00:33:26.840 --> 00:33:28.460
<v Upol> so proud of the team that did it.

804
00:33:30.160 --> 00:33:32.869
<v Upol> They were saying Han and another research

805
00:33:32.870 --> 00:33:34.909
<v Upol> assistant were undergrads at Georgia Tech who

806
00:33:34.910 --> 00:33:37.549
<v Upol> helped us create a really good

807
00:33:37.550 --> 00:33:40.279
<v Upol> data collection pipeline that helped us collect

808
00:33:40.280 --> 00:33:42.259
<v Upol> these rationales to train.

809
00:33:42.260 --> 00:33:44.719
<v Upol> And then we not only train, but we also tested it.

810
00:33:44.720 --> 00:33:47.119
<v Upol> So that was the end to end kind of application of

811
00:33:47.120 --> 00:33:49.519
<v Upol> this that really made the paper.

812
00:33:49.520 --> 00:33:52.549
<v Upol> One of my favorite papers that I've gotten great.

813
00:33:52.550 --> 00:33:55.429
<v Andrey> Yeah, is this reminds me a little bit

814
00:33:55.430 --> 00:33:57.959
<v Andrey> of the whole

815
00:33:57.960 --> 00:34:00.079
<v Andrey> like. Some field of social robotics is quite

816
00:34:00.080 --> 00:34:02.819
<v Andrey> interesting because again, there's a lot to do

817
00:34:02.820 --> 00:34:05.389
<v Andrey> with human perceptions and like, how do

818
00:34:05.390 --> 00:34:08.238
<v Andrey> you communicate intent of grasping a couch

819
00:34:08.239 --> 00:34:10.759
<v Andrey> in a way that you know, people can understand?

820
00:34:10.760 --> 00:34:13.428
<v Andrey> Or how do you appear

821
00:34:13.429 --> 00:34:14.669
<v Andrey> friendly and so on?

822
00:34:14.670 --> 00:34:17.299
<v Andrey> That's its own whole thing, and it's always

823
00:34:17.300 --> 00:34:19.968
<v Andrey> interesting to see that, you know, aside from

824
00:34:19.969 --> 00:34:22.819
<v Andrey> all of the social models, if you need air

825
00:34:22.820 --> 00:34:25.369
<v Andrey> during the real world, this is also a

826
00:34:25.370 --> 00:34:28.019
<v Andrey> big challenge indeed.

827
00:34:28.020 --> 00:34:30.869
<v Andrey> Yes, so in this one, one

828
00:34:30.870 --> 00:34:33.869
<v Andrey> aspect that differs from

829
00:34:33.870 --> 00:34:36.448
<v Andrey> the other white we'll we'll get to soon

830
00:34:36.449 --> 00:34:38.968
<v Andrey> is here, you sort of are still dealing with

831
00:34:38.969 --> 00:34:41.669
<v Andrey> one to one interaction versus playing game

832
00:34:41.670 --> 00:34:44.399
<v Andrey> and then the agent is kind

833
00:34:44.400 --> 00:34:47.339
<v Andrey> of trying to make it clear what's going on.

834
00:34:47.340 --> 00:34:49.979
<v Andrey> And you already mentioned in your example that you

835
00:34:49.980 --> 00:34:52.649
<v Andrey> know you need and many real war situations

836
00:34:52.650 --> 00:34:54.809
<v Andrey> to go beyond that, you need organizational

837
00:34:54.810 --> 00:34:57.329
<v Andrey> context. You need to understand groups of people,

838
00:34:57.330 --> 00:34:58.619
<v Andrey> so to speak.

839
00:34:58.620 --> 00:35:01.169
<v Andrey> And that takes us to

840
00:35:01.170 --> 00:35:03.899
<v Andrey> the concept of socio technical

841
00:35:03.900 --> 00:35:04.900
<v Andrey> challenges.

842
00:35:06.100 --> 00:35:08.969
<v Andrey> Yes. So how did you make that

843
00:35:08.970 --> 00:35:11.779
<v Andrey> turn and

844
00:35:11.780 --> 00:35:15.239
<v Andrey> what is that compared to this one to one paradigm?

845
00:35:15.240 --> 00:35:18.089
<v Upol> Absolutely. So you hit the

846
00:35:18.090 --> 00:35:19.199
<v Upol> nail on the head, right?

847
00:35:19.200 --> 00:35:21.719
<v Upol> There is like a lot of the way we were thinking

848
00:35:21.720 --> 00:35:24.239
<v Upol> about the rational generation or the interaction

849
00:35:24.240 --> 00:35:26.789
<v Upol> paradigm was very much one to one.

850
00:35:26.790 --> 00:35:29.429
<v Upol> And, you know, based on my prior work in

851
00:35:29.430 --> 00:35:31.919
<v Upol> industry settings, I started realizing that is

852
00:35:31.920 --> 00:35:34.530
<v Upol> that truly representative of what happens.

853
00:35:35.700 --> 00:35:38.399
<v Upol> And I started realizing that, no, we need to think

854
00:35:38.400 --> 00:35:41.129
<v Upol> more about like these AI systems.

855
00:35:41.130 --> 00:35:43.199
<v Upol> I'm never in a vacuum.

856
00:35:43.200 --> 00:35:45.929
<v Upol> They're often situated in larger organizational

857
00:35:45.930 --> 00:35:47.399
<v Upol> environments.

858
00:35:47.400 --> 00:35:50.219
<v Upol> So in that case, how do we think about this?

859
00:35:50.220 --> 00:35:52.409
<v Upol> How do we conceptualize this?

860
00:35:52.410 --> 00:35:55.019
<v Upol> So this kind of forced us, and this

861
00:35:55.020 --> 00:35:57.359
<v Upol> is probably the first kind of conceptual paper

862
00:35:57.360 --> 00:35:59.909
<v Upol> that I have written is to kind

863
00:35:59.910 --> 00:36:02.159
<v Upol> of outline. So we kind of coined the term human

864
00:36:02.160 --> 00:36:05.099
<v Upol> centered essay, but we also wanted to seen

865
00:36:05.100 --> 00:36:07.349
<v Upol> how do you operationalize this thing?

866
00:36:07.350 --> 00:36:10.109
<v Upol> So we bridged theories from critical

867
00:36:10.110 --> 00:36:13.439
<v Upol> A.I. studies like critical technical practice

868
00:36:13.440 --> 00:36:16.169
<v Upol> in HCI, like reflective design

869
00:36:16.170 --> 00:36:18.509
<v Upol> and value sensitive design.

870
00:36:18.510 --> 00:36:21.359
<v Upol> And we kind of talked a little bit about, OK,

871
00:36:21.360 --> 00:36:24.119
<v Upol> now we have this insight that we have to not just

872
00:36:24.120 --> 00:36:26.189
<v Upol> care about one person, but also multiple

873
00:36:26.190 --> 00:36:28.289
<v Upol> stakeholders in the system.

874
00:36:28.290 --> 00:36:30.839
<v Upol> So going back to the cybersecurity example, right,

875
00:36:30.840 --> 00:36:32.819
<v Upol> it's not just the analyst who is making the

876
00:36:32.820 --> 00:36:36.209
<v Upol> decision, it's also the decision of the analysts,

877
00:36:36.210 --> 00:36:38.789
<v Upol> previous, who had made similar decisions

878
00:36:38.790 --> 00:36:41.309
<v Upol> in the past. So that kind of forced us to kind of

879
00:36:41.310 --> 00:36:43.949
<v Upol> imagine and envision AI

880
00:36:43.950 --> 00:36:46.019
<v Upol> explainable AI paradigm that is more human

881
00:36:46.020 --> 00:36:48.629
<v Upol> centered and not just one human, but also

882
00:36:48.630 --> 00:36:51.030
<v Upol> incorporates many humans.

883
00:36:52.240 --> 00:36:55.089
<v Andrey> Mm hmm. Yeah, so there comes a socio technical

884
00:36:55.090 --> 00:36:58.779
<v Andrey> aspect. You know, social being,

885
00:36:58.780 --> 00:37:01.299
<v Andrey> you know, interactions between people and even

886
00:37:01.300 --> 00:37:02.799
<v Andrey> organizations.

887
00:37:02.800 --> 00:37:05.349
<v Andrey> So where you Mary, sort of the groups of

888
00:37:05.350 --> 00:37:08.649
<v Andrey> people with the technical problem, which

889
00:37:08.650 --> 00:37:12.009
<v Andrey> now you really need to think about both.

890
00:37:12.010 --> 00:37:14.589
<v Andrey> And that as

891
00:37:14.590 --> 00:37:17.149
<v Andrey> far as other sound was sort of

892
00:37:17.150 --> 00:37:19.749
<v Andrey> kind of new direction that

893
00:37:19.750 --> 00:37:22.389
<v Andrey> wasn't really the norm or stylish

894
00:37:22.390 --> 00:37:23.679
<v Andrey> in the field.

895
00:37:23.680 --> 00:37:26.079
<v Upol> Yeah. And I think that's a very important point.

896
00:37:26.080 --> 00:37:28.719
<v Upol> In this case. I had drawn a lot

897
00:37:28.720 --> 00:37:31.959
<v Upol> of inspiration from the fact

898
00:37:31.960 --> 00:37:33.969
<v Upol> literature of the fairness, accountability and

899
00:37:33.970 --> 00:37:36.819
<v Upol> transparency literature where they were

900
00:37:36.820 --> 00:37:39.129
<v Upol> very much at that time thinking very socially or

901
00:37:39.130 --> 00:37:41.799
<v Upol> technically. And I am always reminded of

902
00:37:41.800 --> 00:37:44.529
<v Upol> I watched this video from Microsoft Research

903
00:37:44.530 --> 00:37:45.849
<v Upol> is like responsible.

904
00:37:45.850 --> 00:37:48.579
<v Upol> I kind of in visions.

905
00:37:48.580 --> 00:37:51.369
<v Upol> And Hannah Wallach, who is at Amazon

906
00:37:51.370 --> 00:37:54.129
<v Upol> New York, had this fascinating line that I cannot

907
00:37:54.130 --> 00:37:55.959
<v Upol> like repeat verbatim. But the version that I

908
00:37:55.960 --> 00:37:58.119
<v Upol> remember is today.

909
00:37:58.120 --> 00:38:00.729
<v Upol> Our systems are AI

910
00:38:00.730 --> 00:38:03.939
<v Upol> systems are embedded in very complex

911
00:38:03.940 --> 00:38:05.679
<v Upol> social environments.

912
00:38:05.680 --> 00:38:08.289
<v Upol> So that means out the effects that

913
00:38:08.290 --> 00:38:10.869
<v Upol> these technical systems have

914
00:38:10.870 --> 00:38:12.309
<v Upol> our social.

915
00:38:12.310 --> 00:38:15.129
<v Upol> So that means that fundamentally socio technical

916
00:38:15.130 --> 00:38:17.589
<v Upol> in nature, in terms of their complexities as well

917
00:38:17.590 --> 00:38:18.699
<v Upol> as that impacts.

918
00:38:18.700 --> 00:38:21.339
<v Upol> So when we keep that in mind,

919
00:38:21.340 --> 00:38:24.069
<v Upol> I started asking myself, how can

920
00:38:24.070 --> 00:38:27.069
<v Upol> we get a good idea about

921
00:38:27.070 --> 00:38:30.369
<v Upol> explainable AI if we do not

922
00:38:30.370 --> 00:38:33.009
<v Upol> take a socio technical perspective given,

923
00:38:33.010 --> 00:38:35.049
<v Upol> you know, in the real world, that's how these

924
00:38:35.050 --> 00:38:36.669
<v Upol> systems are.

925
00:38:36.670 --> 00:38:39.309
<v Upol> So that's actually a lot of the things that drove

926
00:38:39.310 --> 00:38:42.009
<v Upol> these socio technical lens, so to speak.

927
00:38:42.010 --> 00:38:44.589
<v Upol> And you are right, like this was the first paper,

928
00:38:44.590 --> 00:38:46.479
<v Upol> to our knowledge, to kind of highlight that

929
00:38:46.480 --> 00:38:49.089
<v Upol> explicitly in the context of

930
00:38:49.090 --> 00:38:49.919
<v Upol> explainable.

931
00:38:49.920 --> 00:38:52.569
<v Andrey> I yeah, I find it interesting.

932
00:38:52.570 --> 00:38:55.089
<v Andrey> I think it it seems like it would be easy

933
00:38:55.090 --> 00:38:57.909
<v Andrey> to not have this realization

934
00:38:57.910 --> 00:39:00.429
<v Andrey> if you come from a traditional sort of

935
00:39:00.430 --> 00:39:02.809
<v Andrey> A.I. computer science research background where

936
00:39:02.810 --> 00:39:05.709
<v Andrey> you just jump into a Ph.D., you know where

937
00:39:05.710 --> 00:39:08.439
<v Andrey> you work, in your office, in the computer science

938
00:39:08.440 --> 00:39:10.779
<v Andrey> building, you know, doing your research.

939
00:39:10.780 --> 00:39:13.689
<v Andrey> And it's easy to forget sort of about the outside

940
00:39:13.690 --> 00:39:16.239
<v Andrey> world. So I think it's interesting

941
00:39:16.240 --> 00:39:19.089
<v Andrey> also that having had all this background

942
00:39:19.090 --> 00:39:22.269
<v Andrey> outside working in actual organizations,

943
00:39:22.270 --> 00:39:24.159
<v Andrey> I think I would imagine that also

944
00:39:25.660 --> 00:39:28.449
<v Andrey> made it easier for you to get here.

945
00:39:28.450 --> 00:39:30.069
<v Upol> Yeah, it was. And it's humbling, right?

946
00:39:30.070 --> 00:39:32.529
<v Upol> Because you fail so many times trying to do this,

947
00:39:32.530 --> 00:39:34.569
<v Upol> and that's the only way sometimes we learn.

948
00:39:34.570 --> 00:39:37.359
<v Upol> Right? I, you know, my consulting projects,

949
00:39:37.360 --> 00:39:39.339
<v Upol> I never like linear or straightforward because

950
00:39:39.340 --> 00:39:41.619
<v Upol> they often reach out to me when problems are so

951
00:39:41.620 --> 00:39:44.619
<v Upol> complicated that in-house teams need external

952
00:39:44.620 --> 00:39:45.620
<v Upol> help.

953
00:39:46.060 --> 00:39:48.729
<v Upol> And I think, you know, a lot of us then learn

954
00:39:48.730 --> 00:39:50.859
<v Upol> the lesson that I have learned through all of this

955
00:39:50.860 --> 00:39:53.379
<v Upol> is embracing a

956
00:39:53.380 --> 00:39:56.109
<v Upol> sense of, you know, taking

957
00:39:56.110 --> 00:39:59.199
<v Upol> a learning mentality from a lot of the

958
00:39:59.200 --> 00:40:00.519
<v Upol> there is a famous paper.

959
00:40:00.520 --> 00:40:03.369
<v Upol> I forget the name of the author who kind of framed

960
00:40:03.370 --> 00:40:07.059
<v Upol> mistakes as mistakes

961
00:40:07.060 --> 00:40:09.009
<v Upol> like, you know, in a movie, you take multiple

962
00:40:09.010 --> 00:40:11.739
<v Upol> takes and not all the takes work.

963
00:40:11.740 --> 00:40:14.229
<v Upol> So a lot of them are mistakes, right?

964
00:40:14.230 --> 00:40:17.889
<v Upol> So I really embrace that mentality of mistakes.

965
00:40:17.890 --> 00:40:19.659
<v Upol> Not all projects will work out.

966
00:40:19.660 --> 00:40:22.509
<v Upol> You have a lot of mistakes, I guess.

967
00:40:22.510 --> 00:40:24.009
<v Upol> Nothing is a mistake, per se.

968
00:40:24.010 --> 00:40:26.589
<v Upol> And I think that really helped me have a more

969
00:40:26.590 --> 00:40:29.019
<v Upol> iterative mindset, which has paid a lot of

970
00:40:29.020 --> 00:40:31.749
<v Upol> dividends in getting a lot of this work done.

971
00:40:31.750 --> 00:40:34.569
<v Andrey> Yeah, I think it's interesting how

972
00:40:34.570 --> 00:40:37.579
<v Andrey> in some sense, especially doing it

973
00:40:37.580 --> 00:40:39.879
<v Andrey> really enforces that.

974
00:40:39.880 --> 00:40:42.429
<v Andrey> You really have to adapt to that because

975
00:40:42.430 --> 00:40:45.009
<v Andrey> you got to have failures, but almost

976
00:40:45.010 --> 00:40:47.169
<v Andrey> always will inform your understanding and

977
00:40:47.170 --> 00:40:48.939
<v Andrey> ultimately guide you to something interesting,

978
00:40:48.940 --> 00:40:50.040
<v Andrey> ideally, you know.

979
00:40:51.400 --> 00:40:54.099
<v Andrey> Yeah. So as you did research

980
00:40:54.100 --> 00:40:56.829
<v Andrey> that led to this

981
00:40:56.830 --> 00:40:59.739
<v Andrey> paper, human centered explainable AI towards

982
00:40:59.740 --> 00:41:02.449
<v Andrey> a reflective socio technical approach where

983
00:41:02.450 --> 00:41:04.959
<v Andrey> you lay a lot of groundwork for

984
00:41:04.960 --> 00:41:07.809
<v Andrey> how you can move towards that.

985
00:41:07.810 --> 00:41:10.419
<v Andrey> And we we really can't get too much into

986
00:41:10.420 --> 00:41:12.939
<v Andrey> it. It's it's quite detailed and self.

987
00:41:12.940 --> 00:41:15.459
<v Andrey> But he did write this excellent piece on a

988
00:41:15.460 --> 00:41:18.129
<v Andrey> gradient towards human centered,

989
00:41:18.130 --> 00:41:21.249
<v Andrey> explainable A.I. every journey so far.

990
00:41:21.250 --> 00:41:23.589
<v Andrey> So we're going to link to that in the description,

991
00:41:23.590 --> 00:41:26.419
<v Andrey> and you can just fly a gradient and

992
00:41:26.420 --> 00:41:28.449
<v Andrey> recommend you read that.

993
00:41:28.450 --> 00:41:30.519
<v Andrey> But for now, we're going to actually focus on a

994
00:41:30.520 --> 00:41:33.459
<v Andrey> more recent work expanding explainability

995
00:41:33.460 --> 00:41:36.579
<v Andrey> towards social transparency

996
00:41:36.580 --> 00:41:37.810
<v Andrey> in AI systems.

997
00:41:39.100 --> 00:41:41.739
<v Andrey> So to get into it before even

998
00:41:41.740 --> 00:41:43.059
<v Andrey> getting to any of the details,

999
00:41:44.350 --> 00:41:46.629
<v Andrey> you know, what was your goal in starting this

1000
00:41:46.630 --> 00:41:49.179
<v Andrey> project and sort of a problem that

1001
00:41:49.180 --> 00:41:50.180
<v Andrey> motivated it?

1002
00:41:51.130 --> 00:41:52.329
<v Upol> This is.

1003
00:41:52.330 --> 00:41:55.179
<v Upol> Frankly, I feel like this was the paper

1004
00:41:55.180 --> 00:41:57.999
<v Upol> that, like the Human-Centered AI paper,

1005
00:41:58.000 --> 00:42:00.279
<v Upol> was the paper that needed to be written first for

1006
00:42:00.280 --> 00:42:02.349
<v Upol> me to actually write this paper.

1007
00:42:02.350 --> 00:42:04.149
<v Upol> And you know, a lot of the work in that

1008
00:42:04.150 --> 00:42:06.939
<v Upol> cybersecurity company kind of really

1009
00:42:06.940 --> 00:42:08.169
<v Upol> informed this.

1010
00:42:08.170 --> 00:42:10.059
<v Upol> So, you know, for the longest time, I have been

1011
00:42:10.060 --> 00:42:13.089
<v Upol> kind of arguing that we need to look outside

1012
00:42:13.090 --> 00:42:14.090
<v Upol> the box, right?

1013
00:42:15.100 --> 00:42:17.709
<v Upol> Yeah. So then, you know, largely speaking,

1014
00:42:17.710 --> 00:42:19.839
<v Upol> the community will come back and ask her to call.

1015
00:42:19.840 --> 00:42:22.839
<v Upol> I kind of get what you're trying to say, but what

1016
00:42:22.840 --> 00:42:24.129
<v Upol> outside? What is outside?

1017
00:42:24.130 --> 00:42:25.859
<v Upol> What do you want us to think about?

1018
00:42:26.860 --> 00:42:29.529
<v Upol> And the the kernel of this paper

1019
00:42:29.530 --> 00:42:32.049
<v Upol> is fundamentally and as as the title

1020
00:42:32.050 --> 00:42:35.559
<v Upol> kind of says, extending explainability, it expands

1021
00:42:35.560 --> 00:42:38.569
<v Upol> our conception of explainable

1022
00:42:38.570 --> 00:42:41.529
<v Upol> A.I. beyond the realms

1023
00:42:41.530 --> 00:42:43.689
<v Upol> of algorithmic transparency.

1024
00:42:43.690 --> 00:42:45.069
<v Upol> By doing what?

1025
00:42:45.070 --> 00:42:47.679
<v Upol> By adding this new

1026
00:42:47.680 --> 00:42:49.689
<v Upol> concept called social transparency, which is

1027
00:42:49.690 --> 00:42:51.849
<v Upol> actually like, not like New New in the sense that

1028
00:42:51.850 --> 00:42:54.629
<v Upol> we created it in the in the context of XXXII,

1029
00:42:54.630 --> 00:42:56.649
<v Upol> it's new. There is sort of transparency in online

1030
00:42:56.650 --> 00:42:59.309
<v Upol> systems back from the 90s.

1031
00:42:59.310 --> 00:43:01.839
<v Upol> And in the paper, we kind of pay homage to

1032
00:43:01.840 --> 00:43:04.359
<v Upol> a lot of those work, but it's

1033
00:43:04.360 --> 00:43:07.749
<v Upol> fundamentally making the following observation.

1034
00:43:07.750 --> 00:43:10.389
<v Upol> So within AI systems, and I think

1035
00:43:10.390 --> 00:43:13.059
<v Upol> this is where it becomes very tricky when

1036
00:43:13.060 --> 00:43:15.639
<v Upol> when we say AI systems is actually somewhat

1037
00:43:15.640 --> 00:43:18.849
<v Upol> of a misnomer because when we say AI systems

1038
00:43:18.850 --> 00:43:21.279
<v Upol> are a very important part is left out and it's

1039
00:43:21.280 --> 00:43:24.219
<v Upol> often implicit, which is the human part.

1040
00:43:24.220 --> 00:43:26.889
<v Upol> Implicit in AI systems are what

1041
00:43:26.890 --> 00:43:30.279
<v Upol> we call human AI assemblages.

1042
00:43:30.280 --> 00:43:32.889
<v Upol> Right? So these are two coupled points.

1043
00:43:32.890 --> 00:43:35.679
<v Upol> So ideally, what you're really going for

1044
00:43:35.680 --> 00:43:38.469
<v Upol> is the explainability of this assemblage, right?

1045
00:43:38.470 --> 00:43:41.379
<v Upol> The human part of being often implicit.

1046
00:43:41.380 --> 00:43:44.319
<v Upol> But but how can you

1047
00:43:44.320 --> 00:43:45.939
<v Upol> get the explainability?

1048
00:43:45.940 --> 00:43:48.279
<v Upol> All of these assemblage, these two part system,

1049
00:43:48.280 --> 00:43:51.099
<v Upol> the human and the AI by just focusing

1050
00:43:51.100 --> 00:43:53.919
<v Upol> on the air and asking the question that is asking

1051
00:43:53.920 --> 00:43:55.089
<v Upol> in this paper?

1052
00:43:55.090 --> 00:43:56.679
<v Upol> So then the question becomes IRA to Paul.

1053
00:43:56.680 --> 00:43:58.719
<v Upol> I get it like, you know, you can add, know you

1054
00:43:58.720 --> 00:44:01.369
<v Upol> need the human part, but what about it in looking

1055
00:44:01.370 --> 00:44:02.649
<v Upol> very Typekit?

1056
00:44:02.650 --> 00:44:05.409
<v Upol> So that's where if you add the transparency

1057
00:44:05.410 --> 00:44:07.659
<v Upol> on the human side, we kind of introduce this

1058
00:44:07.660 --> 00:44:10.989
<v Upol> notion of social transparency in AI systems,

1059
00:44:10.990 --> 00:44:13.479
<v Upol> right? Operationalize a little bit of this in the

1060
00:44:13.480 --> 00:44:14.349
<v Upol> paper.

1061
00:44:14.350 --> 00:44:17.319
<v Andrey> Right. So yeah, it's I think very

1062
00:44:17.320 --> 00:44:18.639
<v Andrey> interesting about this in terms of

1063
00:44:18.640 --> 00:44:20.679
<v Andrey> operationalizing that.

1064
00:44:20.680 --> 00:44:23.259
<v Andrey> Not only do you highlight this need, which I think

1065
00:44:23.260 --> 00:44:26.139
<v Andrey> is very intuitive, but actually exquisite talk

1066
00:44:26.140 --> 00:44:28.629
<v Andrey> about how to do this, how to be useful and really

1067
00:44:28.630 --> 00:44:31.449
<v Andrey> beyond technical transparency and

1068
00:44:31.450 --> 00:44:33.279
<v Andrey> how to integrate that.

1069
00:44:33.280 --> 00:44:35.799
<v Andrey> And actually, you know, where

1070
00:44:35.800 --> 00:44:38.319
<v Andrey> do you start? How do you how do you do it

1071
00:44:38.320 --> 00:44:39.799
<v Andrey> and so on, right?

1072
00:44:39.800 --> 00:44:40.800
<v Andrey> Hmm.

1073
00:44:41.560 --> 00:44:44.229
<v Andrey> So I guess maybe can

1074
00:44:44.230 --> 00:44:46.749
<v Andrey> we dove in a bit more about this idea of social

1075
00:44:46.750 --> 00:44:49.309
<v Andrey> transparency? Ethics is so important.

1076
00:44:49.310 --> 00:44:51.939
<v Andrey> And so we know because fancy sort of trying

1077
00:44:51.940 --> 00:44:55.929
<v Andrey> to understand what the algorithmic

1078
00:44:55.930 --> 00:44:58.299
<v Andrey> side of it is doing, what the model is thinking,

1079
00:44:58.300 --> 00:45:01.029
<v Andrey> but what is social

1080
00:45:01.030 --> 00:45:02.889
<v Andrey> rationality one of its components?

1081
00:45:02.890 --> 00:45:06.109
<v Andrey> And yeah, how should people understand it?

1082
00:45:06.110 --> 00:45:07.899
<v Upol> Yeah. Well, that's a that's a fascinating

1083
00:45:07.900 --> 00:45:08.900
<v Upol> question.

1084
00:45:09.370 --> 00:45:11.919
<v Upol> So to understand social

1085
00:45:11.920 --> 00:45:14.409
<v Upol> transparency, I think we have to accept a few

1086
00:45:14.410 --> 00:45:15.579
<v Upol> things.

1087
00:45:15.580 --> 00:45:18.189
<v Upol> First, we have to understand and

1088
00:45:18.190 --> 00:45:20.799
<v Upol> acknowledge that work

1089
00:45:20.800 --> 00:45:23.079
<v Upol> is social, right?

1090
00:45:23.080 --> 00:45:24.909
<v Upol> You know, we don't work in silos.

1091
00:45:24.910 --> 00:45:27.759
<v Upol> Most of us, we work within teams.

1092
00:45:27.760 --> 00:45:29.529
<v Upol> So there meet. That means right.

1093
00:45:29.530 --> 00:45:32.049
<v Upol> There is some need to add this

1094
00:45:32.050 --> 00:45:34.779
<v Upol> transparency data in an office when you're working

1095
00:45:34.780 --> 00:45:37.699
<v Upol> with a team or virtually through Slack, right?

1096
00:45:37.700 --> 00:45:39.519
<v Upol> There's a lot of chatter that is going on and

1097
00:45:39.520 --> 00:45:41.529
<v Upol> there is a necessity behind that.

1098
00:45:41.530 --> 00:45:44.949
<v Upol> So that is fast the the fast realization

1099
00:45:44.950 --> 00:45:46.449
<v Upol> that work is social.

1100
00:45:46.450 --> 00:45:48.879
<v Upol> That means there might be a need to make that

1101
00:45:48.880 --> 00:45:51.519
<v Upol> social nature a little bit transparent, especially

1102
00:45:51.520 --> 00:45:53.379
<v Upol> when we're dealing with A.I. mediated decision

1103
00:45:53.380 --> 00:45:54.819
<v Upol> support systems.

1104
00:45:54.820 --> 00:45:58.089
<v Upol> So and you know, as we share in the paper,

1105
00:45:58.090 --> 00:46:00.039
<v Upol> we were trying to.

1106
00:46:00.040 --> 00:46:02.349
<v Upol> So this is also difficult as well to some extent,

1107
00:46:02.350 --> 00:46:03.909
<v Upol> right? One of the challenges that A.I.

1108
00:46:03.910 --> 00:46:06.519
<v Upol> researchers face is how do

1109
00:46:06.520 --> 00:46:09.139
<v Upol> we know what the future really looks like

1110
00:46:10.150 --> 00:46:12.879
<v Upol> without really investing months

1111
00:46:12.880 --> 00:46:15.729
<v Upol> and months of work, building large

1112
00:46:15.730 --> 00:46:17.769
<v Upol> infrastructures and models and then realizing

1113
00:46:17.770 --> 00:46:20.469
<v Upol> we're actually not very useful, but that

1114
00:46:20.470 --> 00:46:23.089
<v Upol> that is a very hard cost

1115
00:46:23.090 --> 00:46:24.759
<v Upol> of doing this.

1116
00:46:24.760 --> 00:46:27.459
<v Upol> So due to kind of explain that

1117
00:46:27.460 --> 00:46:30.009
<v Upol> we used this notion of

1118
00:46:30.010 --> 00:46:31.119
<v Upol> scenario based design.

1119
00:46:31.120 --> 00:46:34.119
<v Upol> So this is coming from the traditions of design

1120
00:46:34.120 --> 00:46:36.699
<v Upol> fiction, as I'm drawing a lot of this

1121
00:46:36.700 --> 00:46:39.999
<v Upol> actually from the theoretical underpinnings

1122
00:46:40.000 --> 00:46:42.219
<v Upol> of the human centered explainable AI paper that we

1123
00:46:42.220 --> 00:46:43.899
<v Upol> just talked about. Mm-Hmm.

1124
00:46:43.900 --> 00:46:46.479
<v Upol> And so using scenario based design,

1125
00:46:46.480 --> 00:46:50.139
<v Upol> we conducted around four workshops

1126
00:46:50.140 --> 00:46:51.339
<v Upol> with a lot of people.

1127
00:46:51.340 --> 00:46:53.499
<v Upol> From different technology companies, just to get a

1128
00:46:53.500 --> 00:46:56.649
<v Upol> sense of what are the things

1129
00:46:56.650 --> 00:46:59.229
<v Upol> that are outside the black box that

1130
00:46:59.230 --> 00:47:02.739
<v Upol> people want when they make a decision

1131
00:47:02.740 --> 00:47:04.249
<v Upol> within the AI system.

1132
00:47:04.250 --> 00:47:06.429
<v Upol> Right. So that's the workshop is meant to kind of

1133
00:47:06.430 --> 00:47:09.069
<v Upol> get a more formative understanding, right?

1134
00:47:09.070 --> 00:47:11.799
<v Andrey> What needs to be made transparent in this social

1135
00:47:11.800 --> 00:47:13.929
<v Andrey> system in terms of

1136
00:47:13.930 --> 00:47:15.999
<v Upol> right, because there are so many things you can

1137
00:47:16.000 --> 00:47:17.289
<v Upol> make transparent, right?

1138
00:47:17.290 --> 00:47:19.479
<v Upol> Because how do you know which one is the right

1139
00:47:19.480 --> 00:47:21.819
<v Upol> thing to do right? And I think through these

1140
00:47:21.820 --> 00:47:24.789
<v Upol> workshops and this is pre-COVID,

1141
00:47:24.790 --> 00:47:27.039
<v Upol> so we have the ability to kind of get in person

1142
00:47:27.040 --> 00:47:28.989
<v Upol> and kind of have these workshops.

1143
00:47:28.990 --> 00:47:31.539
<v Upol> And what we learn was

1144
00:47:31.540 --> 00:47:33.929
<v Upol> that out of this and this is, I think, what we

1145
00:47:33.930 --> 00:47:36.699
<v Upol> what we call in the paper, the four ws,

1146
00:47:36.700 --> 00:47:39.639
<v Upol> right? So in addition

1147
00:47:39.640 --> 00:47:42.159
<v Upol> to the i's

1148
00:47:42.160 --> 00:47:43.989
<v Upol> technical transparency or algorithmic

1149
00:47:43.990 --> 00:47:47.169
<v Upol> transparency, these practitioners,

1150
00:47:47.170 --> 00:47:50.349
<v Upol> data scientists, analysts and others

1151
00:47:50.350 --> 00:47:53.079
<v Upol> wanted to know for things

1152
00:47:53.080 --> 00:47:55.659
<v Upol> who did what,

1153
00:47:55.660 --> 00:47:56.919
<v Upol> when and why.

1154
00:47:58.180 --> 00:48:00.699
<v Upol> So those became again, we are not saying

1155
00:48:00.700 --> 00:48:02.559
<v Upol> this is the end all, be all to all social

1156
00:48:02.560 --> 00:48:04.299
<v Upol> transparency. There might be other socially

1157
00:48:04.300 --> 00:48:06.969
<v Upol> transparent systems that that do

1158
00:48:06.970 --> 00:48:08.859
<v Upol> very well, but actually in the cybersecurity

1159
00:48:08.860 --> 00:48:11.559
<v Upol> example, going back to that when we

1160
00:48:11.560 --> 00:48:14.229
<v Upol> implemented this aspect of who

1161
00:48:14.230 --> 00:48:16.089
<v Upol> did what, when and why. So imagine, like, you

1162
00:48:16.090 --> 00:48:18.669
<v Upol> know, next to a threat, you

1163
00:48:18.670 --> 00:48:20.259
<v Upol> know, close these ports, right?

1164
00:48:20.260 --> 00:48:22.599
<v Upol> Let's imagine that if Julie have social

1165
00:48:22.600 --> 00:48:24.699
<v Upol> transparency, what would do we have seen?

1166
00:48:24.700 --> 00:48:26.649
<v Upol> Julie, who got fired before?

1167
00:48:26.650 --> 00:48:29.169
<v Upol> So when you get this

1168
00:48:30.580 --> 00:48:32.859
<v Upol> new disease, the air is recommending the ports to

1169
00:48:32.860 --> 00:48:35.439
<v Upol> be closed and you're like, OK.

1170
00:48:35.440 --> 00:48:37.539
<v Upol> Is that true? Like, is that real or not?

1171
00:48:37.540 --> 00:48:39.669
<v Upol> I don't know if it's a false positive.

1172
00:48:39.670 --> 00:48:42.429
<v Upol> But then Julie is able to see, you know, maybe

1173
00:48:42.430 --> 00:48:44.799
<v Upol> 10 other people dealing with a very similar

1174
00:48:44.800 --> 00:48:46.629
<v Upol> situation in the past.

1175
00:48:46.630 --> 00:48:49.179
<v Upol> And in one of those Julie scenes, I

1176
00:48:49.180 --> 00:48:51.729
<v Upol> one of the who's right? Maybe imagine this is Bob,

1177
00:48:51.730 --> 00:48:54.639
<v Upol> and Bob is a veteran in the industry.

1178
00:48:54.640 --> 00:48:57.669
<v Upol> He's like a level three analyst, and

1179
00:48:57.670 --> 00:49:00.549
<v Upol> he says, Oh, these are backup

1180
00:49:00.550 --> 00:49:02.319
<v Upol> site reports in law.

1181
00:49:02.320 --> 00:49:03.459
<v Upol> Mm. Right?

1182
00:49:03.460 --> 00:49:05.529
<v Upol> So who did what?

1183
00:49:05.530 --> 00:49:06.530
<v Upol> Right?

1184
00:49:06.940 --> 00:49:09.669
<v Upol> When? Maybe, let's say, three months ago?

1185
00:49:09.670 --> 00:49:11.799
<v Upol> And why? So the why is the reasoning right?

1186
00:49:11.800 --> 00:49:13.389
<v Upol> Like these are backup center reports

1187
00:49:14.830 --> 00:49:17.529
<v Upol> ignored by situating this extra

1188
00:49:17.530 --> 00:49:20.119
<v Upol> piece of information that is actually capturing?

1189
00:49:20.120 --> 00:49:22.239
<v Upol> You know, one might argue, Hey, people like that

1190
00:49:22.240 --> 00:49:23.319
<v Upol> seems like a bad problem.

1191
00:49:23.320 --> 00:49:25.479
<v Upol> They should've just added back to the data center,

1192
00:49:25.480 --> 00:49:27.249
<v Upol> right? That's not good.

1193
00:49:27.250 --> 00:49:29.769
<v Upol> And that is where I think the critical insight

1194
00:49:29.770 --> 00:49:32.769
<v Upol> lies. There is not enough things

1195
00:49:32.770 --> 00:49:34.539
<v Upol> you can add in the dataset.

1196
00:49:34.540 --> 00:49:37.149
<v Upol> It's like a golden goose chase

1197
00:49:37.150 --> 00:49:39.129
<v Andrey> because it's all inside the model, right?

1198
00:49:39.130 --> 00:49:42.129
<v Upol> Exactly. And sometimes things happen dynamically.

1199
00:49:42.130 --> 00:49:44.949
<v Upol> Remember, data sets are basically snapshots

1200
00:49:44.950 --> 00:49:45.950
<v Upol> of the past.

1201
00:49:46.700 --> 00:49:49.279
<v Upol> And work norms actually change

1202
00:49:49.280 --> 00:49:52.159
<v Upol> over time due to the sensitive

1203
00:49:52.160 --> 00:49:54.439
<v Upol> nature of certain cybersecurity explanation

1204
00:49:54.440 --> 00:49:55.549
<v Upol> institutions.

1205
00:49:55.550 --> 00:49:58.159
<v Upol> You do not want certain things to be coded

1206
00:49:58.160 --> 00:49:59.359
<v Upol> into a data set. Right.

1207
00:49:59.360 --> 00:50:01.699
<v Upol> Because what if that gets hacked, then all your

1208
00:50:01.700 --> 00:50:02.929
<v Upol> secrets are out.

1209
00:50:02.930 --> 00:50:05.509
<v Upol> So there will always be

1210
00:50:05.510 --> 00:50:08.269
<v Upol> elements that are not quantifiable,

1211
00:50:08.270 --> 00:50:10.949
<v Upol> that are not acceptable in a cleanly

1212
00:50:10.950 --> 00:50:12.619
<v Upol> named dataset.

1213
00:50:12.620 --> 00:50:15.199
<v Upol> In those cases, those very things that

1214
00:50:15.200 --> 00:50:17.809
<v Upol> are hard to quantify, hard to incorporate

1215
00:50:17.810 --> 00:50:20.509
<v Upol> often can be the difference maker between

1216
00:50:20.510 --> 00:50:23.329
<v Upol> right and wrong decisions where they are.

1217
00:50:23.330 --> 00:50:25.909
<v Upol> So by adding this social transparency, you are

1218
00:50:25.910 --> 00:50:28.879
<v Upol> able to inform someone

1219
00:50:28.880 --> 00:50:31.429
<v Upol> to know when to trust the A.I.

1220
00:50:31.430 --> 00:50:32.430
<v Upol> versus not.

1221
00:50:33.130 --> 00:50:34.209
<v Andrey> Mm hmm.

1222
00:50:34.210 --> 00:50:37.509
<v Andrey> Yeah, exactly, and to dig a bit deeper.

1223
00:50:37.510 --> 00:50:40.479
<v Andrey> I would love to hear how did this scenario

1224
00:50:40.480 --> 00:50:43.249
<v Andrey> based design process

1225
00:50:43.250 --> 00:50:45.909
<v Andrey> work? I think figure one of your work is

1226
00:50:45.910 --> 00:50:48.220
<v Andrey> really interesting is that the scenario used.

1227
00:50:50.020 --> 00:50:52.539
<v Upol> So you know, in this scenario,

1228
00:50:52.540 --> 00:50:55.059
<v Upol> we asked our participants to kind of envision

1229
00:50:55.060 --> 00:50:57.669
<v Upol> being in using a

1230
00:50:57.670 --> 00:51:00.579
<v Upol> AI powered pricing tool to

1231
00:51:00.580 --> 00:51:03.549
<v Upol> price and access management product

1232
00:51:03.550 --> 00:51:05.419
<v Upol> to a customer called Scout.

1233
00:51:05.420 --> 00:51:06.849
<v Upol> Right. So the A.I.

1234
00:51:06.850 --> 00:51:08.799
<v Upol> kind of does its analysis and recommends that,

1235
00:51:08.800 --> 00:51:11.319
<v Upol> hey, you got to sell it at 100 bucks per month

1236
00:51:11.320 --> 00:51:12.939
<v Upol> per account.

1237
00:51:12.940 --> 00:51:15.489
<v Upol> And it did also share some post-rock

1238
00:51:15.490 --> 00:51:17.889
<v Upol> explanations and kind of justifies, White said,

1239
00:51:17.890 --> 00:51:20.409
<v Upol> what it's saying and that the model, the

1240
00:51:20.410 --> 00:51:22.569
<v Upol> technical transparency pieces like the item, the

1241
00:51:22.570 --> 00:51:25.419
<v Upol> quotable goes off a salesperson into account.

1242
00:51:25.420 --> 00:51:27.459
<v Upol> It did a comparative pricing of what similar

1243
00:51:27.460 --> 00:51:30.249
<v Upol> customers pray, and also it gave you the floor.

1244
00:51:30.250 --> 00:51:33.249
<v Upol> So what is the cost price for doing this product?

1245
00:51:33.250 --> 00:51:35.319
<v Upol> So these are so imagine that's the first letter

1246
00:51:35.320 --> 00:51:37.209
<v Upol> and today, right?

1247
00:51:37.210 --> 00:51:38.769
<v Upol> That's the state of the art.

1248
00:51:38.770 --> 00:51:40.779
<v Upol> Nothing is better than that, right?

1249
00:51:40.780 --> 00:51:43.299
<v Upol> We don't have the social transparency that we kind

1250
00:51:43.300 --> 00:51:45.879
<v Upol> of envision in this paper, but

1251
00:51:45.880 --> 00:51:47.859
<v Upol> that is where the state of the art was.

1252
00:51:47.860 --> 00:51:49.569
<v Upol> So that was our grounding moment.

1253
00:51:49.570 --> 00:51:51.759
<v Upol> So we would ask our people as they went through

1254
00:51:51.760 --> 00:51:54.399
<v Upol> the walk to what would you do right now?

1255
00:51:54.400 --> 00:51:55.929
<v Upol> Do you think this is, you know, before we showed

1256
00:51:55.930 --> 00:51:57.329
<v Upol> them any social transparency?

1257
00:51:57.330 --> 00:52:00.249
<v Upol> Right? And we will see that most people

1258
00:52:00.250 --> 00:52:01.449
<v Upol> agree with that.

1259
00:52:01.450 --> 00:52:03.099
<v Upol> Yeah, this seems like a decent. We also kind of

1260
00:52:03.100 --> 00:52:06.009
<v Upol> calibrated the price point by asking experts.

1261
00:52:06.010 --> 00:52:08.139
<v Upol> So we kind of grounded a lot of his data, even

1262
00:52:08.140 --> 00:52:10.449
<v Upol> though it's a scenario. If it's fictional, the

1263
00:52:10.450 --> 00:52:12.489
<v Upol> fiction is grounded in reality, our version of

1264
00:52:12.490 --> 00:52:13.629
<v Upol> reality.

1265
00:52:13.630 --> 00:52:16.419
<v Upol> And then we told them, like, now imagine

1266
00:52:16.420 --> 00:52:17.629
<v Upol> what have you found out?

1267
00:52:17.630 --> 00:52:20.229
<v Upol> And that only one out of 10 people

1268
00:52:20.230 --> 00:52:22.989
<v Upol> sold this product and the recommended price?

1269
00:52:22.990 --> 00:52:24.759
<v Upol> What would you do?

1270
00:52:24.760 --> 00:52:26.559
<v Upol> And you could see our participants kind of get

1271
00:52:26.560 --> 00:52:28.119
<v Upol> very interested, like those like, oh, that's

1272
00:52:28.120 --> 00:52:31.659
<v Upol> really interesting information that

1273
00:52:31.660 --> 00:52:34.029
<v Upol> helps me calibrate what to do.

1274
00:52:34.030 --> 00:52:36.219
<v Upol> So then we kind of dug deeper, which is like the

1275
00:52:36.220 --> 00:52:39.009
<v Upol> bullet points three four five to share

1276
00:52:39.010 --> 00:52:41.739
<v Upol> three examples of past colleagues

1277
00:52:41.740 --> 00:52:44.499
<v Upol> who have dealt with the same customer

1278
00:52:44.500 --> 00:52:47.079
<v Upol> scout on similar products.

1279
00:52:47.080 --> 00:52:50.109
<v Upol> And then one of the most important comments

1280
00:52:50.110 --> 00:52:52.809
<v Upol> were made by Jessica or Jess, who's

1281
00:52:52.810 --> 00:52:54.459
<v Upol> a sales director.

1282
00:52:54.460 --> 00:52:57.009
<v Upol> And it turns out Jess had rejected the

1283
00:52:57.010 --> 00:53:00.309
<v Upol> recommendation, but the sale did happen,

1284
00:53:00.310 --> 00:53:02.439
<v Upol> and the comment was the most important where they

1285
00:53:02.440 --> 00:53:04.599
<v Upol> say that, Hey, is COVID 19.

1286
00:53:04.600 --> 00:53:06.310
<v Upol> And this was done at the height of the pandemic.

1287
00:53:08.080 --> 00:53:11.049
<v Upol> I can't lose a long term, profitable customer.

1288
00:53:11.050 --> 00:53:13.569
<v Upol> So they offered 10 percent below

1289
00:53:13.570 --> 00:53:15.969
<v Upol> the cost price. And that's an important part,

1290
00:53:15.970 --> 00:53:18.789
<v Upol> right? That not only did they give you a discount,

1291
00:53:18.790 --> 00:53:21.459
<v Upol> but just the director had given

1292
00:53:21.460 --> 00:53:24.639
<v Upol> them below the cost price and that

1293
00:53:24.640 --> 00:53:27.159
<v Upol> social context of what was going on that

1294
00:53:27.160 --> 00:53:29.169
<v Upol> was outside of the algorithm, right?

1295
00:53:29.170 --> 00:53:32.139
<v Upol> Very much inform how people acted on it

1296
00:53:32.140 --> 00:53:34.719
<v Upol> because remember, without any of this context,

1297
00:53:34.720 --> 00:53:36.339
<v Upol> they fell. The price was fair.

1298
00:53:36.340 --> 00:53:37.719
<v Upol> It was done the right way.

1299
00:53:37.720 --> 00:53:40.359
<v Upol> You know, the justifications were right,

1300
00:53:40.360 --> 00:53:44.379
<v Upol> but very few people actually,

1301
00:53:44.380 --> 00:53:46.929
<v Upol> you know, offered the same price

1302
00:53:46.930 --> 00:53:49.239
<v Upol> when they knew what others had done, especially

1303
00:53:49.240 --> 00:53:52.659
<v Upol> when a director level person had done it before.

1304
00:53:52.660 --> 00:53:55.209
<v Andrey> Yes. So in a sense, I think

1305
00:53:55.210 --> 00:53:57.339
<v Andrey> going back to something you mentioned, it's

1306
00:53:57.340 --> 00:54:00.219
<v Andrey> letting you know what the model doesn't know.

1307
00:54:00.220 --> 00:54:03.039
<v Andrey> Right? It doesn't know about COVID

1308
00:54:03.040 --> 00:54:04.869
<v Andrey> and these four W's.

1309
00:54:04.870 --> 00:54:06.909
<v Andrey> The social scenario doesn't really explain the

1310
00:54:06.910 --> 00:54:09.549
<v Andrey> mottoes decisions, but it

1311
00:54:09.550 --> 00:54:12.299
<v Andrey> does like to understand via

1312
00:54:12.300 --> 00:54:15.039
<v Andrey> a system better in the sense of like

1313
00:54:15.040 --> 00:54:17.619
<v Andrey> the AI system is situated within the

1314
00:54:17.620 --> 00:54:18.969
<v Andrey> organization.

1315
00:54:18.970 --> 00:54:21.819
<v Andrey> And so you get to know more its weaknesses

1316
00:54:21.820 --> 00:54:23.439
<v Andrey> and where and when to follow it.

1317
00:54:23.440 --> 00:54:26.079
<v Andrey> Maybe when not, which I

1318
00:54:26.080 --> 00:54:28.689
<v Andrey> think would be a lot harder about seeing like,

1319
00:54:28.690 --> 00:54:30.729
<v Andrey> OK, this first person accepted discrimination.

1320
00:54:30.730 --> 00:54:32.110
<v Andrey> This person didn't.

1321
00:54:33.460 --> 00:54:35.919
<v Andrey> And this figure, I think, illustrates that really

1322
00:54:35.920 --> 00:54:36.819
<v Andrey> well.

1323
00:54:36.820 --> 00:54:39.189
<v Upol> And I think it's kind of asking the question like,

1324
00:54:39.190 --> 00:54:41.739
<v Upol> what are the eyes blind spots and

1325
00:54:41.740 --> 00:54:43.989
<v Upol> can other humans who have interacted with this

1326
00:54:43.990 --> 00:54:46.239
<v Upol> system in the past and address it?

1327
00:54:46.240 --> 00:54:49.029
<v Upol> So like, for instance, I am now currently working

1328
00:54:49.030 --> 00:54:51.849
<v Upol> with radiation oncologists on a very similar

1329
00:54:51.850 --> 00:54:54.610
<v Upol> project and in radiation oncology,

1330
00:54:55.990 --> 00:54:59.259
<v Upol> just like in other fields, there is no

1331
00:54:59.260 --> 00:55:00.579
<v Upol> absolute ground truth.

1332
00:55:00.580 --> 00:55:02.709
<v Upol> With 80 senators, we are so comfortable with the

1333
00:55:02.710 --> 00:55:05.109
<v Upol> terminology of ground truth, right?

1334
00:55:05.110 --> 00:55:07.659
<v Upol> But when it comes to using radiation to

1335
00:55:07.660 --> 00:55:10.539
<v Upol> treat cancers, they're established practices.

1336
00:55:10.540 --> 00:55:13.479
<v Upol> There is no like absolute gold thing that everyone

1337
00:55:13.480 --> 00:55:16.869
<v Upol> must do because each patient is different.

1338
00:55:16.870 --> 00:55:19.239
<v Upol> Each treatment facility is different.

1339
00:55:19.240 --> 00:55:20.590
<v Upol> So in that case?

1340
00:55:21.840 --> 00:55:24.989
<v Upol> Knowing when to trust these recommendations,

1341
00:55:24.990 --> 00:55:27.269
<v Upol> foreign by saying, you know, give this much

1342
00:55:27.270 --> 00:55:30.179
<v Upol> radiation to the patient's left optic nerve.

1343
00:55:30.180 --> 00:55:32.819
<v Upol> Right. That's a very high stakes decision,

1344
00:55:32.820 --> 00:55:34.679
<v Upol> right? Because if you do it the wrong way, you can

1345
00:55:34.680 --> 00:55:36.839
<v Upol> blast away my left optic there and take away my

1346
00:55:36.840 --> 00:55:38.369
<v Upol> vision. Right?

1347
00:55:38.370 --> 00:55:40.079
<v Upol> But guess what? What the A.I.

1348
00:55:40.080 --> 00:55:42.689
<v Upol> system might not have known is that

1349
00:55:42.690 --> 00:55:45.059
<v Upol> the patient is blind in the right die.

1350
00:55:45.060 --> 00:55:47.589
<v Upol> So all of the calculus goes away because

1351
00:55:47.590 --> 00:55:49.649
<v Upol> we know there's no like central blindness data in

1352
00:55:49.650 --> 00:55:51.519
<v Upol> randomized controlled trials.

1353
00:55:51.520 --> 00:55:54.269
<v Upol> Right? So just knowing that extra

1354
00:55:54.270 --> 00:55:56.969
<v Upol> piece can help you calibrate how much

1355
00:55:56.970 --> 00:55:59.579
<v Upol> treatment you want to give it and knowing what

1356
00:55:59.580 --> 00:56:01.889
<v Upol> your peers done right, because in these kind of

1357
00:56:01.890 --> 00:56:04.649
<v Upol> communities of practices is very much

1358
00:56:04.650 --> 00:56:06.119
<v Upol> community driven, right?

1359
00:56:06.120 --> 00:56:08.759
<v Upol> Like the radiation oncologist kind of

1360
00:56:08.760 --> 00:56:11.489
<v Upol> have these standards that they co-developed

1361
00:56:11.490 --> 00:56:13.199
<v Upol> together are two studies.

1362
00:56:13.200 --> 00:56:15.899
<v Upol> So this social transparency starts mattering

1363
00:56:15.900 --> 00:56:18.839
<v Upol> extremely when the cost of failure

1364
00:56:18.840 --> 00:56:21.179
<v Upol> is also very high, right?

1365
00:56:21.180 --> 00:56:23.389
<v Upol> Like, you know, blasting someone's optic nerve

1366
00:56:23.390 --> 00:56:25.979
<v Upol> nerve out there, the radiation is a pretty high

1367
00:56:25.980 --> 00:56:28.529
<v Upol> cost rather than, you know, missing a song

1368
00:56:28.530 --> 00:56:29.969
<v Upol> recommendation.

1369
00:56:29.970 --> 00:56:31.739
<v Upol> And I think that's the other part like you don't

1370
00:56:31.740 --> 00:56:34.049
<v Upol> think I don't. Social transparency is not really

1371
00:56:34.050 --> 00:56:36.839
<v Upol> helpful when the stakes are low

1372
00:56:36.840 --> 00:56:38.699
<v Upol> or when the nature of the job is not very

1373
00:56:38.700 --> 00:56:40.529
<v Upol> collaborative, right?

1374
00:56:40.530 --> 00:56:42.989
<v Upol> But the more the stakes are high, the more

1375
00:56:42.990 --> 00:56:44.429
<v Upol> collaboration is needed.

1376
00:56:44.430 --> 00:56:47.129
<v Upol> Social transparency becomes important because what

1377
00:56:47.130 --> 00:56:48.389
<v Upol> then becomes very social.

1378
00:56:49.770 --> 00:56:52.319
<v Andrey> Yeah, it's just makes me feel like you

1379
00:56:52.320 --> 00:56:55.139
<v Andrey> can almost consider

1380
00:56:55.140 --> 00:56:57.929
<v Andrey> this like what if the AI

1381
00:56:57.930 --> 00:57:00.719
<v Andrey> model is in some sense, a coworker,

1382
00:57:00.720 --> 00:57:03.449
<v Andrey> right? When you work with people, some

1383
00:57:03.450 --> 00:57:06.299
<v Andrey> people you trust more and less and when you do

1384
00:57:06.300 --> 00:57:08.459
<v Andrey> decision making, it is sort of collaborative.

1385
00:57:08.460 --> 00:57:10.859
<v Andrey> You know, you might debate, you might ask whatever

1386
00:57:10.860 --> 00:57:13.379
<v Andrey> you consider, address, if you consider that that's

1387
00:57:13.380 --> 00:57:15.929
<v Andrey> not something that you can do with any

1388
00:57:15.930 --> 00:57:18.509
<v Andrey> AI system, at least for now, you can't say, well,

1389
00:57:18.510 --> 00:57:20.579
<v Andrey> you know, have you taken this into that into

1390
00:57:20.580 --> 00:57:23.489
<v Andrey> account? But seeing the social

1391
00:57:23.490 --> 00:57:26.519
<v Andrey> context, it seems to me, was what people

1392
00:57:26.520 --> 00:57:29.159
<v Andrey> might have realized that to me, this comment

1393
00:57:29.160 --> 00:57:31.259
<v Andrey> and now, you know, didn't take into account the

1394
00:57:31.260 --> 00:57:32.260
<v Andrey> COVID thing.

1395
00:57:33.390 --> 00:57:35.069
<v Andrey> So, yeah, she gets interesting in the sense of

1396
00:57:35.070 --> 00:57:38.879
<v Andrey> like you get to know the system

1397
00:57:38.880 --> 00:57:41.609
<v Andrey> as another entity you work with

1398
00:57:41.610 --> 00:57:43.049
<v Andrey> almost.

1399
00:57:43.050 --> 00:57:44.279
<v Upol> Yeah, yeah, exactly.

1400
00:57:44.280 --> 00:57:46.289
<v Upol> And I think that's kind of changes the way we

1401
00:57:46.290 --> 00:57:49.439
<v Upol> think of human collaboration, right?

1402
00:57:49.440 --> 00:57:51.959
<v Upol> Because you are now like, I often think about

1403
00:57:51.960 --> 00:57:53.399
<v Upol> it, it's like, you know, Avatar The Last

1404
00:57:53.400 --> 00:57:54.479
<v Upol> Airbender.

1405
00:57:54.480 --> 00:57:55.179
<v Upol> I don't know.

1406
00:57:55.180 --> 00:57:55.769
<v Andrey> Yeah, yeah.

1407
00:57:55.770 --> 00:57:57.329
<v Upol> So it's like, what does Avatar do?

1408
00:57:57.330 --> 00:57:59.999
<v Upol> And new faces around there, like Avatar and right?

1409
00:58:00.000 --> 00:58:02.159
<v Upol> Like when he faces some difficult choices, he kind

1410
00:58:02.160 --> 00:58:04.799
<v Upol> of seeks the counsel of

1411
00:58:04.800 --> 00:58:07.619
<v Upol> past avatars who had come before

1412
00:58:07.620 --> 00:58:10.259
<v Upol> him. And so the social transparency

1413
00:58:10.260 --> 00:58:13.199
<v Upol> is in a weird way of capturing that historical

1414
00:58:13.200 --> 00:58:16.349
<v Upol> context in a system

1415
00:58:16.350 --> 00:58:18.899
<v Upol> in situ that really makes your

1416
00:58:18.900 --> 00:58:22.529
<v Upol> decision making in that moment much more informed.

1417
00:58:22.530 --> 00:58:23.969
<v Upol> Because at the end of the day, we have to ask

1418
00:58:23.970 --> 00:58:27.029
<v Upol> ourselves why these explanations aren't there.

1419
00:58:27.030 --> 00:58:30.509
<v Upol> They're there to make things actionable.

1420
00:58:30.510 --> 00:58:32.759
<v Upol> If you if someone cannot do something without

1421
00:58:32.760 --> 00:58:35.339
<v Upol> explanation, then there might not be any

1422
00:58:35.340 --> 00:58:37.149
<v Upol> explanation, right?

1423
00:58:37.150 --> 00:58:39.089
<v Upol> Like if the machine is explaining itself and I

1424
00:58:39.090 --> 00:58:41.609
<v Upol> cannot do anything with it, that's very difficult.

1425
00:58:41.610 --> 00:58:44.309
<v Upol> Like, I don't know what purpose of serving

1426
00:58:44.310 --> 00:58:45.719
<v Upol> other than just understanding.

1427
00:58:45.720 --> 00:58:47.729
<v Upol> But if I understand and cannot do anything with

1428
00:58:47.730 --> 00:58:50.069
<v Upol> understanding, what is it there for anyway?

1429
00:58:50.070 --> 00:58:52.919
<v Upol> So social transparency can make things

1430
00:58:52.920 --> 00:58:55.499
<v Upol> more actionable, even if right,

1431
00:58:55.500 --> 00:58:57.199
<v Upol> even if.

1432
00:58:57.200 --> 00:59:00.169
<v Upol> The participants are saying no

1433
00:59:00.170 --> 00:59:02.509
<v Upol> to the system, and that's the crucial part.

1434
00:59:02.510 --> 00:59:05.119
<v Upol> I think it changes how we formulate

1435
00:59:05.120 --> 00:59:07.759
<v Upol> trust because a lot of the work around

1436
00:59:07.760 --> 00:59:10.369
<v Upol> trust that we see is around user

1437
00:59:10.370 --> 00:59:13.019
<v Upol> acceptance. I want my user to like the

1438
00:59:13.020 --> 00:59:15.409
<v Upol> I want my user to accept me.

1439
00:59:15.410 --> 00:59:17.629
<v Upol> But I think what we are seeing is it's not about

1440
00:59:17.630 --> 00:59:20.269
<v Upol> just mindlessly fostering

1441
00:59:20.270 --> 00:59:21.379
<v Upol> trust.

1442
00:59:21.380 --> 00:59:24.259
<v Upol> It's about mindfully calibrating

1443
00:59:24.260 --> 00:59:25.489
<v Upol> trust.

1444
00:59:25.490 --> 00:59:28.009
<v Upol> You don't want people to over trust your system

1445
00:59:28.010 --> 00:59:29.570
<v Upol> because then there are liability issues.

1446
00:59:30.780 --> 00:59:32.259
<v Andrey> Yeah, exactly.

1447
00:59:32.260 --> 00:59:34.979
<v Andrey> And, yeah, so in terms of

1448
00:59:34.980 --> 00:59:38.309
<v Andrey> this process, he started these four workshops

1449
00:59:38.310 --> 00:59:40.859
<v Andrey> you, I think, carried out this idea of the

1450
00:59:40.860 --> 00:59:44.469
<v Andrey> four W's what who lives.

1451
00:59:44.470 --> 00:59:46.709
<v Andrey> And if I understand correctly after the workshops,

1452
00:59:46.710 --> 00:59:49.409
<v Andrey> you then had sort of a more

1453
00:59:49.410 --> 00:59:52.769
<v Andrey> controlled study where 29 participants.

1454
00:59:52.770 --> 00:59:53.879
<v Andrey> Is that right?

1455
00:59:53.880 --> 00:59:56.339
<v Upol> Yeah, yeah. So then then we really did this once

1456
00:59:56.340 --> 00:59:57.839
<v Upol> we built the scenario, right?

1457
00:59:57.840 --> 00:59:59.939
<v Upol> Then you kind of when made people go through the

1458
00:59:59.940 --> 01:00:01.469
<v Upol> scenario of the study.

1459
01:00:01.470 --> 01:00:03.419
<v Upol> So we would walk them through the scenario just

1460
01:00:03.420 --> 01:00:06.359
<v Upol> the way I kind of described a few minutes ago.

1461
01:00:06.360 --> 01:00:08.879
<v Upol> And we will start seeing that how they start

1462
01:00:08.880 --> 01:00:10.859
<v Upol> thinking through this and this is the beauty of

1463
01:00:10.860 --> 01:00:12.359
<v Upol> scenario, these design, right?

1464
01:00:12.360 --> 01:00:14.639
<v Upol> You can think of this as a probe.

1465
01:00:14.640 --> 01:00:17.219
<v Upol> So you are probing for reactions

1466
01:00:17.220 --> 01:00:20.219
<v Upol> about an air power system without

1467
01:00:20.220 --> 01:00:23.069
<v Upol> really needing to invest the severe infrastructure

1468
01:00:23.070 --> 01:00:25.679
<v Upol> that is needed to make a like a full fledged AI

1469
01:00:25.680 --> 01:00:29.039
<v Upol> system. But you're getting very good design

1470
01:00:29.040 --> 01:00:31.709
<v Upol> elements out of this for a lot less

1471
01:00:31.710 --> 01:00:34.529
<v Upol> cost. It does not mean that you don't build

1472
01:00:34.530 --> 01:00:36.209
<v Upol> a system, of course you do.

1473
01:00:36.210 --> 01:00:39.629
<v Upol> So, for instance, in the cybersecurity case,

1474
01:00:39.630 --> 01:00:42.119
<v Upol> once we did very similar studies with them, with

1475
01:00:42.120 --> 01:00:44.759
<v Upol> scenarios, we went and we built out,

1476
01:00:44.760 --> 01:00:46.639
<v Upol> this takes. And guess what, right?

1477
01:00:46.640 --> 01:00:50.099
<v Upol> Like two years into the project with them,

1478
01:00:50.100 --> 01:00:52.859
<v Upol> that company actually now lives

1479
01:00:52.860 --> 01:00:55.709
<v Upol> in a socially transparent world where all

1480
01:00:55.710 --> 01:00:57.749
<v Upol> their decisions are actually automatically

1481
01:00:57.750 --> 01:01:00.839
<v Upol> situated with prior history.

1482
01:01:00.840 --> 01:01:03.959
<v Upol> And they are actually you able to use

1483
01:01:03.960 --> 01:01:07.109
<v Upol> the four W's as training

1484
01:01:07.110 --> 01:01:09.449
<v Upol> to retrain their models so that the decision is

1485
01:01:09.450 --> 01:01:12.119
<v Upol> not only just algorithmically situated but

1486
01:01:12.120 --> 01:01:14.099
<v Upol> also socially situated, right?

1487
01:01:14.100 --> 01:01:17.099
<v Andrey> So it becomes of this sort of feature space

1488
01:01:17.100 --> 01:01:19.829
<v Andrey> to model is informed by it seems.

1489
01:01:19.830 --> 01:01:21.809
<v Upol> Absolutely. And then think about it that way,

1490
01:01:21.810 --> 01:01:24.539
<v Upol> right? Like you are also getting a corpus without

1491
01:01:24.540 --> 01:01:25.800
<v Upol> actually building a corpus.

1492
01:01:26.970 --> 01:01:29.189
<v Upol> Right. Because over time, what is going to happen?

1493
01:01:29.190 --> 01:01:31.679
<v Upol> These four W's are going to get enough density

1494
01:01:31.680 --> 01:01:34.109
<v Upol> depending on who knows, right where they become

1495
01:01:34.110 --> 01:01:36.179
<v Upol> large enough that you can feed back into the

1496
01:01:36.180 --> 01:01:39.599
<v Upol> model. But the cool part is

1497
01:01:39.600 --> 01:01:42.959
<v Upol> from day one, they're giving value to the user.

1498
01:01:42.960 --> 01:01:45.150
<v Upol> Right? So they're not like being

1499
01:01:46.440 --> 01:01:49.019
<v Upol> a grunt work of a data set building task.

1500
01:01:49.020 --> 01:01:50.579
<v Upol> That is the one thing that a lot of my

1501
01:01:50.580 --> 01:01:53.519
<v Upol> cybersecurity analyst stakeholders would counter

1502
01:01:53.520 --> 01:01:55.769
<v Upol> that like, Hey, this is actually useful.

1503
01:01:55.770 --> 01:01:58.229
<v Upol> I like doing this because it doesn't make me feel

1504
01:01:58.230 --> 01:02:00.479
<v Upol> like I'm building a stupid dataset that I might

1505
01:02:00.480 --> 01:02:01.739
<v Upol> not ever see.

1506
01:02:01.740 --> 01:02:04.499
<v Upol> So they're actually building a corpus

1507
01:02:04.500 --> 01:02:07.109
<v Upol> while getting value from it, which is very hard to

1508
01:02:07.110 --> 01:02:09.989
<v Upol> achieve in any kind of dataset building tasks.

1509
01:02:09.990 --> 01:02:12.329
<v Andrey> Mm hmm. Yeah, exactly.

1510
01:02:12.330 --> 01:02:14.829
<v Andrey> And it's interesting to hear that they

1511
01:02:14.830 --> 01:02:17.069
<v Andrey> are more into it.

1512
01:02:17.070 --> 01:02:19.769
<v Andrey> And I think it's also funny that, you know, having

1513
01:02:19.770 --> 01:02:23.129
<v Andrey> done a study in the paper, you are able to

1514
01:02:23.130 --> 01:02:25.649
<v Andrey> not just give you on tape, but actually

1515
01:02:25.650 --> 01:02:28.199
<v Andrey> quotes the study participants and

1516
01:02:28.200 --> 01:02:30.899
<v Andrey> really, you know, showing their

1517
01:02:30.900 --> 01:02:33.779
<v Andrey> words very concretely

1518
01:02:33.780 --> 01:02:36.119
<v Andrey> how you came to your conclusion.

1519
01:02:36.120 --> 01:02:38.759
<v Andrey> So for instance, one quote that I think

1520
01:02:38.760 --> 01:02:41.339
<v Andrey> is really relevant is I hate how it just

1521
01:02:41.340 --> 01:02:43.679
<v Andrey> gives me a confidence level in gibberish to

1522
01:02:43.680 --> 01:02:46.449
<v Andrey> engineers will understand for zero context,

1523
01:02:46.450 --> 01:02:49.169
<v Andrey> right? It's a very human reaction

1524
01:02:49.170 --> 01:02:52.149
<v Andrey> that really tells you, Well, you know,

1525
01:02:52.150 --> 01:02:53.939
<v Andrey> this person, what's the context, right?

1526
01:02:55.230 --> 01:02:57.269
<v Andrey> So then, you know, we've talked through somebody

1527
01:02:57.270 --> 01:02:59.969
<v Andrey> resembles your study, and

1528
01:02:59.970 --> 01:03:03.059
<v Andrey> now I think we can dove in a little more.

1529
01:03:03.060 --> 01:03:05.729
<v Andrey> So we've talked about social concerns.

1530
01:03:05.730 --> 01:03:08.069
<v Andrey> And I think in the paper, you also break down a

1531
01:03:08.070 --> 01:03:10.859
<v Andrey> little bit what exactly is

1532
01:03:10.860 --> 01:03:13.439
<v Andrey> made visible, what you want

1533
01:03:13.440 --> 01:03:15.929
<v Andrey> to make visible. So the decision making context

1534
01:03:15.930 --> 01:03:18.179
<v Andrey> and the organizational context.

1535
01:03:18.180 --> 01:03:20.729
<v Andrey> So yeah, what is involved

1536
01:03:20.730 --> 01:03:22.469
<v Andrey> in these things that people really need to

1537
01:03:22.470 --> 01:03:23.699
<v Andrey> understand?

1538
01:03:23.700 --> 01:03:26.459
<v Upol> Yeah. So, you know, through the four W's.

1539
01:03:26.460 --> 01:03:28.619
<v Upol> So these are the kind of like that you can think

1540
01:03:28.620 --> 01:03:31.559
<v Upol> of them, the vehicles that carry this context,

1541
01:03:31.560 --> 01:03:32.709
<v Upol> right? Mm hmm.

1542
01:03:32.710 --> 01:03:35.489
<v Upol> So the four is the first thing,

1543
01:03:35.490 --> 01:03:38.159
<v Upol> and I think we kind of shared a framework that

1544
01:03:38.160 --> 01:03:40.769
<v Upol> sees how it makes context

1545
01:03:40.770 --> 01:03:43.429
<v Upol> visible at three levels, which do the

1546
01:03:43.430 --> 01:03:45.749
<v Upol> technical, the decision making and the

1547
01:03:45.750 --> 01:03:47.669
<v Upol> organizational right.

1548
01:03:47.670 --> 01:03:48.839
<v Upol> So but

1549
01:03:50.280 --> 01:03:53.069
<v Upol> with the umbrella of all three

1550
01:03:53.070 --> 01:03:55.829
<v Upol> is the first to use what we call cool knowledge,

1551
01:03:55.830 --> 01:03:56.939
<v Upol> right?

1552
01:03:56.940 --> 01:03:59.519
<v Upol> So crew knowledge is really an important

1553
01:03:59.520 --> 01:04:02.399
<v Upol> part of these informal knowledge

1554
01:04:02.400 --> 01:04:04.409
<v Upol> that is acquired through hands on experience.

1555
01:04:04.410 --> 01:04:07.049
<v Upol> It's part, and it's tacit of

1556
01:04:07.050 --> 01:04:09.059
<v Upol> every job that anyone has ever done.

1557
01:04:09.060 --> 01:04:12.199
<v Upol> Right? And it's often situated locally

1558
01:04:12.200 --> 01:04:14.999
<v Upol> in a in a tight knit community of practice,

1559
01:04:15.000 --> 01:04:17.609
<v Upol> sort of like an aggregated set of Milhouse.

1560
01:04:17.610 --> 01:04:20.249
<v Upol> Right? So the Y

1561
01:04:20.250 --> 01:04:23.189
<v Upol> right, the Y is actually giving insight

1562
01:04:23.190 --> 01:04:24.899
<v Upol> into that knowledge.

1563
01:04:24.900 --> 01:04:26.699
<v Upol> These are the variables that would be important

1564
01:04:26.700 --> 01:04:28.979
<v Upol> for decision making, but sometimes are not

1565
01:04:28.980 --> 01:04:32.119
<v Upol> captured in the eyes kind of feature space.

1566
01:04:32.120 --> 01:04:34.769
<v Upol> Right? The other part is like social

1567
01:04:34.770 --> 01:04:36.960
<v Upol> transparency can support analogical reasoning

1568
01:04:37.980 --> 01:04:39.929
<v Upol> in terms of like, OK, if someone has made the

1569
01:04:39.930 --> 01:04:42.329
<v Upol> decision in the past, like you remember, just I

1570
01:04:42.330 --> 01:04:44.849
<v Upol> just gave a discount for the COVID

1571
01:04:44.850 --> 01:04:47.939
<v Upol> case. So that means I too can give

1572
01:04:47.940 --> 01:04:49.799
<v Upol> the discount on the Kovic case.

1573
01:04:49.800 --> 01:04:51.059
<v Upol> Right?

1574
01:04:51.060 --> 01:04:52.949
<v Upol> So it's aiming.

1575
01:04:52.950 --> 01:04:55.409
<v Upol> So at the at the technical level, it helps you

1576
01:04:55.410 --> 01:04:58.349
<v Upol> calibrate the trust on the air right

1577
01:04:58.350 --> 01:05:00.179
<v Upol> and the decision making level.

1578
01:05:00.180 --> 01:05:02.759
<v Upol> It can foster a sense of confidence and

1579
01:05:02.760 --> 01:05:05.339
<v Upol> a decision making resilience that

1580
01:05:05.340 --> 01:05:06.839
<v Upol> you know. How good are you?

1581
01:05:06.840 --> 01:05:09.509
<v Upol> Can you trust A.I.? Can you not trust and

1582
01:05:09.510 --> 01:05:10.859
<v Upol> self confidence, right?

1583
01:05:10.860 --> 01:05:13.379
<v Upol> Yes, these difference between like, do I trust

1584
01:05:13.380 --> 01:05:15.959
<v Upol> the A.I. versus do I trust myself to act

1585
01:05:15.960 --> 01:05:17.109
<v Upol> on that?

1586
01:05:17.110 --> 01:05:18.689
<v Upol> And I think those two are slightly different

1587
01:05:18.690 --> 01:05:22.139
<v Upol> constructs and organizationally

1588
01:05:22.140 --> 01:05:24.959
<v Upol> leases. So for them to use is capturing

1589
01:05:24.960 --> 01:05:27.509
<v Upol> these tacit knowledge and the meta knowledge

1590
01:05:27.510 --> 01:05:29.139
<v Upol> of how an organization will work.

1591
01:05:29.140 --> 01:05:32.129
<v Upol> So you get an understanding of norms and values,

1592
01:05:32.130 --> 01:05:33.329
<v Upol> right?

1593
01:05:33.330 --> 01:05:35.759
<v Upol> It kind of encodes a level of institutional

1594
01:05:35.760 --> 01:05:38.729
<v Upol> memory, and it promotes

1595
01:05:38.730 --> 01:05:41.129
<v Upol> accountability because you can audit it, right?

1596
01:05:41.130 --> 01:05:43.649
<v Upol> If you know who did what, when and why you

1597
01:05:43.650 --> 01:05:45.420
<v Upol> can go back and audit things.

1598
01:05:46.470 --> 01:05:48.719
<v Upol> So those are some of the things that we got out of

1599
01:05:48.720 --> 01:05:51.239
<v Upol> this that are helpful when it comes to

1600
01:05:51.240 --> 01:05:53.969
<v Upol> making the AI powered decision making.

1601
01:05:53.970 --> 01:05:56.399
<v Andrey> Yeah, it's quite interesting this notion of

1602
01:05:56.400 --> 01:05:58.739
<v Andrey> decision making and organizational context.

1603
01:05:58.740 --> 01:06:01.289
<v Andrey> I think you define decision

1604
01:06:01.290 --> 01:06:03.839
<v Andrey> as sort of, you know, localized

1605
01:06:03.840 --> 01:06:06.389
<v Andrey> to a decision. So like, you know, you choose a

1606
01:06:06.390 --> 01:06:08.759
<v Andrey> price quarter, you you think about similar price

1607
01:06:08.760 --> 01:06:11.429
<v Andrey> quotas. Organization context is something

1608
01:06:11.430 --> 01:06:14.849
<v Andrey> that's easier to forget, but it's sort of,

1609
01:06:14.850 --> 01:06:17.459
<v Andrey> you know, what do we stand for?

1610
01:06:17.460 --> 01:06:20.049
<v Andrey> You know, how aggressive are we?

1611
01:06:20.050 --> 01:06:22.289
<v Andrey> You know, these sorts of things that are or in

1612
01:06:22.290 --> 01:06:25.259
<v Andrey> general and

1613
01:06:25.260 --> 01:06:26.389
<v Andrey> yeah, pretty interesting to.

1614
01:06:26.390 --> 01:06:29.159
<v Andrey> I think and I guess you came

1615
01:06:29.160 --> 01:06:32.159
<v Andrey> to understanding this sort of split

1616
01:06:32.160 --> 01:06:34.919
<v Andrey> by just seeing what people used

1617
01:06:34.920 --> 01:06:37.739
<v Andrey> or included in their four W's.

1618
01:06:37.740 --> 01:06:40.349
<v Upol> Yes, I think this came back from those workshops.

1619
01:06:40.350 --> 01:06:42.449
<v Upol> I think where we kind of understand like, OK,

1620
01:06:42.450 --> 01:06:44.009
<v Upol> because, you know, we were thinking, maybe there

1621
01:06:44.010 --> 01:06:46.529
<v Upol> is an h like how maybe there is

1622
01:06:46.530 --> 01:06:48.659
<v Upol> another W like where.

1623
01:06:48.660 --> 01:06:50.400
<v Upol> So we were trying to understand

1624
01:06:51.450 --> 01:06:54.089
<v Upol> what would be the minimum viable product, so

1625
01:06:54.090 --> 01:06:56.969
<v Upol> to speak about in the in the social transparency,

1626
01:06:56.970 --> 01:06:58.769
<v Upol> because we didn't also want to overwhelm people.

1627
01:06:59.820 --> 01:07:01.979
<v Upol> So the way we kind of understood like what they

1628
01:07:01.980 --> 01:07:04.709
<v Upol> were doing is actually through the case

1629
01:07:04.710 --> 01:07:06.449
<v Upol> studies that we have done in addition to this

1630
01:07:06.450 --> 01:07:07.529
<v Upol> study. Right.

1631
01:07:07.530 --> 01:07:08.789
<v Upol> So we were trying.

1632
01:07:08.790 --> 01:07:11.429
<v Upol> We were inspired by that aspect.

1633
01:07:11.430 --> 01:07:13.679
<v Upol> And that's why, like, you know, in the in table

1634
01:07:13.680 --> 01:07:16.289
<v Upol> three, we kind of talk about, you know

1635
01:07:16.290 --> 01:07:17.829
<v Upol> what? What was it?

1636
01:07:17.830 --> 01:07:20.759
<v Upol> So it's an action taken on the API, the decision

1637
01:07:20.760 --> 01:07:23.699
<v Upol> outcome. Why is like the comments

1638
01:07:23.700 --> 01:07:26.879
<v Upol> with the rationale that justifies the decision

1639
01:07:26.880 --> 01:07:29.309
<v Upol> because of what and why is always linked?

1640
01:07:29.310 --> 01:07:32.159
<v Upol> And then who the name the organizational

1641
01:07:32.160 --> 01:07:35.129
<v Upol> role, because sometimes seniority starts

1642
01:07:35.130 --> 01:07:38.039
<v Upol> playing a role like it was a director.

1643
01:07:38.040 --> 01:07:40.709
<v Upol> You kind of take their their view a little

1644
01:07:40.710 --> 01:07:42.659
<v Upol> more than others.

1645
01:07:42.660 --> 01:07:44.939
<v Upol> And then when is the time of decision?

1646
01:07:44.940 --> 01:07:47.219
<v Upol> And that's important because sometimes something

1647
01:07:47.220 --> 01:07:49.319
<v Upol> some decisions are not relevant, right?

1648
01:07:49.320 --> 01:07:51.689
<v Upol> So think about like, you know, pre-COVID,

1649
01:07:51.690 --> 01:07:53.789
<v Upol> decisions do not become very relevant during

1650
01:07:53.790 --> 01:07:56.099
<v Upol> COVID. So those are some of the things that we

1651
01:07:56.100 --> 01:07:58.859
<v Upol> found, not just by our workshops,

1652
01:07:58.860 --> 01:08:01.529
<v Upol> but also analyzing the data, the qualitative data

1653
01:08:01.530 --> 01:08:03.689
<v Upol> through the interviews and the walk throughs that

1654
01:08:03.690 --> 01:08:04.690
<v Upol> we had.

1655
01:08:05.280 --> 01:08:08.099
<v Andrey> Yeah. And then you found, you know, the what

1656
01:08:08.100 --> 01:08:10.679
<v Andrey> is really important, the why is important, the

1657
01:08:10.680 --> 01:08:13.059
<v Andrey> when, you know, sometimes, but you know.

1658
01:08:13.060 --> 01:08:15.569
<v Andrey> Yeah. And then also, I think

1659
01:08:15.570 --> 01:08:18.148
<v Andrey> probably in fungi, sort of how you

1660
01:08:18.149 --> 01:08:19.559
<v Andrey> present things.

1661
01:08:19.560 --> 01:08:21.539
<v Upol> We actually asked our participants at the end of

1662
01:08:21.540 --> 01:08:23.969
<v Upol> it. I'm like, Can you rank it and tell me why?

1663
01:08:23.970 --> 01:08:27.059
<v Upol> Right? So we would make them rank the them

1664
01:08:27.060 --> 01:08:29.278
<v Upol> like, tell me what you cannot live without and

1665
01:08:29.279 --> 01:08:32.068
<v Upol> everyone saying, I can't live without the what.

1666
01:08:32.069 --> 01:08:33.809
<v Upol> And then I said, OK, imagine that I can give you

1667
01:08:33.810 --> 01:08:35.429
<v Upol> one more. What would that be like?

1668
01:08:35.430 --> 01:08:37.108
<v Upol> Oh, I need another wide.

1669
01:08:37.109 --> 01:08:39.398
<v Upol> And then I said, Now imagine I give you one more.

1670
01:08:39.399 --> 01:08:41.099
<v Upol> That's I need to know the whole.

1671
01:08:41.100 --> 01:08:43.469
<v Upol> So that's how we kind of made them do this ranking

1672
01:08:43.470 --> 01:08:46.169
<v Upol> task and then get a sense of importance,

1673
01:08:46.170 --> 01:08:48.719
<v Upol> because sometimes many companies might not have

1674
01:08:48.720 --> 01:08:50.278
<v Upol> all before that.

1675
01:08:50.279 --> 01:08:52.889
<v Upol> There might be privacy concerns that prevent

1676
01:08:52.890 --> 01:08:54.629
<v Upol> the HU from being shot.

1677
01:08:54.630 --> 01:08:56.789
<v Upol> Right. Because you can also see, like, you know,

1678
01:08:56.790 --> 01:08:59.489
<v Upol> biases creep up, like if I show the profile

1679
01:08:59.490 --> 01:09:01.799
<v Upol> picture and you know, if you can guess the

1680
01:09:01.800 --> 01:09:05.309
<v Upol> person's race or gender from the profile picture,

1681
01:09:05.310 --> 01:09:08.459
<v Upol> it can create certain biased viewpoints

1682
01:09:08.460 --> 01:09:09.778
<v Upol> or even the location, right?

1683
01:09:09.779 --> 01:09:12.509
<v Upol> Because certain companies are multinational

1684
01:09:12.510 --> 01:09:14.909
<v Upol> and it could be that, you know, certain locations

1685
01:09:14.910 --> 01:09:17.938
<v Upol> are not often looked positively enough.

1686
01:09:17.939 --> 01:09:20.608
<v Upol> And that's my bias, the receiver's

1687
01:09:20.609 --> 01:09:21.898
<v Upol> perception.

1688
01:09:21.899 --> 01:09:22.899
<v Upol> Mm hmm.

1689
01:09:23.490 --> 01:09:26.728
<v Andrey> Yeah, exactly. And I think again, it's interesting

1690
01:09:26.729 --> 01:09:29.818
<v Andrey> here, just reading the paper,

1691
01:09:29.819 --> 01:09:32.519
<v Andrey> which I think is, is, you know, I would recommend

1692
01:09:32.520 --> 01:09:34.739
<v Andrey> it. I think it's quite approachable.

1693
01:09:34.740 --> 01:09:37.438
<v Andrey> Is again, you have these quotes

1694
01:09:37.439 --> 01:09:39.989
<v Andrey> from the study participants that make it

1695
01:09:39.990 --> 01:09:41.278
<v Andrey> very concrete.

1696
01:09:41.279 --> 01:09:43.919
<v Andrey> One of them is the outcome should be a

1697
01:09:43.920 --> 01:09:46.469
<v Andrey> tldr the why is there

1698
01:09:46.470 --> 01:09:49.049
<v Andrey> if I'm interested, then

1699
01:09:49.050 --> 01:09:50.249
<v Andrey> there's also the issue.

1700
01:09:50.250 --> 01:09:52.289
<v Andrey> Someone said if I knew for to reach out to, I

1701
01:09:52.290 --> 01:09:55.109
<v Andrey> could find out the rest of the story and so on.

1702
01:09:55.110 --> 01:09:57.659
<v Andrey> So again, it's it's really giving you a

1703
01:09:57.660 --> 01:10:00.299
<v Andrey> sense of how your

1704
01:10:00.300 --> 01:10:03.089
<v Andrey> study and interaction with people

1705
01:10:03.090 --> 01:10:05.789
<v Andrey> led you to your conclusions, which

1706
01:10:05.790 --> 01:10:07.470
<v Andrey> I really enjoyed in reading the paper.

1707
01:10:08.960 --> 01:10:11.419
<v Upol> No, you know, thank you so much for the kind words

1708
01:10:11.420 --> 01:10:13.400
<v Upol> we put a lot of love into this paper.

1709
01:10:14.440 --> 01:10:15.440
<v Andrey> Yeah.

1710
01:10:15.940 --> 01:10:18.789
<v Andrey> And, yeah, you know, that's what you need

1711
01:10:18.790 --> 01:10:21.339
<v Andrey> to make a paper really enjoyable, so your

1712
01:10:21.340 --> 01:10:22.340
<v Andrey> work pay off.

1713
01:10:23.420 --> 01:10:26.019
<v Andrey> Now I think we can touch on a lot of

1714
01:10:26.020 --> 01:10:28.569
<v Andrey> it and it went through, I think hopefully the

1715
01:10:28.570 --> 01:10:31.179
<v Andrey> most this stuff in terms of the study elements and

1716
01:10:31.180 --> 01:10:33.969
<v Andrey> the four W's and make clear a social

1717
01:10:33.970 --> 01:10:36.549
<v Andrey> transparency is now on to

1718
01:10:36.550 --> 01:10:39.219
<v Andrey> a couple final things.

1719
01:10:39.220 --> 01:10:40.810
<v Andrey> So we've

1720
01:10:41.890 --> 01:10:45.309
<v Andrey> said, you know, it's good to have

1721
01:10:45.310 --> 01:10:47.709
<v Andrey> this on top of what is already there.

1722
01:10:47.710 --> 01:10:49.569
<v Andrey> I'll go to make sure it's fancy.

1723
01:10:49.570 --> 01:10:52.269
<v Andrey> So you need to add the social transparency

1724
01:10:52.270 --> 01:10:54.819
<v Andrey> and one question there as well is it

1725
01:10:54.820 --> 01:10:57.699
<v Andrey> easy or is it a very challenge is in place

1726
01:10:57.700 --> 01:11:00.969
<v Andrey> that would make it harder to do that?

1727
01:11:00.970 --> 01:11:02.229
<v Upol> Yeah, that's a that's a good point.

1728
01:11:02.230 --> 01:11:04.779
<v Upol> I think, you know, as as with everything, there

1729
01:11:04.780 --> 01:11:07.359
<v Upol> has to be the infrastructure that

1730
01:11:07.360 --> 01:11:08.739
<v Upol> is supported, right?

1731
01:11:09.940 --> 01:11:12.549
<v Upol> And there are challenges like privacy.

1732
01:11:12.550 --> 01:11:15.309
<v Upol> There are challenges like biases

1733
01:11:15.310 --> 01:11:18.099
<v Upol> or information overload, as well as incentives

1734
01:11:18.100 --> 01:11:20.889
<v Upol> like if you want to engage in a socially

1735
01:11:20.890 --> 01:11:23.589
<v Upol> transparent system that has to be incentive

1736
01:11:23.590 --> 01:11:26.289
<v Upol> for people to engage with it

1737
01:11:26.290 --> 01:11:28.089
<v Upol> like, you know, give those four W's as they're

1738
01:11:28.090 --> 01:11:29.979
<v Upol> working, that is a burden that is added, right?

1739
01:11:29.980 --> 01:11:31.239
<v Upol> Like no fees, lunches.

1740
01:11:32.290 --> 01:11:34.899
<v Upol> And so that means we have to be

1741
01:11:34.900 --> 01:11:36.849
<v Upol> very mindful of that.

1742
01:11:36.850 --> 01:11:38.349
<v Upol> And you know, we can, you know, with the Ford

1743
01:11:38.350 --> 01:11:40.659
<v Upol> family, you could also kind of promote groupthink,

1744
01:11:40.660 --> 01:11:43.209
<v Upol> right? Imagine in a company culture where you're

1745
01:11:43.210 --> 01:11:45.249
<v Upol> not allowed to go against your boss and you see a

1746
01:11:45.250 --> 01:11:47.229
<v Upol> comment from your boss previously.

1747
01:11:47.230 --> 01:11:49.919
<v Upol> So so we have to be careful.

1748
01:11:49.920 --> 01:11:51.189
<v Upol> You know, it's not a golden bullet.

1749
01:11:52.240 --> 01:11:54.189
<v Upol> So we have to be very careful when we

1750
01:11:54.190 --> 01:11:56.949
<v Upol> operationalize the social transparency

1751
01:11:56.950 --> 01:11:59.949
<v Upol> that we are trying to be very mindful

1752
01:11:59.950 --> 01:12:01.969
<v Upol> of some of these challenges, like, you know, do we

1753
01:12:01.970 --> 01:12:04.539
<v Upol> really want to see all the four W's at every

1754
01:12:04.540 --> 01:12:07.059
<v Upol> single time? No, there are ways to summarize

1755
01:12:07.060 --> 01:12:09.609
<v Upol> it. And we have done that in my project with

1756
01:12:09.610 --> 01:12:11.829
<v Upol> the cyber security people, we have been able to

1757
01:12:11.830 --> 01:12:14.349
<v Upol> figure out how to summarize these aspects at

1758
01:12:14.350 --> 01:12:16.029
<v Upol> a level of detail that is actionable.

1759
01:12:17.810 --> 01:12:18.999
<v Andrey> Yeah.

1760
01:12:19.000 --> 01:12:21.759
<v Andrey> And so speaking of cyber security,

1761
01:12:21.760 --> 01:12:24.279
<v Andrey> people say, take it outside

1762
01:12:24.280 --> 01:12:27.039
<v Andrey> the study, I was interacting with

1763
01:12:27.040 --> 01:12:28.839
<v Andrey> these participants and,

1764
01:12:30.130 --> 01:12:33.039
<v Andrey> you know, figuring out the of context.

1765
01:12:33.040 --> 01:12:35.589
<v Andrey> You also took this for context

1766
01:12:35.590 --> 01:12:38.199
<v Andrey> to an actual organization and then

1767
01:12:38.200 --> 01:12:38.979
<v Andrey> tried it out.

1768
01:12:38.980 --> 01:12:40.089
<v Upol> Is that right? Yeah.

1769
01:12:40.090 --> 01:12:42.819
<v Upol> So like if you remember just from a timeline

1770
01:12:42.820 --> 01:12:44.119
<v Upol> perspective, right?

1771
01:12:44.120 --> 01:12:46.989
<v Upol> So by the time I think we wrote the paper

1772
01:12:46.990 --> 01:12:48.819
<v Upol> we already had.

1773
01:12:48.820 --> 01:12:50.019
<v Upol> This is obviously the study.

1774
01:12:50.020 --> 01:12:51.799
<v Upol> So there was an empirical study that was done.

1775
01:12:51.800 --> 01:12:54.549
<v Upol> Separate from this in parallel was

1776
01:12:54.550 --> 01:12:56.949
<v Upol> the cyber security project that I was running for

1777
01:12:56.950 --> 01:12:58.329
<v Upol> a long, long time.

1778
01:12:58.330 --> 01:13:00.939
<v Upol> And what I had the, I guess, the luxury

1779
01:13:00.940 --> 01:13:03.639
<v Upol> of knowing the future to some extent is

1780
01:13:03.640 --> 01:13:06.039
<v Upol> we were able to incorporate a lot of these four ws

1781
01:13:06.040 --> 01:13:08.649
<v Upol> into their system and they lived

1782
01:13:08.650 --> 01:13:11.199
<v Upol> in a socially transparent world when we wrote

1783
01:13:11.200 --> 01:13:13.749
<v Upol> this paper. So that's why we were able to talk

1784
01:13:13.750 --> 01:13:16.899
<v Upol> a lot about these transfer cases challenges

1785
01:13:16.900 --> 01:13:18.609
<v Upol> because those are some of the challenges we faced

1786
01:13:18.610 --> 01:13:21.219
<v Upol> in the real world when we were trying to implement

1787
01:13:21.220 --> 01:13:23.529
<v Upol> this in an enterprise setting that is

1788
01:13:23.530 --> 01:13:24.969
<v Upol> multinational.

1789
01:13:24.970 --> 01:13:27.609
<v Andrey> I see. So when you presented this

1790
01:13:27.610 --> 01:13:29.589
<v Andrey> and sort of said, we should do this,

1791
01:13:30.880 --> 01:13:32.829
<v Andrey> you know how receptive our people today sort of

1792
01:13:32.830 --> 01:13:34.539
<v Andrey> get it right away or

1793
01:13:34.540 --> 01:13:37.089
<v Upol> initially there was a little bit of like

1794
01:13:37.090 --> 01:13:39.759
<v Upol> hesitation, I think, because someone said, like,

1795
01:13:39.760 --> 01:13:41.109
<v Upol> how is this explainability,

1796
01:13:43.180 --> 01:13:45.669
<v Upol> right? Because there is this and there is a very

1797
01:13:45.670 --> 01:13:48.279
<v Upol> powerful like AI developer like this is not

1798
01:13:48.280 --> 01:13:50.079
<v Upol> explainability. And I think that's kind of like

1799
01:13:50.080 --> 01:13:53.289
<v Upol> the idea of the paper kind of came to light.

1800
01:13:53.290 --> 01:13:56.709
<v Upol> Our idea of explainability is so narrow

1801
01:13:56.710 --> 01:13:59.499
<v Upol> that we have a hard time

1802
01:13:59.500 --> 01:14:02.409
<v Upol> kind of even envisioning more than that.

1803
01:14:02.410 --> 01:14:05.019
<v Upol> So what we actually did to kind of address those

1804
01:14:05.020 --> 01:14:07.029
<v Upol> kind of concerns is, you know, as we see as you

1805
01:14:07.030 --> 01:14:09.939
<v Upol> saw also on the paper in this empirical study that

1806
01:14:09.940 --> 01:14:11.260
<v Upol> we had concrete

1807
01:14:12.910 --> 01:14:15.759
<v Upol> like directly from the stakeholder

1808
01:14:15.760 --> 01:14:18.699
<v Upol> information about how these additional context

1809
01:14:18.700 --> 01:14:21.249
<v Upol> help them understand the system,

1810
01:14:21.250 --> 01:14:23.589
<v Upol> right? And then if we go back to our initial

1811
01:14:23.590 --> 01:14:25.929
<v Upol> definition of explainability rights, things that

1812
01:14:25.930 --> 01:14:28.359
<v Upol> helped me understand the AI systems, right?

1813
01:14:28.360 --> 01:14:30.069
<v Upol> And in this case, the AI systems are not

1814
01:14:30.070 --> 01:14:32.919
<v Upol> algorithm. These are human AI assemblages.

1815
01:14:32.920 --> 01:14:35.499
<v Upol> Right? So and they're socio technically

1816
01:14:35.500 --> 01:14:37.899
<v Upol> situated. So there you go.

1817
01:14:37.900 --> 01:14:40.689
<v Upol> So initially, there was a lot of

1818
01:14:40.690 --> 01:14:43.239
<v Upol> pushback, but what the proof is often in the

1819
01:14:43.240 --> 01:14:46.359
<v Upol> pudding. So when we added social transparency,

1820
01:14:46.360 --> 01:14:48.979
<v Upol> the engagement went from like two percent

1821
01:14:48.980 --> 01:14:50.639
<v Upol> to ninety six percent.

1822
01:14:50.640 --> 01:14:53.549
<v Upol> Right? That you can't ignore.

1823
01:14:53.550 --> 01:14:56.049
<v Upol> Right. And so so those are some

1824
01:14:56.050 --> 01:14:57.399
<v Upol> of the things that helped a lot of the

1825
01:14:57.400 --> 01:15:00.339
<v Upol> stakeholders have more buy in

1826
01:15:00.340 --> 01:15:02.589
<v Upol> and get a sense of, OK, now this is important.

1827
01:15:02.590 --> 01:15:05.139
<v Upol> This might not look algorithmic, but

1828
01:15:05.140 --> 01:15:07.719
<v Upol> it has everything to do with the algorithm,

1829
01:15:07.720 --> 01:15:08.720
<v Upol> right?

1830
01:15:09.130 --> 01:15:11.499
<v Andrey> Yeah, I guess it harkens back to the title of a

1831
01:15:11.500 --> 01:15:14.889
<v Andrey> work, right? Expanding its pie ability,

1832
01:15:14.890 --> 01:15:16.679
<v Andrey> you know, as person said, how is this

1833
01:15:16.680 --> 01:15:18.969
<v Andrey> expandability while you've pointed out, then you

1834
01:15:18.970 --> 01:15:21.549
<v Andrey> sort of make the argument that this should be

1835
01:15:21.550 --> 01:15:24.849
<v Andrey> part of expandability, and

1836
01:15:24.850 --> 01:15:28.419
<v Andrey> by adding it, you get sort of a more holistic,

1837
01:15:28.420 --> 01:15:29.829
<v Andrey> full understanding.

1838
01:15:29.830 --> 01:15:32.199
<v Andrey> Is that kind of a fair characterization?

1839
01:15:32.200 --> 01:15:34.539
<v Upol> Yeah, yeah. And I think, you know, sometimes the

1840
01:15:34.540 --> 01:15:37.239
<v Upol> simplicity is kind of elusive

1841
01:15:37.240 --> 01:15:38.709
<v Upol> and deceptive.

1842
01:15:38.710 --> 01:15:41.439
<v Upol> But, you know, we also have to understand

1843
01:15:41.440 --> 01:15:43.659
<v Upol> that sometimes very powerful ideas and also very

1844
01:15:43.660 --> 01:15:44.739
<v Upol> simple ideals.

1845
01:15:44.740 --> 01:15:47.259
<v Upol> And I think within AI, we have to kind of

1846
01:15:47.260 --> 01:15:49.509
<v Upol> go back to those roots at some point, like not

1847
01:15:49.510 --> 01:15:51.579
<v Upol> everything that is complex is good.

1848
01:15:51.580 --> 01:15:54.639
<v Upol> Neither is not everything that is simple is bad.

1849
01:15:54.640 --> 01:15:57.219
<v Upol> You can have very good ideas that are very simple.

1850
01:15:57.220 --> 01:15:59.919
<v Andrey> Yeah, exactly. Simple ideas can be very powerful.

1851
01:15:59.920 --> 01:16:02.739
<v Andrey> And I guess one of the key insights here is

1852
01:16:02.740 --> 01:16:05.499
<v Andrey> social transparency as a concept and as something

1853
01:16:05.500 --> 01:16:08.289
<v Andrey> that needs to be part of expandability.

1854
01:16:08.290 --> 01:16:11.019
<v Andrey> So just to go back and situate within

1855
01:16:11.020 --> 01:16:13.749
<v Andrey> the XIII research

1856
01:16:13.750 --> 01:16:15.789
<v Andrey> field, you know,

1857
01:16:16.810 --> 01:16:19.419
<v Andrey> I don't know too much about the context of that

1858
01:16:19.420 --> 01:16:20.749
<v Andrey> field and what is going on there.

1859
01:16:20.750 --> 01:16:22.240
<v Andrey> So what do you think could be

1860
01:16:23.440 --> 01:16:26.169
<v Andrey> hopefully, I guess, the impact and

1861
01:16:26.170 --> 01:16:28.689
<v Andrey> what this could enable as far as future

1862
01:16:28.690 --> 01:16:30.039
<v Andrey> research?

1863
01:16:30.040 --> 01:16:33.069
<v Upol> First of all, I think it makes this very nebulous

1864
01:16:33.070 --> 01:16:36.429
<v Upol> topic of socio organizational context tractable,

1865
01:16:36.430 --> 01:16:39.039
<v Upol> right? Like for concrete things to

1866
01:16:39.040 --> 01:16:41.679
<v Upol> go for, and that's a good starting point.

1867
01:16:41.680 --> 01:16:44.469
<v Upol> It gives people to grasp on to that and build

1868
01:16:44.470 --> 01:16:46.269
<v Upol> on it. And I think that's what we actually invite

1869
01:16:46.270 --> 01:16:49.269
<v Upol> people to do right is

1870
01:16:49.270 --> 01:16:51.759
<v Upol> now that we have at least started the

1871
01:16:51.760 --> 01:16:54.789
<v Upol> conversation. That explainability

1872
01:16:54.790 --> 01:16:57.549
<v Upol> is beyond algorithmic transparency and

1873
01:16:57.550 --> 01:17:00.309
<v Upol> given the community one way

1874
01:17:00.310 --> 01:17:03.069
<v Upol> of capturing the socio organizational context,

1875
01:17:03.070 --> 01:17:05.560
<v Upol> I think now it starts to seed

1876
01:17:06.880 --> 01:17:09.219
<v Upol> more ideas. And I think there is some fascinating

1877
01:17:09.220 --> 01:17:12.549
<v Upol> paper that I've seen after that around

1878
01:17:12.550 --> 01:17:15.189
<v Upol> and ideas actually that

1879
01:17:15.190 --> 01:17:17.869
<v Upol> talk. Using this notion of social transparency

1880
01:17:17.870 --> 01:17:21.229
<v Upol> talked about end to end lifecycle perspectives

1881
01:17:21.230 --> 01:17:23.569
<v Upol> within explainability, like who needs to know

1882
01:17:23.570 --> 01:17:25.939
<v Upol> what, when and why, like sheep and ocher and

1883
01:17:25.940 --> 01:17:27.949
<v Upol> Christine Wolfe and others have kind of written

1884
01:17:27.950 --> 01:17:29.499
<v Upol> about it. So I didn't get it.

1885
01:17:29.500 --> 01:17:32.359
<v Upol> It gives us bedrock for future

1886
01:17:32.360 --> 01:17:34.909
<v Upol> work to kind of build on it, and I hope it does,

1887
01:17:34.910 --> 01:17:37.609
<v Upol> and I'll work within

1888
01:17:37.610 --> 01:17:39.979
<v Upol> explainability takes far beyond social

1889
01:17:39.980 --> 01:17:41.239
<v Upol> transparency.

1890
01:17:41.240 --> 01:17:43.249
<v Upol> There are other things that are outside the box

1891
01:17:43.250 --> 01:17:45.019
<v Upol> that also need to be included.

1892
01:17:45.020 --> 01:17:46.639
<v Upol> And how do we encode that?

1893
01:17:46.640 --> 01:17:48.829
<v Upol> I hope people use this kind of scenario, these

1894
01:17:48.830 --> 01:17:51.439
<v Upol> design techniques, and it is also not shy

1895
01:17:51.440 --> 01:17:53.449
<v Upol> away from the fact that if something as simple,

1896
01:17:53.450 --> 01:17:56.059
<v Upol> right, as long as powerful, that's

1897
01:17:56.060 --> 01:17:58.130
<v Upol> still a valid and good contribution.

1898
01:17:59.140 --> 01:18:01.599
<v Andrey> Yeah, I guess in a sense, that's how you want

1899
01:18:01.600 --> 01:18:03.669
<v Andrey> research to work, someone reads a paper and it's

1900
01:18:03.670 --> 01:18:06.549
<v Andrey> like, Wow, this is cool, but what if he did this

1901
01:18:06.550 --> 01:18:08.899
<v Andrey> or this thing doesn't work?

1902
01:18:08.900 --> 01:18:10.539
<v Andrey> You know, I have this idea.

1903
01:18:10.540 --> 01:18:12.639
<v Andrey> So that makes a lot of sense.

1904
01:18:12.640 --> 01:18:15.689
<v Andrey> And also to that notion of sort of the context

1905
01:18:15.690 --> 01:18:18.249
<v Andrey> and the field itself, we talked about on

1906
01:18:18.250 --> 01:18:21.249
<v Andrey> a bit of a push back, it got at the

1907
01:18:21.250 --> 01:18:24.489
<v Andrey> industry level within the research community.

1908
01:18:24.490 --> 01:18:26.319
<v Andrey> You know, when you submitted it, when you got

1909
01:18:26.320 --> 01:18:28.869
<v Andrey> reviews, when you presented it, what was

1910
01:18:28.870 --> 01:18:31.149
<v Andrey> the reception of your colleagues?

1911
01:18:32.230 --> 01:18:33.999
<v Upol> I think it was surprising to us.

1912
01:18:34.000 --> 01:18:35.889
<v Upol> We always thought when we wrote the paper that

1913
01:18:35.890 --> 01:18:39.099
<v Upol> people either hate it or they will love it.

1914
01:18:39.100 --> 01:18:41.709
<v Upol> I don't think anyone who's going to be neutral to

1915
01:18:41.710 --> 01:18:44.349
<v Upol> it because it was making a very provocative

1916
01:18:44.350 --> 01:18:46.989
<v Upol> argument. It was making the argument that

1917
01:18:46.990 --> 01:18:49.419
<v Upol> explainability is not transgressive.

1918
01:18:49.420 --> 01:18:51.069
<v Upol> It is more than that. And it's not just saying

1919
01:18:51.070 --> 01:18:53.769
<v Upol> that it's like this one way of doing it.

1920
01:18:53.770 --> 01:18:57.729
<v Upol> So and clearly it was well-received,

1921
01:18:57.730 --> 01:19:00.249
<v Upol> and the presentation at Chi

1922
01:19:00.250 --> 01:19:01.270
<v Upol> went very well.

1923
01:19:02.830 --> 01:19:05.289
<v Upol> And, you know, we were very lucky to receive this

1924
01:19:05.290 --> 01:19:07.089
<v Upol> paper, honorable mention on it as well.

1925
01:19:08.170 --> 01:19:10.719
<v Upol> So I think overall, it went better than

1926
01:19:10.720 --> 01:19:13.009
<v Upol> we expected it, to be honest.

1927
01:19:13.010 --> 01:19:16.329
<v Andrey> Yeah, it's good to hear that given

1928
01:19:16.330 --> 01:19:19.059
<v Andrey> again, this was it looks to be quite

1929
01:19:19.060 --> 01:19:22.269
<v Andrey> Stafford guy is pretty big, right?

1930
01:19:22.270 --> 01:19:24.969
<v Upol> Yeah, it is the premier HCI conference.

1931
01:19:24.970 --> 01:19:27.339
<v Upol> So like, not like nervous for now because nervous

1932
01:19:27.340 --> 01:19:29.439
<v Upol> runs at a different scale, but like in terms of

1933
01:19:29.440 --> 01:19:30.849
<v Upol> like the premiere venue.

1934
01:19:30.850 --> 01:19:33.169
<v Upol> Right? Hi, is that for HCI?

1935
01:19:33.170 --> 01:19:34.709
<v Upol> What nerve sign before?

1936
01:19:34.710 --> 01:19:36.459
<v Upol> No, I guess that's a different way of looking at

1937
01:19:36.460 --> 01:19:36.969
<v Upol> it.

1938
01:19:36.970 --> 01:19:39.309
<v Andrey> Well, so yeah, that's really cool.

1939
01:19:39.310 --> 01:19:41.829
<v Andrey> And we'll have a link to that paper again

1940
01:19:41.830 --> 01:19:44.499
<v Andrey> and a description and our Substack.

1941
01:19:44.500 --> 01:19:47.379
<v Andrey> So if you do want to get more into it,

1942
01:19:47.380 --> 01:19:49.569
<v Andrey> you can just click and read it.

1943
01:19:49.570 --> 01:19:52.989
<v Andrey> And that's just to touch on a bit

1944
01:19:52.990 --> 01:19:54.879
<v Andrey> what has happened since.

1945
01:19:54.880 --> 01:19:57.369
<v Andrey> In your research, you had actually a couple of

1946
01:19:57.370 --> 01:19:58.659
<v Andrey> weeks.

1947
01:19:58.660 --> 01:20:01.689
<v Andrey> So first up, you have the WHO

1948
01:20:01.690 --> 01:20:04.569
<v Andrey> and explainable AI how AI background

1949
01:20:04.570 --> 01:20:07.179
<v Andrey> shapes perceptions of AI explanations.

1950
01:20:07.180 --> 01:20:09.819
<v Andrey> How does that relate to your prior work

1951
01:20:09.820 --> 01:20:12.609
<v Andrey> and endless work? And sort of what what was

1952
01:20:12.610 --> 01:20:13.779
<v Andrey> what is it?

1953
01:20:13.780 --> 01:20:16.449
<v Upol> Absolutely. So I mean, this is directly related

1954
01:20:16.450 --> 01:20:17.799
<v Upol> to a human centered, explainable way.

1955
01:20:17.800 --> 01:20:20.619
<v Upol> I kind of work in the sense that

1956
01:20:20.620 --> 01:20:23.199
<v Upol> not all humans are the same when it

1957
01:20:23.200 --> 01:20:25.599
<v Upol> comes to interacting with the AI system.

1958
01:20:25.600 --> 01:20:28.269
<v Upol> I don't think anyone will challenge that

1959
01:20:28.270 --> 01:20:29.859
<v Upol> observation. Right.

1960
01:20:29.860 --> 01:20:32.559
<v Upol> But then the question becomes, OK, who are

1961
01:20:32.560 --> 01:20:33.969
<v Upol> these people?

1962
01:20:33.970 --> 01:20:36.819
<v Upol> How do their different views or characteristics

1963
01:20:36.820 --> 01:20:39.939
<v Upol> impact how they interpret explanations?

1964
01:20:39.940 --> 01:20:42.639
<v Upol> So in this paper, it's just something

1965
01:20:42.640 --> 01:20:45.099
<v Upol> that we looked at like a very critical dimension,

1966
01:20:45.100 --> 01:20:46.509
<v Upol> which is any AI background.

1967
01:20:46.510 --> 01:20:48.459
<v Upol> Like if you think about consumers of A.I.

1968
01:20:48.460 --> 01:20:50.349
<v Upol> technology versus creators of A.I.

1969
01:20:50.350 --> 01:20:52.899
<v Upol> technology, oftentimes consumers

1970
01:20:52.900 --> 01:20:55.029
<v Upol> don't have the level of AI background that the

1971
01:20:55.030 --> 01:20:57.219
<v Upol> creators have, right?

1972
01:20:57.220 --> 01:20:59.589
<v Upol> So given that this background is a consequential

1973
01:20:59.590 --> 01:21:01.869
<v Upol> dimension, but also the fact that it might be

1974
01:21:01.870 --> 01:21:05.469
<v Upol> absent in the users of systems that we build.

1975
01:21:05.470 --> 01:21:09.189
<v Upol> How does that background actually impact

1976
01:21:09.190 --> 01:21:10.749
<v Upol> the perceptions of these A.I.

1977
01:21:10.750 --> 01:21:13.239
<v Upol> explanations, right? Because again, we're making

1978
01:21:13.240 --> 01:21:16.869
<v Upol> the explanations also for the receiver explaining

1979
01:21:16.870 --> 01:21:19.209
<v Upol> that and then they explain that so that this is

1980
01:21:19.210 --> 01:21:21.579
<v Upol> the paper that is, I think, the first paper that

1981
01:21:21.580 --> 01:21:24.789
<v Upol> kind of explores the AI background

1982
01:21:24.790 --> 01:21:27.649
<v Upol> as there's a dimension to

1983
01:21:27.650 --> 01:21:30.039
<v Upol> to see like, well, how does that impact like we

1984
01:21:30.040 --> 01:21:32.469
<v Upol> see humans, humans, but who are these humans?

1985
01:21:32.470 --> 01:21:35.019
<v Upol> Well, let's look at two two groups of

1986
01:21:35.020 --> 01:21:38.109
<v Upol> humans like people with and people without.

1987
01:21:38.110 --> 01:21:41.199
<v Upol> So this paper kind of presents a study

1988
01:21:41.200 --> 01:21:43.809
<v Upol> based largely actually on the Frogger work

1989
01:21:43.810 --> 01:21:46.539
<v Upol> now way back when to kind of get

1990
01:21:46.540 --> 01:21:47.540
<v Upol> at these questions.

1991
01:21:48.640 --> 01:21:51.249
<v Andrey> Yeah, it makes me think also,

1992
01:21:51.250 --> 01:21:53.769
<v Andrey> aside from like, you know, I

1993
01:21:53.770 --> 01:21:56.379
<v Andrey> develop or not add it all up, or even just like

1994
01:21:56.380 --> 01:21:59.119
<v Andrey> programmer who were and resources a person

1995
01:21:59.120 --> 01:22:00.559
<v Andrey> in sales.

1996
01:22:00.560 --> 01:22:03.309
<v Andrey> You might interact with the AI system

1997
01:22:03.310 --> 01:22:05.919
<v Andrey> differently, so it seems no good

1998
01:22:05.920 --> 01:22:07.449
<v Andrey> to take into account, for sure.

1999
01:22:07.450 --> 01:22:08.450
<v Andrey> Yeah.

2000
01:22:09.190 --> 01:22:12.759
<v Andrey> And then I think also you had

2001
01:22:12.760 --> 01:22:15.519
<v Andrey> this elevator explainability pitfalls

2002
01:22:15.520 --> 01:22:18.519
<v Andrey> beyond dark patterns and explainable

2003
01:22:18.520 --> 01:22:21.579
<v Andrey> AI, which sounds a little bit exciting.

2004
01:22:22.590 --> 01:22:24.339
<v Andrey> So, yeah, what's that about?

2005
01:22:24.340 --> 01:22:27.009
<v Upol> So this paper is actually related to the WHO

2006
01:22:27.010 --> 01:22:29.589
<v Upol> in my paper, because one of the findings in

2007
01:22:29.590 --> 01:22:32.199
<v Upol> WHO and say that we got was

2008
01:22:32.200 --> 01:22:33.729
<v Upol> we're both groups.

2009
01:22:33.730 --> 01:22:37.269
<v Upol> The group with AI and NONYE backgrounds

2010
01:22:37.270 --> 01:22:40.839
<v Upol> had exhibited unwarranted faith

2011
01:22:40.840 --> 01:22:44.499
<v Upol> in numerical based explanations

2012
01:22:44.500 --> 01:22:46.779
<v Upol> that had no

2013
01:22:49.180 --> 01:22:50.709
<v Upol> meaning behind them, so to speak.

2014
01:22:50.710 --> 01:22:52.989
<v Upol> But even if people did not understand what the

2015
01:22:52.990 --> 01:22:55.779
<v Upol> numbers meant, there was a level of over

2016
01:22:55.780 --> 01:22:57.219
<v Upol> trust in them.

2017
01:22:57.220 --> 01:22:58.539
<v Upol> So based on.

2018
01:22:58.540 --> 01:23:00.729
<v Upol> That observation, what is interesting is like we

2019
01:23:00.730 --> 01:23:02.089
<v Upol> were not trying to trick anyone, right?

2020
01:23:02.090 --> 01:23:04.539
<v Upol> Like that's the importance of this finding that in

2021
01:23:04.540 --> 01:23:06.379
<v Upol> the study, we were not trying to trick anyone.

2022
01:23:06.380 --> 01:23:08.589
<v Upol> We just use the numerical explanations as a

2023
01:23:08.590 --> 01:23:11.109
<v Upol> baseline. Our main instrument was

2024
01:23:11.110 --> 01:23:13.750
<v Upol> the textual explanations, the actual rationale.

2025
01:23:15.130 --> 01:23:17.529
<v Upol> And while trying to examine that, we were like, Oh

2026
01:23:17.530 --> 01:23:20.169
<v Upol> my God, why are people like so in love

2027
01:23:20.170 --> 01:23:22.839
<v Upol> with these numbers, then that they don't

2028
01:23:22.840 --> 01:23:25.389
<v Upol> understand? Because we have qualitative data

2029
01:23:25.390 --> 01:23:28.059
<v Upol> where they tell us, I don't understand it now, but

2030
01:23:28.060 --> 01:23:30.039
<v Upol> I can understand it later.

2031
01:23:30.040 --> 01:23:32.619
<v Upol> And what is interesting is that people

2032
01:23:32.620 --> 01:23:35.619
<v Upol> with AI background and those without.

2033
01:23:35.620 --> 01:23:38.319
<v Upol> Have different results

2034
01:23:38.320 --> 01:23:40.989
<v Upol> for over trusting the A.I.,

2035
01:23:40.990 --> 01:23:42.879
<v Upol> right? So over trusting the numbers.

2036
01:23:42.880 --> 01:23:45.429
<v Upol> Excuse me. Yeah. So we started asking the

2037
01:23:45.430 --> 01:23:46.659
<v Upol> questions. All right.

2038
01:23:46.660 --> 01:23:49.689
<v Upol> There are many times where harmful

2039
01:23:49.690 --> 01:23:52.569
<v Upol> effects can happen, like over trust,

2040
01:23:52.570 --> 01:23:55.239
<v Upol> even when best of

2041
01:23:55.240 --> 01:23:57.969
<v Upol> intentions are there, like in our case.

2042
01:23:57.970 --> 01:24:00.549
<v Upol> Right. A lot of harmful work and

2043
01:24:00.550 --> 01:24:03.099
<v Upol> explainable AI is couched under this term called

2044
01:24:03.100 --> 01:24:05.259
<v Upol> dark patterns, which are basically deceptive

2045
01:24:05.260 --> 01:24:06.519
<v Upol> practices.

2046
01:24:06.520 --> 01:24:08.799
<v Upol> It's easiest to explain it from the other side,

2047
01:24:08.800 --> 01:24:11.049
<v Upol> like if you think about like, you know, in certain

2048
01:24:11.050 --> 01:24:13.299
<v Upol> websites, they have all these like like

2049
01:24:13.300 --> 01:24:15.189
<v Upol> transparent like ads.

2050
01:24:15.190 --> 01:24:17.469
<v Upol> And when you're trying to click the play button

2051
01:24:17.470 --> 01:24:19.239
<v Upol> like 10000 windows, open up, right?

2052
01:24:19.240 --> 01:24:21.129
<v Upol> And you have to take them 10000 politicians to get

2053
01:24:21.130 --> 01:24:23.829
<v Upol> it. So there's a dark side that kind of drives

2054
01:24:23.830 --> 01:24:26.859
<v Upol> clicks by tricking the user,

2055
01:24:26.860 --> 01:24:29.379
<v Upol> you know, not all harm patterns like harmful

2056
01:24:29.380 --> 01:24:31.119
<v Upol> patterns are created equal.

2057
01:24:31.120 --> 01:24:34.029
<v Upol> So what happens when

2058
01:24:34.030 --> 01:24:36.699
<v Upol> harmful effects emerge, when there is

2059
01:24:36.700 --> 01:24:38.769
<v Upol> no bad intention behind it?

2060
01:24:38.770 --> 01:24:41.319
<v Upol> Right, right? So to answer that question,

2061
01:24:41.320 --> 01:24:43.869
<v Upol> we wrote another kind of conceptual paper, and

2062
01:24:43.870 --> 01:24:47.319
<v Upol> we call these things explainable the pitfalls.

2063
01:24:47.320 --> 01:24:49.989
<v Upol> Right? So these pitfalls are certain

2064
01:24:49.990 --> 01:24:52.599
<v Upol> things that you might not intend for bad things

2065
01:24:52.600 --> 01:24:55.179
<v Upol> to happen, but like a pitfall in a real piece of

2066
01:24:55.180 --> 01:24:57.729
<v Upol> like in the real world, you might inadvertently

2067
01:24:57.730 --> 01:24:59.349
<v Upol> fall into it. Right?

2068
01:24:59.350 --> 01:25:00.669
<v Upol> Because, you know, it's not like pitfalls have

2069
01:25:00.670 --> 01:25:02.169
<v Upol> there to like trap people.

2070
01:25:02.170 --> 01:25:04.869
<v Upol> Sometimes the pitfalls emerge in nature, in

2071
01:25:04.870 --> 01:25:08.049
<v Upol> jungles and other places by the construction

2072
01:25:08.050 --> 01:25:10.839
<v Upol> site, and that you might inadvertently fall into

2073
01:25:10.840 --> 01:25:13.509
<v Upol> it. So this paper is kind of trying to articulate

2074
01:25:13.510 --> 01:25:15.819
<v Upol> what are explainability pitfalls?

2075
01:25:15.820 --> 01:25:17.559
<v Upol> How do you address them?

2076
01:25:17.560 --> 01:25:19.869
<v Upol> What are some of the strategies to mitigate them?

2077
01:25:19.870 --> 01:25:22.089
<v Upol> So this is more of another kind of a conceptual

2078
01:25:22.090 --> 01:25:24.939
<v Upol> paper situated with a case study,

2079
01:25:24.940 --> 01:25:27.649
<v Upol> and it recently got into the human centered

2080
01:25:27.650 --> 01:25:29.169
<v Upol> A.I. workshop at in Europe.

2081
01:25:29.170 --> 01:25:31.029
<v Upol> So this year, so we are looking forward to sharing

2082
01:25:31.030 --> 01:25:32.509
<v Upol> it with the community as well.

2083
01:25:32.510 --> 01:25:35.319
<v Andrey> Oh, it's exciting. Yeah, that's roughly

2084
01:25:35.320 --> 01:25:37.929
<v Andrey> in a moment, right? Yeah, yeah.

2085
01:25:37.930 --> 01:25:39.529
<v Andrey> Yeah, that's that's interesting.

2086
01:25:39.530 --> 01:25:42.279
<v Andrey> This concept of sheer is something you

2087
01:25:42.280 --> 01:25:45.999
<v Andrey> should avoid doing seems like a good idea,

2088
01:25:46.000 --> 01:25:48.759
<v Andrey> almost publishing negative results, which

2089
01:25:48.760 --> 01:25:50.229
<v Andrey> is which is fun.

2090
01:25:51.820 --> 01:25:54.379
<v Andrey> Well, we went for a lot of your work

2091
01:25:54.380 --> 01:25:56.290
<v Andrey> and then almost traced

2092
01:25:57.310 --> 01:25:59.049
<v Andrey> from the beginning to the present.

2093
01:25:59.050 --> 01:26:01.269
<v Andrey> But of course, it's also important again, to

2094
01:26:01.270 --> 01:26:04.239
<v Andrey> mention, as you have done before, that this was,

2095
01:26:04.240 --> 01:26:06.249
<v Andrey> you know, a lot of this was done with many

2096
01:26:06.250 --> 01:26:09.579
<v Andrey> collaborators and you built on a lot of

2097
01:26:09.580 --> 01:26:12.099
<v Andrey> prior research, obviously in many fields.

2098
01:26:12.100 --> 01:26:14.859
<v Andrey> This is true of any research job

2099
01:26:14.860 --> 01:26:16.030
<v Andrey> because you were present,

2100
01:26:18.100 --> 01:26:21.159
<v Andrey> maybe beyond your papers.

2101
01:26:21.160 --> 01:26:23.799
<v Andrey> What kind of is the

2102
01:26:23.800 --> 01:26:26.409
<v Andrey> situation when it comes to community

2103
01:26:26.410 --> 01:26:28.929
<v Andrey> working on XIII,

2104
01:26:28.930 --> 01:26:31.539
<v Andrey> Nick's family AI and also human centered

2105
01:26:31.540 --> 01:26:34.239
<v Andrey> XIII, you know, is is

2106
01:26:34.240 --> 01:26:36.889
<v Andrey> is your being human centered or socio

2107
01:26:36.890 --> 01:26:39.549
<v Andrey> technical? Is that becoming more

2108
01:26:39.550 --> 01:26:42.189
<v Andrey> popular or are more people so aware

2109
01:26:42.190 --> 01:26:43.839
<v Andrey> of it, that sort of thing?

2110
01:26:43.840 --> 01:26:46.149
<v Upol> No, you're absolutely right. I think, you know, I

2111
01:26:46.150 --> 01:26:48.009
<v Upol> stand in the shoulder of giants, right?

2112
01:26:48.010 --> 01:26:51.249
<v Upol> There's no two ways about it without the fantastic

2113
01:26:51.250 --> 01:26:52.839
<v Upol> people I work with.

2114
01:26:52.840 --> 01:26:55.180
<v Upol> None of this work becomes reality

2115
01:26:56.330 --> 01:26:58.989
<v Upol> and the communities, and it's something

2116
01:26:58.990 --> 01:27:00.189
<v Upol> that I care deeply about.

2117
01:27:00.190 --> 01:27:03.189
<v Upol> So we have been very lucky in this context.

2118
01:27:03.190 --> 01:27:05.979
<v Upol> And by 2020 one, we

2119
01:27:05.980 --> 01:27:08.499
<v Upol> were able to host the first human centered

2120
01:27:08.500 --> 01:27:10.299
<v Upol> explainable AI workshop.

2121
01:27:10.300 --> 01:27:12.579
<v Upol> It was actually one of the largest attended

2122
01:27:12.580 --> 01:27:15.249
<v Upol> workshops and trials during

2123
01:27:15.250 --> 01:27:17.889
<v Upol> more than 100 people came over

2124
01:27:17.890 --> 01:27:19.209
<v Upol> 14 countries.

2125
01:27:20.470 --> 01:27:23.919
<v Upol> So we had a stellar group of

2126
01:27:23.920 --> 01:27:25.389
<v Upol> papers.

2127
01:27:25.390 --> 01:27:28.029
<v Upol> We had a keynote from Tim Miller,

2128
01:27:28.030 --> 01:27:30.279
<v Upol> an expert panel discussions.

2129
01:27:30.280 --> 01:27:32.769
<v Upol> So I think that community is still going on.

2130
01:27:32.770 --> 01:27:35.349
<v Upol> And actually, we did just propose to host the

2131
01:27:35.350 --> 01:27:38.499
<v Upol> second workshop at Chi.

2132
01:27:38.500 --> 01:27:41.409
<v Upol> And I think after this, we want to take it beyond.

2133
01:27:41.410 --> 01:27:43.449
<v Upol> We want to take it down. Europe's who want to take

2134
01:27:43.450 --> 01:27:46.179
<v Upol> it to triple AI, to try to see

2135
01:27:46.180 --> 01:27:48.829
<v Upol> how more can we intersect with

2136
01:27:48.830 --> 01:27:51.559
<v Upol> more other communities around

2137
01:27:51.560 --> 01:27:54.729
<v Upol> HCI, like other relevant social groups, right?

2138
01:27:54.730 --> 01:27:57.559
<v Upol> The computer vision people and this people.

2139
01:27:57.560 --> 01:27:59.869
<v Upol> So. These are some things that we deeply care

2140
01:27:59.870 --> 01:28:02.959
<v Upol> about, and that is something that I would

2141
01:28:02.960 --> 01:28:06.139
<v Upol> that I'm kind of like

2142
01:28:06.140 --> 01:28:07.140
<v Upol> looking forward.

2143
01:28:07.960 --> 01:28:09.939
<v Andrey> Yeah, definitely so.

2144
01:28:09.940 --> 01:28:13.269
<v Andrey> And just to get it a bit more into that, you know,

2145
01:28:13.270 --> 01:28:15.519
<v Andrey> what's next for you both in terms of this

2146
01:28:15.520 --> 01:28:18.129
<v Andrey> community aspect of, you know, having

2147
01:28:18.130 --> 01:28:19.659
<v Andrey> various events to

2148
01:28:21.130 --> 01:28:23.229
<v Andrey> let people know about to see you and also in terms

2149
01:28:23.230 --> 01:28:25.809
<v Andrey> of, I guess, where your research is headed.

2150
01:28:25.810 --> 01:28:28.509
<v Upol> Yeah, I think for me, I as I share, if there's

2151
01:28:28.510 --> 01:28:30.519
<v Upol> a project that I'm doing with radiation oncology,

2152
01:28:30.520 --> 01:28:32.829
<v Upol> it's actually exploring social transparency in

2153
01:28:32.830 --> 01:28:34.899
<v Upol> their world and this has been actually a value

2154
01:28:34.900 --> 01:28:36.789
<v Upol> long term engagement.

2155
01:28:36.790 --> 01:28:38.679
<v Upol> I've been working with them for more than two

2156
01:28:38.680 --> 01:28:39.939
<v Upol> years now.

2157
01:28:39.940 --> 01:28:42.039
<v Upol> I've also kind of been working with the Data and

2158
01:28:42.040 --> 01:28:44.619
<v Upol> Society Institute on

2159
01:28:44.620 --> 01:28:47.139
<v Upol> Algorithmic Justice Issues around the Global

2160
01:28:47.140 --> 01:28:50.019
<v Upol> South. So you know what happens when

2161
01:28:50.020 --> 01:28:52.599
<v Upol> we all talk a lot about algorithmic

2162
01:28:52.600 --> 01:28:53.890
<v Upol> deployment right

2163
01:28:55.180 --> 01:28:57.639
<v Upol> before deployment? Dataset creation?

2164
01:28:57.640 --> 01:29:00.919
<v Upol> But what happens when algorithms get taken out

2165
01:29:00.920 --> 01:29:03.099
<v Upol> and what happens, then what happens when they're

2166
01:29:03.100 --> 01:29:04.569
<v Upol> no longer used?

2167
01:29:04.570 --> 01:29:07.059
<v Upol> So there is a project that I'm running that has

2168
01:29:07.060 --> 01:29:09.789
<v Upol> explainability component, as well as algorithmic

2169
01:29:09.790 --> 01:29:12.369
<v Upol> justice component around being

2170
01:29:12.370 --> 01:29:15.969
<v Upol> creating the algorithmic trading of the

2171
01:29:15.970 --> 01:29:18.669
<v Upol> DC exams, which are like

2172
01:29:18.670 --> 01:29:21.039
<v Upol> basically international exams administered by

2173
01:29:21.040 --> 01:29:23.619
<v Upol> Ofqual and UK governing

2174
01:29:23.620 --> 01:29:26.529
<v Upol> boards. But these exams are actually administered

2175
01:29:26.530 --> 01:29:28.569
<v Upol> in over one hundred and sixty countries.

2176
01:29:28.570 --> 01:29:31.269
<v Upol> So you might recall that in August

2177
01:29:31.270 --> 01:29:33.819
<v Upol> of twenty twenty, there were protests around an

2178
01:29:33.820 --> 01:29:36.389
<v Upol> algorithm grading a lot of students

2179
01:29:36.390 --> 01:29:38.589
<v Upol> know. While the reporting was great.

2180
01:29:38.590 --> 01:29:40.299
<v Upol> It only focused on the U.K.

2181
01:29:40.300 --> 01:29:42.789
<v Upol> we really don't know what happened in the other

2182
01:29:42.790 --> 01:29:44.769
<v Upol> one hundred and sixty countries where these exams

2183
01:29:44.770 --> 01:29:46.059
<v Upol> were administered.

2184
01:29:46.060 --> 01:29:48.639
<v Upol> So, you know, beyond, you know, as I say, denies

2185
01:29:48.640 --> 01:29:50.739
<v Upol> you kindly shared my bio, right?

2186
01:29:50.740 --> 01:29:52.509
<v Upol> What happens to the people who are not on the

2187
01:29:52.510 --> 01:29:53.749
<v Upol> table?

2188
01:29:53.750 --> 01:29:55.279
<v Upol> And I think if you don't

2189
01:29:56.300 --> 01:29:58.849
<v Upol> amplify people's voices, we're not at the table,

2190
01:29:58.850 --> 01:30:01.149
<v Upol> they often end up on the menu.

2191
01:30:01.150 --> 01:30:03.729
<v Upol> So I think coming for the circle like that,

2192
01:30:03.730 --> 01:30:06.039
<v Upol> something that I'm deeply curious about, so that's

2193
01:30:06.040 --> 01:30:08.709
<v Upol> roughly like, you know what things are

2194
01:30:08.710 --> 01:30:11.199
<v Upol> and if I have the privilege of giving a keynote at

2195
01:30:11.200 --> 01:30:13.569
<v Upol> the World Usability Day actually tomorrow on

2196
01:30:13.570 --> 01:30:16.299
<v Upol> November 11th, I have some invited talks lined

2197
01:30:16.300 --> 01:30:19.479
<v Upol> up at the University of Buffalo on the 30th

2198
01:30:19.480 --> 01:30:22.029
<v Upol> and then an expert panel discussion actually

2199
01:30:22.030 --> 01:30:24.639
<v Upol> at the university's medical school,

2200
01:30:24.640 --> 01:30:27.909
<v Upol> the Stanford Medical School, to the conference.

2201
01:30:27.910 --> 01:30:30.639
<v Upol> So that's that's pretty much like like a ramp up

2202
01:30:30.640 --> 01:30:31.899
<v Upol> to the end of the year.

2203
01:30:31.900 --> 01:30:33.249
<v Andrey> Cool, yeah.

2204
01:30:33.250 --> 01:30:35.919
<v Andrey> Sadly, will release as five guests

2205
01:30:35.920 --> 01:30:37.029
<v Andrey> pass through 11.

2206
01:30:38.230 --> 01:30:41.109
<v Andrey> But will these talks be recorded

2207
01:30:41.110 --> 01:30:42.769
<v Andrey> or public? Could be.

2208
01:30:42.770 --> 01:30:44.139
<v Upol> That's a very good point.

2209
01:30:44.140 --> 01:30:46.419
<v Upol> Thank you so much for asking. So I am going to

2210
01:30:46.420 --> 01:30:48.789
<v Upol> check it. I wonder what I would recommend if the

2211
01:30:48.790 --> 01:30:50.439
<v Upol> listeners are there.

2212
01:30:50.440 --> 01:30:53.079
<v Upol> If you check out my Twitter, if they are

2213
01:30:53.080 --> 01:30:55.629
<v Upol> public, I will be sure to make sure that

2214
01:30:55.630 --> 01:30:58.599
<v Upol> they are published and shared widely.

2215
01:30:58.600 --> 01:31:01.149
<v Upol> So as of now, I'm not sure which of these

2216
01:31:01.150 --> 01:31:02.650
<v Upol> would be public versus not.

2217
01:31:03.670 --> 01:31:06.189
<v Upol> But if they are, I will publish them on my

2218
01:31:06.190 --> 01:31:08.859
<v Upol> Twitter. So if people are interested

2219
01:31:08.860 --> 01:31:10.689
<v Upol> and I think we can also add links to them

2220
01:31:11.800 --> 01:31:12.870
<v Upol> after the podcast.

2221
01:31:13.930 --> 01:31:15.399
<v Andrey> Exactly. Yeah. So you can look down that

2222
01:31:15.400 --> 01:31:17.999
<v Andrey> description. We'll figure it out and

2223
01:31:18.000 --> 01:31:20.829
<v Andrey> we'll have links to

2224
01:31:20.830 --> 01:31:22.929
<v Andrey> this and all papers and everything.

2225
01:31:24.100 --> 01:31:26.379
<v Andrey> All right. So that's cool.

2226
01:31:26.380 --> 01:31:29.139
<v Andrey> And then as I like to wrap up,

2227
01:31:29.140 --> 01:31:31.809
<v Andrey> after all this intense discussion of research

2228
01:31:31.810 --> 01:31:34.749
<v Andrey> and ideas and studies, just,

2229
01:31:34.750 --> 01:31:36.999
<v Andrey> you know, a little bit about you and not your

2230
01:31:37.000 --> 01:31:38.679
<v Andrey> research.

2231
01:31:38.680 --> 01:31:40.839
<v Andrey> What do you do these days?

2232
01:31:40.840 --> 01:31:43.359
<v Andrey> Or, you know, in general,

2233
01:31:44.440 --> 01:31:46.929
<v Andrey> beyond research, what are your main hobbies?

2234
01:31:46.930 --> 01:31:48.699
<v Andrey> What are your main interests?

2235
01:31:48.700 --> 01:31:51.339
<v Upol> Yeah, I guess I'm, you know, I've been I

2236
01:31:51.340 --> 01:31:52.569
<v Upol> love to cook.

2237
01:31:52.570 --> 01:31:54.189
<v Upol> I think that is something that has been

2238
01:31:55.270 --> 01:31:57.969
<v Upol> during the stay at home and pandemic

2239
01:31:57.970 --> 01:32:00.069
<v Upol> mode has been a blessing.

2240
01:32:01.180 --> 01:32:04.209
<v Upol> I absolutely love European

2241
01:32:04.210 --> 01:32:05.959
<v Upol> football or soccer.

2242
01:32:05.960 --> 01:32:07.959
<v Upol> All my team is not doing very well.

2243
01:32:07.960 --> 01:32:10.749
<v Upol> Manchester United right now, but I

2244
01:32:10.750 --> 01:32:13.779
<v Upol> tend to. That is my escape and I also

2245
01:32:13.780 --> 01:32:17.319
<v Upol> play this game called Football Manager.

2246
01:32:17.320 --> 01:32:20.229
<v Upol> I have not like fantasy football,

2247
01:32:20.230 --> 01:32:22.209
<v Upol> but it's like kind of like that where it's a very

2248
01:32:22.210 --> 01:32:23.259
<v Upol> data driven engine.

2249
01:32:24.850 --> 01:32:27.579
<v Upol> And that's how it comes up to

2250
01:32:28.960 --> 01:32:31.509
<v Upol> like this game engine that kind of predicts

2251
01:32:31.510 --> 01:32:33.759
<v Upol> the future. I'm going to simulate games.

2252
01:32:33.760 --> 01:32:36.669
<v Upol> That is my escape in terms of

2253
01:32:36.670 --> 01:32:38.979
<v Upol> all the things in reality.

2254
01:32:38.980 --> 01:32:41.739
<v Upol> But I absolutely a big

2255
01:32:41.740 --> 01:32:43.289
<v Upol> fan of old school hip hop.

2256
01:32:43.290 --> 01:32:45.279
<v Upol> So I listen to a lot of music.

2257
01:32:45.280 --> 01:32:48.639
<v Upol> I, whenever I get some time, I do

2258
01:32:48.640 --> 01:32:49.779
<v Upol> mix beats

2259
01:32:51.430 --> 01:32:54.459
<v Upol> on my own time for my own

2260
01:32:54.460 --> 01:32:56.499
<v Upol> enjoyment. I don't think I have a song called

2261
01:32:56.500 --> 01:32:59.499
<v Upol> Account or anything now, but those are my

2262
01:32:59.500 --> 01:33:01.779
<v Upol> ways of keeping sane.

2263
01:33:01.780 --> 01:33:04.239
<v Upol> But most importantly, one of the most cherished

2264
01:33:04.240 --> 01:33:06.999
<v Upol> things that I do is

2265
01:33:07.000 --> 01:33:09.819
<v Upol> mentoring young researchers,

2266
01:33:09.820 --> 01:33:13.089
<v Upol> especially who are underrepresented, especially

2267
01:33:13.090 --> 01:33:15.399
<v Upol> who are from the global south.

2268
01:33:15.400 --> 01:33:18.009
<v Upol> So I'm very proud of all

2269
01:33:18.010 --> 01:33:21.339
<v Upol> the mentees that have taught me so much

2270
01:33:21.340 --> 01:33:23.649
<v Upol> throughout the years, like ever since 2000.

2271
01:33:23.650 --> 01:33:26.259
<v Upol> I think 11 12, I've

2272
01:33:26.260 --> 01:33:28.660
<v Upol> had the privilege of mentoring around 100 hundred

2273
01:33:28.661 --> 01:33:30.909
<v Upol> people from many different countries in Asia and

2274
01:33:30.910 --> 01:33:33.489
<v Upol> Africa and kind of guiding them

2275
01:33:33.490 --> 01:33:35.349
<v Upol> through high school and those.

2276
01:33:35.350 --> 01:33:38.319
<v Upol> That is something that like gives me a lot of joy

2277
01:33:38.320 --> 01:33:40.149
<v Upol> actually, like whenever I get free time.

2278
01:33:40.150 --> 01:33:42.399
<v Upol> That's actually what I do. And during application

2279
01:33:42.400 --> 01:33:44.439
<v Upol> season, it's usually gets tough because we have a

2280
01:33:44.440 --> 01:33:48.009
<v Upol> lot of requests to review applications

2281
01:33:48.010 --> 01:33:49.629
<v Upol> because, you know, sometimes as you can imagine,

2282
01:33:49.630 --> 01:33:51.279
<v Upol> life like the application.

2283
01:33:51.280 --> 01:33:54.009
<v Upol> The statement of purpose is often a black box,

2284
01:33:54.010 --> 01:33:55.839
<v Upol> right? And you don't know what to write.

2285
01:33:55.840 --> 01:33:58.389
<v Upol> So that is one thing that I get a lot of

2286
01:33:58.390 --> 01:33:59.859
<v Upol> joy from.

2287
01:33:59.860 --> 01:34:01.549
<v Andrey> Yeah, that's that's fantastic.

2288
01:34:01.550 --> 01:34:04.149
<v Andrey> I think we all guys share

2289
01:34:04.150 --> 01:34:06.669
<v Andrey> what a good deal of mentorship as Diaz's

2290
01:34:06.670 --> 01:34:09.339
<v Andrey> adviser for a reason, you know, as an

2291
01:34:09.340 --> 01:34:10.899
<v Andrey> assigned mentor.

2292
01:34:10.900 --> 01:34:13.419
<v Andrey> So it's it does feel nice to give back,

2293
01:34:13.420 --> 01:34:15.909
<v Andrey> and I have always enjoyed being a teaching

2294
01:34:15.910 --> 01:34:18.459
<v Andrey> assistant and these various

2295
01:34:18.460 --> 01:34:20.949
<v Andrey> things are always pretty rewarding for me.

2296
01:34:22.330 --> 01:34:24.999
<v Andrey> Well, that was a really fun interview.

2297
01:34:25.000 --> 01:34:27.609
<v Andrey> It was great to see

2298
01:34:27.610 --> 01:34:30.669
<v Andrey> or hear about this human

2299
01:34:30.670 --> 01:34:33.609
<v Andrey> centered A.I. as a researcher who talks

2300
01:34:33.610 --> 01:34:36.219
<v Andrey> of robots refreshing to think about

2301
01:34:36.220 --> 01:34:37.689
<v Andrey> people for once.

2302
01:34:37.690 --> 01:34:40.329
<v Andrey> Thank you so much for being

2303
01:34:40.330 --> 01:34:41.769
<v Andrey> on the podcast.

2304
01:34:41.770 --> 01:34:42.909
<v Upol> My pleasure. Thank you, Andre.

2305
01:34:42.910 --> 01:34:45.669
<v Upol> I so appreciate the opportunity to talk to you

2306
01:34:45.670 --> 01:34:48.159
<v Upol> and an animal in a way to you.

2307
01:34:48.160 --> 01:34:49.779
<v Upol> Talk to the listeners.

2308
01:34:49.780 --> 01:34:50.829
<v Upol> Thank you.

2309
01:34:50.830 --> 01:34:52.149
<v Andrey> Absolutely.

2310
01:34:52.150 --> 01:34:55.269
<v Andrey> And once again, this is The Gradient podcast.

2311
01:34:55.270 --> 01:34:58.149
<v Andrey> Check out our magazine website

2312
01:34:58.150 --> 01:35:00.449
<v Andrey> at The Gradient dot com.

2313
01:35:00.450 --> 01:35:02.939
<v Andrey> To you, Earl. And our newsletter and actually this

2314
01:35:02.940 --> 01:35:05.489
<v Andrey> podcast at The Gradient pub that Substack

2315
01:35:05.490 --> 01:35:08.009
<v Andrey> dot com, you can support us there

2316
01:35:08.010 --> 01:35:10.709
<v Andrey> by subscribing and also share all

2317
01:35:10.710 --> 01:35:13.439
<v Andrey> of this review on this Apple and

2318
01:35:13.440 --> 01:35:14.879
<v Andrey> all these kinds of things.

2319
01:35:14.880 --> 01:35:17.249
<v Andrey> So if you dig this stuff, we would appreciate your

2320
01:35:17.250 --> 01:35:18.479
<v Andrey> support.

2321
01:35:18.480 --> 01:35:20.789
<v Andrey> Thank you so much for listening and be sure to

2322
01:35:20.790 --> 01:35:22.890
<v Andrey> tune into our future episodes.

