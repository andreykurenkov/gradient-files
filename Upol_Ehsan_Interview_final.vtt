WEBVTT

1
00:00:05.400 --> 00:00:08.159
<v ANDREY>Hello and welcome to the 18th episode

2
00:00:08.160 --> 00:00:10.469
<v ANDREY>of The Gradient podcast,.

3
00:00:10.470 --> 00:00:13.259
<v ANDREY>The Gradient is a digital magazine that aims to

4
00:00:13.260 --> 00:00:16.109
<v ANDREY>be a place for discussion about research and trends

5
00:00:16.110 --> 00:00:19.139
<v ANDREY>in artificial intelligence and machine learning,

6
00:00:19.140 --> 00:00:21.659
<v ANDREY>we interview various people in A.I such as

7
00:00:21.660 --> 00:00:25.079
<v ANDREY>engineers, researchers, artists and more.

8
00:00:25.080 --> 00:00:27.539
<v ANDREY>I'm your host Andrey Kurenkov.

9
00:00:27.540 --> 00:00:29.519
<v ANDREY>In this episode, I'm excited to be interviewing

10
00:00:31.530 --> 00:00:34.529
<v ANDREY>Upol Ehsan. Upol Cares about people first technology

11
00:00:34.530 --> 00:00:35.669
<v ANDREY>second.

12
00:00:35.670 --> 00:00:38.159
<v ANDREY>He's a doctoral candidate in the School of

13
00:00:38.160 --> 00:00:40.799
<v ANDREY>Interactive Computing at Georgia Tech and an

14
00:00:40.800 --> 00:00:43.469
<v ANDREY>affiliate at the Data and Society Research

15
00:00:43.470 --> 00:00:44.669
<v ANDREY>Institute.

16
00:00:44.670 --> 00:00:46.889
<v ANDREY>Combining his expertize in A.I.

17
00:00:46.890 --> 00:00:49.649
<v ANDREY>and background and philosophy as work and

18
00:00:49.650 --> 00:00:52.649
<v ANDREY>explainable, A.I., or SETI,

19
00:00:52.650 --> 00:00:55.439
<v ANDREY>aims to foster a future where anyone, regardless

20
00:00:55.440 --> 00:00:58.319
<v ANDREY>of our background, can use air power technology

21
00:00:58.320 --> 00:00:59.849
<v ANDREY>with dignity.

22
00:00:59.850 --> 00:01:02.519
<v ANDREY>Putting the human first and focusing on how our

23
00:01:02.520 --> 00:01:06.029
<v ANDREY>values shape the use and abuse of technology.

24
00:01:06.030 --> 00:01:09.179
<v ANDREY>His work has coined the term human centered,

25
00:01:09.180 --> 00:01:12.179
<v ANDREY>explainable A.I., which is subfield

26
00:01:12.180 --> 00:01:13.829
<v ANDREY>of expandable A.I.

27
00:01:13.830 --> 00:01:16.079
<v ANDREY>and a charted its visions.

28
00:01:16.080 --> 00:01:18.749
<v ANDREY>Actively publishing and top peer reviewed venues

29
00:01:18.750 --> 00:01:21.479
<v ANDREY>like his work, has received multiple awards

30
00:01:21.480 --> 00:01:24.809
<v ANDREY>and been covered in major media outlets

31
00:01:24.810 --> 00:01:26.729
<v ANDREY>bringing industry academia.

32
00:01:26.730 --> 00:01:29.339
<v ANDREY>He serves on multiple program committees

33
00:01:29.340 --> 00:01:32.519
<v ANDREY>and A.I. and AI conferences such as

34
00:01:32.520 --> 00:01:35.369
<v ANDREY>Europe's NDIS, and equally connects

35
00:01:35.370 --> 00:01:38.039
<v ANDREY>these communities by

36
00:01:38.040 --> 00:01:39.809
<v ANDREY>promoting equity and ethics and AI.

37
00:01:39.810 --> 00:01:42.479
<v ANDREY>He wants to ensure stakeholders who aren't at

38
00:01:42.480 --> 00:01:45.269
<v ANDREY>the table do not end up on

39
00:01:45.270 --> 00:01:46.270
<v ANDREY>the menu.

40
00:01:46.860 --> 00:01:49.769
<v ANDREY>Outside research He is an advisor for

41
00:01:49.770 --> 00:01:52.019
<v ANDREY>Ilar Asia and Educational Institute.

42
00:01:52.020 --> 00:01:55.139
<v ANDREY>He started for underprivileged children subjected

43
00:01:55.140 --> 00:01:57.569
<v ANDREY>to child labor and Twitter.

44
00:01:57.570 --> 00:01:59.589
<v ANDREY>You can follow him at at.

45
00:01:59.590 --> 00:02:02.579
<v ANDREY>Upul Assan New pop

46
00:02:02.580 --> 00:02:06.389
<v ANDREY>up O L E H s n,

47
00:02:06.390 --> 00:02:07.860
<v ANDREY>so I'm very excited for this.

48
00:02:08.940 --> 00:02:10.679
<v ANDREY>Paul has written to The Guardian before, and I think

49
00:02:10.680 --> 00:02:12.509
<v ANDREY>his work is super cool.

50
00:02:12.510 --> 00:02:15.539
<v ANDREY>Welcome to the podcast, Supo.

51
00:02:15.540 --> 00:02:17.129
<v UPOL>Thank you for having me on.

52
00:02:17.130 --> 00:02:18.210
<v UPOL>It's pleasure to be here.

53
00:02:19.290 --> 00:02:21.959
<v ANDREY>Definitely. So as we usually do

54
00:02:21.960 --> 00:02:25.229
<v ANDREY>when these episodes before diving into your work

55
00:02:25.230 --> 00:02:28.109
<v ANDREY>a bit on your sort of background, I'm curious,

56
00:02:28.110 --> 00:02:30.809
<v ANDREY>how did you get into working

57
00:02:30.810 --> 00:02:33.569
<v ANDREY>on? I think your trajectory might

58
00:02:33.570 --> 00:02:35.699
<v ANDREY>be interesting or your background or philosophy as

59
00:02:35.700 --> 00:02:36.989
<v ANDREY>well.

60
00:02:36.990 --> 00:02:40.289
<v UPOL>Yes, I think I have Isaac Asimov

61
00:02:40.290 --> 00:02:43.679
<v UPOL>to kind of attribute that credit to.

62
00:02:43.680 --> 00:02:46.439
<v UPOL>When I was very young, I got hooked into

63
00:02:46.440 --> 00:02:48.989
<v UPOL>his books. I have read forty seven of his books, not

64
00:02:48.990 --> 00:02:51.749
<v UPOL>just the science fiction that he a lot.

65
00:02:51.750 --> 00:02:52.439
<v ANDREY>Yeah, yeah,

66
00:02:52.440 --> 00:02:55.199
<v UPOL>I mean, the maestro is is someone who's near

67
00:02:55.200 --> 00:02:57.389
<v UPOL>and dear to my heart, which makes watching

68
00:02:57.390 --> 00:03:00.149
<v UPOL>foundation and Apple TV right now a very scary

69
00:03:00.150 --> 00:03:02.759
<v UPOL>prospect. Oh yeah, because I remember those

70
00:03:02.760 --> 00:03:06.359
<v UPOL>things, but I think Asimov

71
00:03:06.360 --> 00:03:09.479
<v UPOL>pushed me to think about artificial intelligence

72
00:03:09.480 --> 00:03:12.269
<v UPOL>in ways that I don't think I would have thought

73
00:03:12.270 --> 00:03:14.879
<v UPOL>of, because all of his books, if

74
00:03:14.880 --> 00:03:18.269
<v UPOL>you think about it, it's about how does

75
00:03:18.270 --> 00:03:21.179
<v UPOL>how can we find flaws

76
00:03:21.180 --> 00:03:23.879
<v UPOL>in the three laws of robotics that kind

77
00:03:23.880 --> 00:03:25.229
<v UPOL>of he proposed, right?

78
00:03:26.400 --> 00:03:29.339
<v UPOL>And in college, I was

79
00:03:29.340 --> 00:03:31.739
<v UPOL>very so. I grew up in a philosophy department that

80
00:03:31.740 --> 00:03:34.469
<v UPOL>had a lot of cognitive scientists in them,

81
00:03:34.470 --> 00:03:36.659
<v UPOL>but who were teaching analytic philosophy.

82
00:03:37.680 --> 00:03:39.329
<v UPOL>And that's where I actually got into.

83
00:03:39.330 --> 00:03:41.189
<v UPOL>I got hooked into it.

84
00:03:41.190 --> 00:03:43.949
<v UPOL>I was like, OK, and maybe initially

85
00:03:43.950 --> 00:03:47.579
<v UPOL>I had more ambitious goals of creating

86
00:03:47.580 --> 00:03:49.289
<v UPOL>something like AGI, so to speak.

87
00:03:49.290 --> 00:03:51.959
<v UPOL>But then over time, I started getting more practical

88
00:03:51.960 --> 00:03:55.199
<v UPOL>about it. And after graduating,

89
00:03:55.200 --> 00:03:57.299
<v UPOL>I actually spent a lot of time doing management,

90
00:03:57.300 --> 00:03:59.729
<v UPOL>consulting and then then a startup.

91
00:03:59.730 --> 00:04:02.729
<v UPOL>And in those experiences, I was dealing

92
00:04:02.730 --> 00:04:06.479
<v UPOL>with innovative applications, but mostly

93
00:04:06.480 --> 00:04:08.159
<v UPOL>on the consumer side.

94
00:04:08.160 --> 00:04:10.709
<v UPOL>So I had clients who are really using this at the

95
00:04:10.710 --> 00:04:14.099
<v UPOL>enterprise level, and I was seeing how

96
00:04:14.100 --> 00:04:16.828
<v UPOL>sometimes despite best intentions,

97
00:04:16.829 --> 00:04:19.828
<v UPOL>the real use of these systems

98
00:04:19.829 --> 00:04:21.059
<v UPOL>were suffering.

99
00:04:21.060 --> 00:04:23.849
<v UPOL>So that's one way when I got into the

100
00:04:23.850 --> 00:04:26.879
<v UPOL>Ph.D. journey, I started thinking of

101
00:04:26.880 --> 00:04:29.699
<v UPOL>artificial intelligence, but from the human side.

102
00:04:31.200 --> 00:04:33.899
<v ANDREY>Right, and this was roughly when

103
00:04:33.900 --> 00:04:34.829
<v ANDREY>what year?

104
00:04:34.830 --> 00:04:37.499
<v UPOL>Yeah, so I had like

105
00:04:37.500 --> 00:04:40.139
<v UPOL>so it was like I started the peace Deep Learning

106
00:04:40.140 --> 00:04:44.279
<v UPOL>roughly around 20 or 15 16.

107
00:04:44.280 --> 00:04:46.919
<v UPOL>But the work that I had done before that was

108
00:04:46.920 --> 00:04:49.409
<v UPOL>like the last four years before that, that's around

109
00:04:49.410 --> 00:04:51.010
<v UPOL>like 2012 13.

110
00:04:52.230 --> 00:04:54.519
<v UPOL>So that's like the industry experience very much

111
00:04:54.520 --> 00:04:58.139
<v UPOL>drives a lot of my insights into the work today,

112
00:04:58.140 --> 00:05:01.229
<v UPOL>especially seeing people and I do consult

113
00:05:01.230 --> 00:05:02.729
<v UPOL>even now.

114
00:05:02.730 --> 00:05:05.579
<v UPOL>So I'm very much into the applied setting of these

115
00:05:05.580 --> 00:05:08.339
<v UPOL>research discussions, which help me kind of bridge

116
00:05:08.340 --> 00:05:11.069
<v UPOL>too. That's why you'll see, even in my work,

117
00:05:11.070 --> 00:05:13.439
<v UPOL>I do tend to have a more applied kind of a

118
00:05:13.440 --> 00:05:14.440
<v UPOL>connotation.

119
00:05:15.150 --> 00:05:18.239
<v ANDREY>Yeah, yeah. I was just wondering because I think,

120
00:05:18.240 --> 00:05:20.429
<v ANDREY>you know, obviously there's been a huge boom.

121
00:05:20.430 --> 00:05:23.789
<v ANDREY>I have a past decade and explainable AI,

122
00:05:23.790 --> 00:05:26.999
<v ANDREY>which you know your work with and has been

123
00:05:27.000 --> 00:05:29.639
<v ANDREY>more and more an area study.

124
00:05:29.640 --> 00:05:32.509
<v ANDREY>But I think it took it a little while

125
00:05:32.510 --> 00:05:35.159
<v ANDREY>it of catching up in some sense as as

126
00:05:35.160 --> 00:05:37.019
<v ANDREY>I was getting deployed.

127
00:05:37.020 --> 00:05:39.809
<v ANDREY>Yeah. And then so you started your journey in

128
00:05:39.810 --> 00:05:42.809
<v ANDREY>2015. Did you go to expand aid right away

129
00:05:42.810 --> 00:05:45.599
<v ANDREY>or did it sort of did you find your way

130
00:05:45.600 --> 00:05:47.459
<v ANDREY>there a bit later?

131
00:05:47.460 --> 00:05:48.719
<v UPOL>That's a really great question.

132
00:05:48.720 --> 00:05:51.599
<v UPOL>No, I actually started my journey doing

133
00:05:51.600 --> 00:05:54.719
<v UPOL>affective computing, so I was very much interested

134
00:05:54.720 --> 00:05:59.069
<v UPOL>in helping children with autism, learn

135
00:05:59.070 --> 00:06:01.829
<v UPOL>about non-verbal communication to

136
00:06:01.830 --> 00:06:04.499
<v UPOL>head up displays, and Google Glass was very hot back

137
00:06:04.500 --> 00:06:05.939
<v UPOL>then. Oh yeah.

138
00:06:05.940 --> 00:06:08.279
<v UPOL>So I was trying to develop algorithms trying to help

139
00:06:09.330 --> 00:06:12.539
<v UPOL>people who had had difficulties

140
00:06:12.540 --> 00:06:15.209
<v UPOL>processing social signals to use some kind

141
00:06:15.210 --> 00:06:17.939
<v UPOL>of a prosthetic to kind of in that

142
00:06:17.940 --> 00:06:20.219
<v UPOL>social interaction. So that's how I actually

143
00:06:20.220 --> 00:06:20.759
<v UPOL>started.

144
00:06:20.760 --> 00:06:23.609
<v UPOL>And then after that, I am originally

145
00:06:23.610 --> 00:06:26.369
<v UPOL>from Bangladesh. So I, the global south has

146
00:06:26.370 --> 00:06:29.159
<v UPOL>been very much and is still very

147
00:06:29.160 --> 00:06:32.009
<v UPOL>much a core part of my existence.

148
00:06:32.010 --> 00:06:34.109
<v UPOL>So after that, I started looking at how do these

149
00:06:34.110 --> 00:06:36.959
<v UPOL>technologies kind of behave in

150
00:06:36.960 --> 00:06:39.779
<v UPOL>the global south, where the technology is not

151
00:06:39.780 --> 00:06:41.100
<v UPOL>necessarily made in?

152
00:06:42.900 --> 00:06:45.629
<v UPOL>After that, I think it was

153
00:06:45.630 --> 00:06:48.509
<v UPOL>in two thousand sixteen or seventeen where DARPA

154
00:06:48.510 --> 00:06:49.669
<v UPOL>had that essay.

155
00:06:49.670 --> 00:06:52.859
<v UPOL>I grant and

156
00:06:52.860 --> 00:06:55.979
<v UPOL>that was the first time where because

157
00:06:55.980 --> 00:06:58.349
<v UPOL>it's interesting, right? Like explainability of A.I.

158
00:06:58.350 --> 00:07:01.109
<v UPOL>is not real. If you look at the literature

159
00:07:01.110 --> 00:07:03.809
<v UPOL>in the 80s, there is a lot of work, in fact, that

160
00:07:03.810 --> 00:07:06.299
<v UPOL>comics label and I was coined back in the 80s of the

161
00:07:06.300 --> 00:07:07.300
<v UPOL>90s.

162
00:07:08.130 --> 00:07:10.319
<v UPOL>This was based on the knowledge, you know, the

163
00:07:10.320 --> 00:07:13.769
<v UPOL>knowledge based systems like we had a second there.

164
00:07:13.770 --> 00:07:16.329
<v UPOL>But with the advent of Deep Learning and Deep

165
00:07:16.330 --> 00:07:19.589
<v UPOL>Learning becoming kind of enterprise

166
00:07:19.590 --> 00:07:23.009
<v UPOL>level almost like coming of age,

167
00:07:23.010 --> 00:07:26.009
<v UPOL>you see, then there is this need

168
00:07:26.010 --> 00:07:28.919
<v UPOL>to hold these systems accountable.

169
00:07:28.920 --> 00:07:31.889
<v UPOL>So I actually had

170
00:07:31.890 --> 00:07:34.619
<v UPOL>walked into my advisor's office at that time

171
00:07:34.620 --> 00:07:37.799
<v UPOL>and I was asking, you know, what kind of projects

172
00:07:37.800 --> 00:07:39.479
<v UPOL>do we have to work on?

173
00:07:39.480 --> 00:07:42.089
<v UPOL>And he said, And my advisor

174
00:07:42.090 --> 00:07:43.979
<v UPOL>is fantastic microneedle.

175
00:07:43.980 --> 00:07:46.649
<v UPOL>And Mark kind of said that, hey, there is

176
00:07:46.650 --> 00:07:49.829
<v UPOL>this other project that no one really

177
00:07:49.830 --> 00:07:52.259
<v UPOL>has taken upon themselves because we don't really

178
00:07:52.260 --> 00:07:54.899
<v UPOL>know what it would look like.

179
00:07:54.900 --> 00:07:56.949
<v UPOL>And I said, What is it? Is this explainable?

180
00:07:56.950 --> 00:07:59.639
<v UPOL>I think until at that time, like I had not

181
00:07:59.640 --> 00:08:02.369
<v UPOL>heard about the term, I was like, This sounds like

182
00:08:02.370 --> 00:08:05.519
<v UPOL>interesting. And I think upon reflection,

183
00:08:05.520 --> 00:08:08.669
<v UPOL>what I realized about myself is I do very well

184
00:08:08.670 --> 00:08:11.549
<v UPOL>when it's so empty slate and I get to paint

185
00:08:11.550 --> 00:08:14.649
<v UPOL>my own picture rather than very well form.

186
00:08:14.650 --> 00:08:17.339
<v UPOL>So I was very lucky to get

187
00:08:17.340 --> 00:08:19.949
<v UPOL>into that debate very early

188
00:08:19.950 --> 00:08:22.679
<v UPOL>on. In the second resurgence, I would argue,

189
00:08:22.680 --> 00:08:25.379
<v UPOL>because the second life I has had is, I think,

190
00:08:25.380 --> 00:08:28.024
<v UPOL>much more longer t.

191
00:08:28.025 --> 00:08:30.509
<v UPOL>Han the first life it had because it was there and

192
00:08:30.510 --> 00:08:33.179
<v UPOL>but it also wasn't there in the early 1980s.

193
00:08:34.830 --> 00:08:37.499
<v UPOL>So then I started looking

194
00:08:37.500 --> 00:08:38.999
<v UPOL>into it.

195
00:08:39.000 --> 00:08:41.619
<v UPOL>I started on the algorithmic side, frankly, and

196
00:08:41.620 --> 00:08:43.558
<v UPOL>then trying to work with algorithms.

197
00:08:43.559 --> 00:08:46.709
<v UPOL>And then over time, I got on my Human side

198
00:08:46.710 --> 00:08:50.399
<v UPOL>and you are right. I think explainable

199
00:08:50.400 --> 00:08:52.979
<v UPOL>AI is very much in flux.

200
00:08:52.980 --> 00:08:55.799
<v UPOL>That's how I would talk about it.

201
00:08:55.800 --> 00:08:58.529
<v UPOL>I think we as a community, we are still trying

202
00:08:58.530 --> 00:09:02.039
<v UPOL>to figure out how to navigate

203
00:09:02.040 --> 00:09:05.099
<v UPOL>this field, being consistent

204
00:09:05.100 --> 00:09:07.769
<v UPOL>in our terminology in the way

205
00:09:07.770 --> 00:09:09.329
<v UPOL>we do our work.

206
00:09:09.330 --> 00:09:12.359
<v UPOL>But there is also a certain level of beauty

207
00:09:12.360 --> 00:09:15.419
<v UPOL>in that. And in that case, I'm

208
00:09:15.420 --> 00:09:18.089
<v UPOL>kind of drawn by the social construction of

209
00:09:18.090 --> 00:09:21.209
<v UPOL>technology lenses, something pioneered

210
00:09:21.210 --> 00:09:23.969
<v UPOL>by way, a biker, and he talked

211
00:09:23.970 --> 00:09:25.889
<v UPOL>about relevant social groups.

212
00:09:25.890 --> 00:09:27.719
<v UPOL>So in any piece of technology, you will have

213
00:09:27.720 --> 00:09:30.299
<v UPOL>relevant social groups in that.

214
00:09:30.300 --> 00:09:33.149
<v UPOL>Is why they was talking about bicycles, so

215
00:09:33.150 --> 00:09:35.909
<v UPOL>bicycles have very other social groups,

216
00:09:35.910 --> 00:09:38.759
<v UPOL>and each relevant social groups

217
00:09:38.760 --> 00:09:41.519
<v UPOL>are these are stakeholders who have skin in the game

218
00:09:41.520 --> 00:09:44.099
<v UPOL>actually give meaning to the technology as much as

219
00:09:44.100 --> 00:09:46.379
<v UPOL>the technology gives meaning to the rights of.

220
00:09:46.380 --> 00:09:49.169
<v UPOL>If you think about the mountain bikes and BMX

221
00:09:49.170 --> 00:09:52.259
<v UPOL>bikes now, you know, like racing bikes

222
00:09:52.260 --> 00:09:53.909
<v UPOL>on different bikes.

223
00:09:53.910 --> 00:09:55.859
<v UPOL>And it's because of the stakeholders.

224
00:09:55.860 --> 00:09:57.629
<v UPOL>They get very different, meaning all of them are

225
00:09:57.630 --> 00:09:59.759
<v UPOL>bicycles, but they look very different.

226
00:09:59.760 --> 00:10:02.639
<v UPOL>And I think within explainability we have people

227
00:10:02.640 --> 00:10:05.589
<v UPOL>from the algorithmic side, basically

228
00:10:05.590 --> 00:10:08.489
<v UPOL>the items from the HCI

229
00:10:08.490 --> 00:10:11.129
<v UPOL>side. And now we are having stakeholders

230
00:10:11.130 --> 00:10:13.919
<v UPOL>in the public policy side, in the regulation

231
00:10:13.920 --> 00:10:15.839
<v UPOL>side, in the auditing side.

232
00:10:15.840 --> 00:10:18.059
<v UPOL>So I think each of these stakeholders are also

233
00:10:18.060 --> 00:10:21.209
<v UPOL>adding their own lenses to what is explainable

234
00:10:21.210 --> 00:10:23.819
<v UPOL>and which is why you will see a lot of

235
00:10:23.820 --> 00:10:24.820
<v UPOL>flux.

236
00:10:25.440 --> 00:10:28.319
<v ANDREY>Yeah, it's super interesting seeing this field

237
00:10:28.320 --> 00:10:30.959
<v ANDREY>kind of grow, and there's so much area to

238
00:10:30.960 --> 00:10:32.999
<v ANDREY>cover that.

239
00:10:33.000 --> 00:10:35.789
<v ANDREY>I think, you know, maybe compared

240
00:10:35.790 --> 00:10:38.579
<v ANDREY>to selling computer here. And you know, I think

241
00:10:38.580 --> 00:10:41.249
<v ANDREY>there's a lot more kind of maybe

242
00:10:41.250 --> 00:10:44.880
<v ANDREY>foundational or at least a conceptually

243
00:10:46.650 --> 00:10:49.109
<v ANDREY>important work. And then we'll get into what I think

244
00:10:49.110 --> 00:10:52.169
<v ANDREY>are yours and they could be could be called that.

245
00:10:52.170 --> 00:10:53.789
<v ANDREY>Yeah, your journey is really interesting.

246
00:10:53.790 --> 00:10:56.549
<v ANDREY>It's always fun to hear about how people bring in

247
00:10:56.550 --> 00:10:59.369
<v ANDREY>their experience before repeatedly and how

248
00:10:59.370 --> 00:11:02.069
<v ANDREY>that sort of guides their their

249
00:11:02.070 --> 00:11:03.329
<v ANDREY>direction.

250
00:11:03.330 --> 00:11:05.549
<v ANDREY>In my case, I started in robotics, in high school

251
00:11:05.550 --> 00:11:07.169
<v ANDREY>and then, you know, I did it in college.

252
00:11:07.170 --> 00:11:09.779
<v ANDREY>And then, you know, even when I went in some of the

253
00:11:09.780 --> 00:11:12.299
<v ANDREY>interactions and I came back to it.

254
00:11:12.300 --> 00:11:14.969
<v ANDREY>So it's it's always interesting to

255
00:11:14.970 --> 00:11:16.199
<v ANDREY>see how it happens.

256
00:11:16.200 --> 00:11:18.509
<v UPOL>I love that story because it's weird, right?

257
00:11:18.510 --> 00:11:21.239
<v UPOL>Because I have an undergrad, I have a

258
00:11:21.240 --> 00:11:24.029
<v UPOL>like a B.S. in electrical engineering and a B.A.

259
00:11:24.030 --> 00:11:25.289
<v UPOL>in philosophy, right?

260
00:11:25.290 --> 00:11:27.539
<v UPOL>And I never thought I would use that philosophy

261
00:11:27.540 --> 00:11:30.419
<v UPOL>degree on a daily basis

262
00:11:30.420 --> 00:11:33.049
<v UPOL>as much as I use it today.

263
00:11:33.050 --> 00:11:35.749
<v UPOL>In fact, my edge in

264
00:11:35.750 --> 00:11:38.299
<v UPOL>explainable air actually comes from my philosophy

265
00:11:38.300 --> 00:11:41.059
<v UPOL>training because I can't

266
00:11:41.060 --> 00:11:43.729
<v UPOL>access the writing that is coming

267
00:11:43.730 --> 00:11:45.499
<v UPOL>from the air because even as academics are part of

268
00:11:45.500 --> 00:11:48.169
<v UPOL>our training is how to read a certain body

269
00:11:48.170 --> 00:11:49.399
<v UPOL>of work.

270
00:11:49.400 --> 00:11:51.319
<v UPOL>But then when you're also trained in computer

271
00:11:51.320 --> 00:11:53.259
<v UPOL>science, you can bridge it.

272
00:11:53.260 --> 00:11:55.999
<v UPOL>Mm-Hmm. And I think there is something to be said

273
00:11:56.000 --> 00:11:58.249
<v UPOL>there, especially for PhD student or other

274
00:11:58.250 --> 00:11:59.539
<v UPOL>practitioners and researchers.

275
00:11:59.540 --> 00:12:02.389
<v UPOL>Listening is I have been my mentors

276
00:12:02.390 --> 00:12:03.979
<v UPOL>have always said like, you know, if you really want

277
00:12:03.980 --> 00:12:06.679
<v UPOL>to make a name, pick an area and pick an

278
00:12:06.680 --> 00:12:09.799
<v UPOL>Area B and then intersect them

279
00:12:09.800 --> 00:12:12.769
<v UPOL>and you might actually get a C that is

280
00:12:12.770 --> 00:12:15.469
<v UPOL>has a has an interesting angle to it that makes

281
00:12:15.470 --> 00:12:18.199
<v UPOL>your work more relevant, more impactful.

282
00:12:18.200 --> 00:12:20.719
<v UPOL>So I love also your story about robotics and how

283
00:12:20.720 --> 00:12:22.009
<v UPOL>your back full circle.

284
00:12:22.010 --> 00:12:24.949
<v UPOL>I think many of us in some ways are at the other

285
00:12:24.950 --> 00:12:28.459
<v UPOL>end up where our interest kind of started.

286
00:12:28.460 --> 00:12:30.919
<v ANDREY>Yeah, for sure. It's it's quite interesting.

287
00:12:30.920 --> 00:12:33.619
<v ANDREY>You know, I worked in robotics a lot in undergrad

288
00:12:33.620 --> 00:12:36.339
<v ANDREY>and I worked a lot and then were kind of classical

289
00:12:36.340 --> 00:12:38.779
<v ANDREY>robotic algorithms not knowing you all nets.

290
00:12:38.780 --> 00:12:41.449
<v ANDREY>And then that definitely informed by

291
00:12:41.450 --> 00:12:43.579
<v ANDREY>understanding and my ability to get into it.

292
00:12:43.580 --> 00:12:46.370
<v ANDREY>So always, always call to see how that happens.

293
00:12:47.750 --> 00:12:50.719
<v ANDREY>So that's kind of my

294
00:12:50.720 --> 00:12:53.149
<v ANDREY>introduction to how you got here out of a way.

295
00:12:53.150 --> 00:12:55.879
<v ANDREY>Let's start diving into your work

296
00:12:55.880 --> 00:12:59.089
<v ANDREY>will be focusing a lot on a particular

297
00:12:59.090 --> 00:13:02.179
<v ANDREY>paper that I think is very cool.

298
00:13:02.180 --> 00:13:05.149
<v ANDREY>But before that, let's just give the listeners

299
00:13:05.150 --> 00:13:07.789
<v ANDREY>a bit of a conceptual kind of

300
00:13:07.790 --> 00:13:10.549
<v ANDREY>introduction to a field, I suppose, and manage our

301
00:13:10.550 --> 00:13:12.289
<v ANDREY>work. So

302
00:13:13.550 --> 00:13:16.609
<v ANDREY>just common basics, you know,

303
00:13:16.610 --> 00:13:19.679
<v ANDREY>quick introduction to explain what explainability

304
00:13:19.680 --> 00:13:22.369
<v ANDREY>is, maybe, you know, and that's a pretty

305
00:13:22.370 --> 00:13:25.399
<v ANDREY>flat surface level and why it's important.

306
00:13:25.400 --> 00:13:27.739
<v UPOL>Yeah. So let's start with why it's important and

307
00:13:27.740 --> 00:13:29.719
<v UPOL>then I'll share why what it is and I think the

308
00:13:29.720 --> 00:13:31.669
<v UPOL>importance of drives what it is.

309
00:13:31.670 --> 00:13:34.519
<v UPOL>So with with with today, like the AI

310
00:13:34.520 --> 00:13:37.609
<v UPOL>powered decision making is everywhere

311
00:13:37.610 --> 00:13:40.459
<v UPOL>from radiation radiologists

312
00:13:40.460 --> 00:13:43.369
<v UPOL>using AI powered decision support

313
00:13:43.370 --> 00:13:46.309
<v UPOL>systems to diagnose chest COVID pneumonia

314
00:13:46.310 --> 00:13:49.099
<v UPOL>on chest x rays right to

315
00:13:49.100 --> 00:13:51.979
<v UPOL>loan officers using algorithms to determine

316
00:13:51.980 --> 00:13:53.510
<v UPOL>if you are loan worthy or not.

317
00:13:54.590 --> 00:13:57.289
<v UPOL>Do you know the recidivism cases,

318
00:13:57.290 --> 00:14:00.139
<v UPOL>right? So as

319
00:14:00.140 --> 00:14:03.229
<v UPOL>we go on, more and more consequential

320
00:14:03.230 --> 00:14:06.319
<v UPOL>decisions that we are making are either

321
00:14:06.320 --> 00:14:09.289
<v UPOL>powered through AI or automated

322
00:14:09.290 --> 00:14:12.349
<v UPOL>by. So this actually creates

323
00:14:12.350 --> 00:14:15.289
<v UPOL>a need for AI to be held

324
00:14:15.290 --> 00:14:16.549
<v UPOL>accountable.

325
00:14:16.550 --> 00:14:19.759
<v UPOL>Like if something is doing something consequential,

326
00:14:19.760 --> 00:14:22.459
<v UPOL>I need to be able to ask

327
00:14:22.460 --> 00:14:23.629
<v UPOL>why?

328
00:14:23.630 --> 00:14:26.689
<v UPOL>Hmm. And the answer to that question

329
00:14:26.690 --> 00:14:28.820
<v UPOL>is where explainable AI comes in.

330
00:14:29.830 --> 00:14:32.229
<v UPOL>Broadly speaking, and many people have many

331
00:14:32.230 --> 00:14:35.569
<v UPOL>different definitions of it, at least the way

332
00:14:35.570 --> 00:14:38.289
<v UPOL>our lab and I have conceptualized it in the years of

333
00:14:38.290 --> 00:14:40.479
<v UPOL>work we have done is explainable.

334
00:14:40.480 --> 00:14:43.579
<v UPOL>AI refers to the techniques,

335
00:14:43.580 --> 00:14:47.019
<v UPOL>the strategies, the philosophies

336
00:14:47.020 --> 00:14:49.749
<v UPOL>that can help us

337
00:14:49.750 --> 00:14:52.599
<v UPOL>as stakeholders within the AI system

338
00:14:52.600 --> 00:14:54.819
<v UPOL>so it could be end users, developers, data

339
00:14:54.820 --> 00:14:58.269
<v UPOL>scientists understand

340
00:14:58.270 --> 00:15:01.659
<v UPOL>why the the system

341
00:15:01.660 --> 00:15:02.799
<v UPOL>did what it did.

342
00:15:04.090 --> 00:15:06.849
<v UPOL>And again, this is why it's also human

343
00:15:06.850 --> 00:15:09.129
<v UPOL>centered in the sense that it's not just the

344
00:15:09.130 --> 00:15:11.079
<v UPOL>algorithm, right? There's a human at the end of it

345
00:15:11.080 --> 00:15:13.779
<v UPOL>trying to understand it so it

346
00:15:13.780 --> 00:15:15.819
<v UPOL>can take many forms.

347
00:15:15.820 --> 00:15:18.789
<v UPOL>Sometimes these explanations can be in the form

348
00:15:18.790 --> 00:15:21.639
<v UPOL>of natural language, plain English, for instance,

349
00:15:21.640 --> 00:15:24.069
<v UPOL>explanations like textual.

350
00:15:24.070 --> 00:15:26.889
<v UPOL>Sometimes these explanations can be in the form

351
00:15:26.890 --> 00:15:28.090
<v UPOL>of visualizations.

352
00:15:29.170 --> 00:15:31.779
<v UPOL>Sometimes these explanations can be in the form

353
00:15:31.780 --> 00:15:34.449
<v UPOL>of data structures, so they have the guts

354
00:15:34.450 --> 00:15:37.329
<v UPOL>of a neural net where you are trying to figure out

355
00:15:37.330 --> 00:15:39.639
<v UPOL>which layer is what's important.

356
00:15:39.640 --> 00:15:42.669
<v UPOL>So these explanations and explainable?

357
00:15:42.670 --> 00:15:45.099
<v UPOL>I think the takeaway is very pluralistic.

358
00:15:45.100 --> 00:15:46.419
<v UPOL>It's not monolithic.

359
00:15:46.420 --> 00:15:48.699
<v UPOL>It's not. There's not one little thing that fits

360
00:15:48.700 --> 00:15:51.429
<v UPOL>all. But at the core of it, it's

361
00:15:51.430 --> 00:15:54.249
<v UPOL>about understanding the decision making

362
00:15:54.250 --> 00:15:57.009
<v UPOL>in a way that makes sense to the user in a way

363
00:15:57.010 --> 00:15:59.799
<v UPOL>that makes sense for the person interpreting it.

364
00:15:59.800 --> 00:16:01.699
<v ANDREY>Does that help? Yeah, no.

365
00:16:01.700 --> 00:16:03.459
<v ANDREY>That explains it. I think quite well.

366
00:16:03.460 --> 00:16:06.099
<v ANDREY>And I guess it's

367
00:16:06.100 --> 00:16:09.069
<v ANDREY>worth noting that this is especially difficult

368
00:16:09.070 --> 00:16:11.869
<v ANDREY>these days because we are working a lot

369
00:16:11.870 --> 00:16:13.419
<v ANDREY>to a Deep Learning.

370
00:16:13.420 --> 00:16:16.079
<v ANDREY>The way that works is you have a huge model

371
00:16:16.080 --> 00:16:17.289
<v ANDREY>of what awaits you.

372
00:16:17.290 --> 00:16:18.639
<v ANDREY>You trained on that a set.

373
00:16:18.640 --> 00:16:20.589
<v ANDREY>And then what you get is a phase where you can fry

374
00:16:20.590 --> 00:16:22.689
<v ANDREY>an endpoint and get an output right of the

375
00:16:22.690 --> 00:16:24.039
<v ANDREY>challenges.

376
00:16:24.040 --> 00:16:26.709
<v ANDREY>Now explain why it's doing what it's

377
00:16:26.710 --> 00:16:27.729
<v ANDREY>doing, right?

378
00:16:27.730 --> 00:16:29.189
<v UPOL>Absolutely. Yeah.

379
00:16:29.190 --> 00:16:32.709
<v UPOL>And actually, now that brings to another point,

380
00:16:32.710 --> 00:16:34.659
<v UPOL>you know, there are many ways and then you hear

381
00:16:34.660 --> 00:16:37.619
<v UPOL>different words being kind of used.

382
00:16:37.620 --> 00:16:41.139
<v UPOL>In my view, I kind of split explainability

383
00:16:41.140 --> 00:16:43.749
<v UPOL>into like transparency,

384
00:16:43.750 --> 00:16:46.659
<v UPOL>interpretability kind of

385
00:16:46.660 --> 00:16:49.359
<v UPOL>branches and then post hoc explainability.

386
00:16:49.360 --> 00:16:51.549
<v UPOL>So I'll cover all eight of these.

387
00:16:51.550 --> 00:16:54.309
<v UPOL>So transparency would be almost like clear boxing

388
00:16:54.310 --> 00:16:56.979
<v UPOL>it so like instead of like black boxing it could

389
00:16:56.980 --> 00:17:00.099
<v UPOL>you just make the model just completely transparent,

390
00:17:00.100 --> 00:17:01.629
<v UPOL>like that's just one of the ideal to

391
00:17:01.630 --> 00:17:03.969
<v ANDREY>understand the model itself.

392
00:17:03.970 --> 00:17:06.578
<v UPOL>Then interpretability involves, I add

393
00:17:06.579 --> 00:17:09.189
<v UPOL>in my view, the able to scrutinize an

394
00:17:09.190 --> 00:17:11.259
<v UPOL>algorithm. So in other words, like like a decision

395
00:17:11.260 --> 00:17:13.449
<v UPOL>tree life, like the infrastructure or the

396
00:17:13.450 --> 00:17:16.269
<v UPOL>architecture of forms,

397
00:17:16.270 --> 00:17:19.088
<v UPOL>the fact that I can poke and prod

398
00:17:19.089 --> 00:17:22.179
<v UPOL>and I can get a good understanding

399
00:17:22.180 --> 00:17:25.118
<v UPOL>and I can interpret what the model is doing right.

400
00:17:25.119 --> 00:17:27.879
<v UPOL>But that also requires a level of expertize like

401
00:17:27.880 --> 00:17:29.739
<v UPOL>you need to have the training to interpret a

402
00:17:29.740 --> 00:17:31.929
<v UPOL>decision tree. You cannot just, you know, you can't

403
00:17:31.930 --> 00:17:33.669
<v UPOL>just give anyone on the street like, Hey, who's a

404
00:17:33.670 --> 00:17:35.679
<v UPOL>decision tree? Interpret it, right?

405
00:17:35.680 --> 00:17:37.899
<v UPOL>So there's this level of interpretation that comes

406
00:17:37.900 --> 00:17:41.079
<v UPOL>in, but the architecture of the model

407
00:17:41.080 --> 00:17:43.389
<v UPOL>should also be able to support it.

408
00:17:43.390 --> 00:17:45.909
<v UPOL>Not as you seem like deep learning algorithms are

409
00:17:45.910 --> 00:17:48.279
<v UPOL>not really interpretive or by their architecture,

410
00:17:48.280 --> 00:17:51.519
<v UPOL>right? Like they're not very friendly on their side.

411
00:17:51.520 --> 00:17:54.129
<v UPOL>So recently, there has been a very big

412
00:17:54.130 --> 00:17:57.099
<v UPOL>push towards what we call post hoc explanations.

413
00:17:57.100 --> 00:17:59.769
<v UPOL>Right? So adding a layer, a

414
00:17:59.770 --> 00:18:02.559
<v UPOL>model on top of the black box, so to speak, and make

415
00:18:02.560 --> 00:18:04.149
<v UPOL>it somewhat transparent.

416
00:18:04.150 --> 00:18:06.459
<v UPOL>So in other words, can I generate the explanation

417
00:18:06.460 --> 00:18:09.399
<v UPOL>after the decision has been made?

418
00:18:09.400 --> 00:18:12.009
<v UPOL>So those are the three main branches you see

419
00:18:12.010 --> 00:18:14.799
<v UPOL>work within explainable AI these days,

420
00:18:14.800 --> 00:18:17.679
<v UPOL>and a lot of people do use the word explainability

421
00:18:17.680 --> 00:18:20.559
<v UPOL>and interpretability interchangeably.

422
00:18:20.560 --> 00:18:23.289
<v UPOL>I don't. I tend to see explainability

423
00:18:23.290 --> 00:18:26.169
<v UPOL>as a larger umbrella that

424
00:18:26.170 --> 00:18:28.989
<v UPOL>can house, but doesn't mean I'm right to be honest,

425
00:18:28.990 --> 00:18:31.299
<v UPOL>like it's being very precise about what you're

426
00:18:31.300 --> 00:18:32.500
<v UPOL>saying when you're saying it.

427
00:18:33.520 --> 00:18:36.159
<v UPOL>Does that help like kind of give the demarcation of

428
00:18:36.160 --> 00:18:38.739
<v UPOL>the landscape as well the area in the work?

429
00:18:38.740 --> 00:18:41.349
<v ANDREY>Yeah, yeah. Of course it's it's interesting that at

430
00:18:41.350 --> 00:18:43.359
<v ANDREY>least you can think of it in these different

431
00:18:43.360 --> 00:18:46.359
<v ANDREY>dimensions, and I think that also helps understand

432
00:18:46.360 --> 00:18:49.179
<v ANDREY>sort of the ways you

433
00:18:49.180 --> 00:18:51.939
<v ANDREY>might approach it. And speaking of that, as

434
00:18:51.940 --> 00:18:54.879
<v ANDREY>you introduced in the intro,

435
00:18:54.880 --> 00:18:57.969
<v ANDREY>your work focuses in particular on human centered

436
00:18:57.970 --> 00:19:00.789
<v ANDREY>XIII, which is in some ways

437
00:19:00.790 --> 00:19:03.809
<v ANDREY>in contrast to algorithm centered XIII.

438
00:19:03.810 --> 00:19:06.489
<v ANDREY>So what is human centered

439
00:19:06.490 --> 00:19:08.229
<v ANDREY>XIII, in your view?

440
00:19:08.230 --> 00:19:10.239
<v ANDREY>Again, as kind of a surface level?

441
00:19:10.240 --> 00:19:12.939
<v UPOL>Yeah, it's about, I guess,

442
00:19:12.940 --> 00:19:15.279
<v UPOL>the way to kind of think about Incentive XIII is the

443
00:19:15.280 --> 00:19:17.949
<v UPOL>following like, there is a myth often in explainable

444
00:19:17.950 --> 00:19:20.739
<v UPOL>AI, where we tend to think that if

445
00:19:20.740 --> 00:19:23.709
<v UPOL>we could just open the black box,

446
00:19:23.710 --> 00:19:25.179
<v UPOL>everything will be fine.

447
00:19:25.180 --> 00:19:26.180
<v UPOL>Right?

448
00:19:26.770 --> 00:19:29.299
<v UPOL>And my my my response to the myth is also.

449
00:19:29.300 --> 00:19:32.089
<v UPOL>And not everything that matters

450
00:19:32.090 --> 00:19:34.339
<v UPOL>actually is inside the box.

451
00:19:34.340 --> 00:19:37.289
<v UPOL>Why? Because humans don't live inside

452
00:19:37.290 --> 00:19:40.909
<v UPOL>the black box office, they're outside and around it.

453
00:19:40.910 --> 00:19:42.170
<v UPOL>And given,

454
00:19:43.520 --> 00:19:46.399
<v UPOL>you know, humans are so instrumental

455
00:19:46.400 --> 00:19:48.259
<v UPOL>in this ecosystem, right?

456
00:19:50.030 --> 00:19:52.699
<v UPOL>It might not be a bad idea to start looking

457
00:19:52.700 --> 00:19:55.339
<v UPOL>around the box to understand what

458
00:19:55.340 --> 00:19:56.539
<v UPOL>are these value systems?

459
00:19:56.540 --> 00:19:59.329
<v UPOL>What are people's ways of thinking that can

460
00:19:59.330 --> 00:20:02.059
<v UPOL>ultimately aid that

461
00:20:02.060 --> 00:20:04.279
<v UPOL>understanding ability that is so instrumental,

462
00:20:04.280 --> 00:20:07.099
<v UPOL>explainable and so human centered, explainable

463
00:20:07.100 --> 00:20:09.829
<v UPOL>AI? What it does is it fundamentally shifts

464
00:20:09.830 --> 00:20:12.559
<v UPOL>the attention, and it doesn't say that

465
00:20:12.560 --> 00:20:15.699
<v UPOL>algorithm centered work is bad by any means.

466
00:20:15.700 --> 00:20:18.409
<v UPOL>It's not saying that what we're saying is we need

467
00:20:18.410 --> 00:20:21.619
<v UPOL>to put just as much attention

468
00:20:21.620 --> 00:20:24.559
<v UPOL>on the human on who is opening

469
00:20:24.560 --> 00:20:27.199
<v UPOL>the box as much as opening the

470
00:20:27.200 --> 00:20:27.889
<v UPOL>box.

471
00:20:27.890 --> 00:20:30.889
<v ANDREY>Right? Do you need to sort of pay attention,

472
00:20:30.890 --> 00:20:33.929
<v ANDREY>care about the human aspect and not just

473
00:20:33.930 --> 00:20:36.709
<v ANDREY>think about the model? And then, you know,

474
00:20:36.710 --> 00:20:39.829
<v ANDREY>maybe we humans take with you, develop

475
00:20:39.830 --> 00:20:41.839
<v ANDREY>a model later and they can figure it out.

476
00:20:41.840 --> 00:20:44.599
<v ANDREY>That makes a lot of sense, and you have a great

477
00:20:44.600 --> 00:20:47.749
<v ANDREY>motivating example of this

478
00:20:47.750 --> 00:20:50.509
<v ANDREY>in your Great Again article having to do with

479
00:20:50.510 --> 00:20:53.419
<v ANDREY>this Fire Wall management thing

480
00:20:53.420 --> 00:20:56.209
<v ANDREY>and why human centered aspect was necessary.

481
00:20:56.210 --> 00:20:58.279
<v ANDREY>So, yeah, I find that very cool.

482
00:20:58.280 --> 00:21:00.079
<v ANDREY>Can you go ahead?

483
00:21:00.080 --> 00:21:02.689
<v UPOL>Yeah. So this was a

484
00:21:02.690 --> 00:21:05.029
<v UPOL>this was a consulting project, but I had the

485
00:21:05.030 --> 00:21:07.609
<v UPOL>privilege of kind of helping out with.

486
00:21:07.610 --> 00:21:10.249
<v UPOL>They had a cybersecurity company, had hired me

487
00:21:10.250 --> 00:21:12.889
<v UPOL>to address a very interesting issue of

488
00:21:12.890 --> 00:21:15.519
<v UPOL>this firewall management system.

489
00:21:15.520 --> 00:21:18.289
<v UPOL>And in that environment, one thing

490
00:21:18.290 --> 00:21:20.959
<v UPOL>that happens is the problem was that bloat.

491
00:21:20.960 --> 00:21:22.279
<v UPOL>So what is it? Bloat?

492
00:21:22.280 --> 00:21:25.129
<v UPOL>Bloat is what happens when people open course

493
00:21:25.130 --> 00:21:27.499
<v UPOL>on a firewall and forget to close them.

494
00:21:27.500 --> 00:21:30.859
<v UPOL>So over time, you get a bunch of stuff

495
00:21:30.860 --> 00:21:33.379
<v UPOL>that is open. But then what happens is at an

496
00:21:33.380 --> 00:21:36.139
<v UPOL>enterprise scale, there is so many

497
00:21:36.140 --> 00:21:38.749
<v UPOL>open course that is humanly impossible to go

498
00:21:38.750 --> 00:21:40.579
<v UPOL>to every one of them and check.

499
00:21:40.580 --> 00:21:41.959
<v UPOL>Oh, wow. Right.

500
00:21:41.960 --> 00:21:44.509
<v UPOL>So they had a system that would analyze all these

501
00:21:44.510 --> 00:21:47.209
<v UPOL>ports and suggest which

502
00:21:47.210 --> 00:21:49.969
<v UPOL>ones do remain closed versus which ones do remain

503
00:21:49.970 --> 00:21:51.289
<v UPOL>open.

504
00:21:51.290 --> 00:21:53.929
<v UPOL>The problem was the problem here was

505
00:21:53.930 --> 00:21:55.729
<v UPOL>rather tricky.

506
00:21:55.730 --> 00:21:58.639
<v UPOL>The system was actually performing rather well

507
00:21:58.640 --> 00:22:00.439
<v UPOL>around the 90 percent accuracy.

508
00:22:00.440 --> 00:22:04.039
<v UPOL>It had really good algorithmic transparency.

509
00:22:04.040 --> 00:22:05.299
<v UPOL>But the problem was.

510
00:22:06.610 --> 00:22:09.549
<v UPOL>Less than two percent of the workforce

511
00:22:09.550 --> 00:22:12.399
<v UPOL>was actually engaging with it and using

512
00:22:12.400 --> 00:22:13.400
<v UPOL>it.

513
00:22:14.090 --> 00:22:17.059
<v ANDREY>Yeah, and that's not what you want.

514
00:22:17.060 --> 00:22:18.979
<v ANDREY>Yeah, and then what was that?

515
00:22:18.980 --> 00:22:19.699
<v ANDREY>Yeah.

516
00:22:19.700 --> 00:22:23.029
<v UPOL>So and you know, I was brought in with the

517
00:22:23.030 --> 00:22:25.759
<v UPOL>task of fixing this and the assumption was

518
00:22:25.760 --> 00:22:27.859
<v UPOL>still back then and this was before we kind of

519
00:22:27.860 --> 00:22:29.659
<v UPOL>coined the term in the Senate, say I.

520
00:22:29.660 --> 00:22:32.179
<v UPOL>And this is the project that actually drives a lot

521
00:22:32.180 --> 00:22:33.709
<v UPOL>of that thinking.

522
00:22:33.710 --> 00:22:36.169
<v UPOL>And the assumption was, you know, maybe the solution

523
00:22:36.170 --> 00:22:38.479
<v UPOL>is within the algorithm, just fix the algorithm,

524
00:22:38.480 --> 00:22:41.239
<v UPOL>maybe make it explain better, maybe

525
00:22:41.240 --> 00:22:43.369
<v UPOL>open the box differently, so to speak.

526
00:22:44.900 --> 00:22:47.569
<v UPOL>And what I found at the end of the day just

527
00:22:47.570 --> 00:22:51.079
<v UPOL>to give a cut. The long story short, I guess,

528
00:22:51.080 --> 00:22:52.080
<v UPOL>is.

529
00:22:52.670 --> 00:22:56.509
<v UPOL>There was nothing that was wrong with the algorithm.

530
00:22:56.510 --> 00:22:59.239
<v UPOL>The explainability that this company was

531
00:22:59.240 --> 00:23:02.119
<v UPOL>looking for was at the intersection

532
00:23:02.120 --> 00:23:04.789
<v UPOL>of the human and the machine not included in the

533
00:23:04.790 --> 00:23:05.790
<v UPOL>machine.

534
00:23:06.290 --> 00:23:08.989
<v UPOL>So what we found in this project

535
00:23:08.990 --> 00:23:11.539
<v UPOL>presumption was still that something must be wrong

536
00:23:11.540 --> 00:23:13.699
<v UPOL>with the algorithm. This was before we had coined

537
00:23:13.700 --> 00:23:15.679
<v UPOL>the term Human-Centered XIII.

538
00:23:15.680 --> 00:23:18.319
<v UPOL>A lot of the work here actually drove

539
00:23:18.320 --> 00:23:19.490
<v UPOL>the philosophy behind it.

540
00:23:20.630 --> 00:23:23.929
<v UPOL>And one thing that that came up was

541
00:23:23.930 --> 00:23:26.749
<v UPOL>nothing was actually like we couldn't

542
00:23:26.750 --> 00:23:30.079
<v UPOL>do much at the algorithmic level that helped

543
00:23:30.080 --> 00:23:33.199
<v UPOL>the explainability of the system, the changes

544
00:23:33.200 --> 00:23:35.569
<v UPOL>that had to be done, which actually I think we'll

545
00:23:35.570 --> 00:23:37.879
<v UPOL>get into when we discuss the expanding

546
00:23:37.880 --> 00:23:41.179
<v UPOL>explainability paper is at the

547
00:23:41.180 --> 00:23:42.469
<v UPOL>social level.

548
00:23:42.470 --> 00:23:45.589
<v UPOL>So what was the problem here was people

549
00:23:45.590 --> 00:23:48.469
<v UPOL>had no idea how to calibrate

550
00:23:48.470 --> 00:23:51.649
<v UPOL>their trust on this system

551
00:23:51.650 --> 00:23:54.499
<v UPOL>that without really understanding

552
00:23:54.500 --> 00:23:57.169
<v UPOL>how others are also interacting with

553
00:23:57.170 --> 00:23:58.669
<v UPOL>the system. Right.

554
00:23:58.670 --> 00:24:00.979
<v UPOL>So for instance, if I'm faced with a new system and

555
00:24:00.980 --> 00:24:03.949
<v UPOL>there is no notion of the ground truth, right?

556
00:24:03.950 --> 00:24:06.889
<v UPOL>And the easiest example to share here was

557
00:24:06.890 --> 00:24:09.229
<v UPOL>there was a young analyst and I'm using pseudonyms

558
00:24:09.230 --> 00:24:11.810
<v UPOL>like Julie and Julie

559
00:24:13.010 --> 00:24:14.749
<v UPOL>had a recommendation from the A.I.

560
00:24:14.750 --> 00:24:17.359
<v UPOL>system and to close a few

561
00:24:17.360 --> 00:24:20.089
<v UPOL>ports. And on paper, the recommendation

562
00:24:20.090 --> 00:24:21.090
<v UPOL>was not wrong.

563
00:24:22.070 --> 00:24:24.889
<v UPOL>I have suggested that, hey, you know, if you

564
00:24:24.890 --> 00:24:26.749
<v UPOL>close these ports because they have been open for a

565
00:24:26.750 --> 00:24:28.069
<v UPOL>long time, they have not been used.

566
00:24:28.070 --> 00:24:31.279
<v UPOL>So technically, these are not bad suggestions.

567
00:24:31.280 --> 00:24:33.769
<v UPOL>Julie, not knowing a lot of the institutional

568
00:24:33.770 --> 00:24:36.619
<v UPOL>history and how things are done accepted

569
00:24:36.620 --> 00:24:37.640
<v UPOL>this decision.

570
00:24:38.700 --> 00:24:41.309
<v UPOL>Two weeks later, the company faced a

571
00:24:41.310 --> 00:24:42.310
<v UPOL>breach.

572
00:24:43.240 --> 00:24:46.019
<v UPOL>And then lost around $2 billion in

573
00:24:46.020 --> 00:24:47.020
<v UPOL>one.

574
00:24:47.810 --> 00:24:50.569
<v UPOL>What had happened was Julie

575
00:24:50.570 --> 00:24:53.419
<v UPOL>had accidentally closed following

576
00:24:53.420 --> 00:24:56.419
<v UPOL>the A's recommendation, the backup center reports.

577
00:24:56.420 --> 00:24:59.179
<v UPOL>Right. So because their backups are reports,

578
00:24:59.180 --> 00:25:01.219
<v UPOL>of course, it's good that they have not been used,

579
00:25:01.220 --> 00:25:04.219
<v UPOL>right? It is also good that they're open.

580
00:25:04.220 --> 00:25:06.529
<v UPOL>So this kind of highlights a very interesting

581
00:25:06.530 --> 00:25:09.379
<v UPOL>tension here that even though the air

582
00:25:09.380 --> 00:25:12.049
<v UPOL>system was technically not right,

583
00:25:12.050 --> 00:25:14.299
<v UPOL>Julie actually got fired.

584
00:25:14.300 --> 00:25:16.169
<v UPOL>Oh, that's that's a shame.

585
00:25:16.170 --> 00:25:19.009
<v UPOL>Yeah, yeah. So the accountability is squarely light

586
00:25:19.010 --> 00:25:21.829
<v UPOL>on the human user, even though the human

587
00:25:21.830 --> 00:25:23.679
<v UPOL>user in this case, they are not data scientist,

588
00:25:23.680 --> 00:25:26.059
<v UPOL>either cybersecurity analysts, they shouldn't have

589
00:25:26.060 --> 00:25:28.639
<v UPOL>to know how this guy is working.

590
00:25:28.640 --> 00:25:31.519
<v UPOL>So it's very hard in real world situations

591
00:25:31.520 --> 00:25:34.369
<v UPOL>to answer the following question one does

592
00:25:34.370 --> 00:25:35.509
<v UPOL>this A.I.

593
00:25:35.510 --> 00:25:36.529
<v UPOL>not know,

594
00:25:38.240 --> 00:25:41.419
<v UPOL>right? And to address that question

595
00:25:41.420 --> 00:25:43.160
<v UPOL>is almost an unknown, unknown, right?

596
00:25:44.210 --> 00:25:46.789
<v UPOL>You need and in this case, in this case study, they

597
00:25:46.790 --> 00:25:48.649
<v UPOL>needed this thing.

598
00:25:48.650 --> 00:25:51.619
<v UPOL>What the socio organizational context

599
00:25:51.620 --> 00:25:53.809
<v UPOL>to help them understand how are other people dealing

600
00:25:53.810 --> 00:25:56.479
<v UPOL>with it and and watching how others

601
00:25:56.480 --> 00:25:58.459
<v UPOL>are acting with it, they were able to develop more

602
00:25:58.460 --> 00:26:01.099
<v UPOL>robust mental models of how

603
00:26:01.100 --> 00:26:03.559
<v UPOL>to calibrate that trust on the system.

604
00:26:03.560 --> 00:26:06.259
<v UPOL>In other words, which are the situations that

605
00:26:06.260 --> 00:26:08.779
<v UPOL>I want to see really well and which are the

606
00:26:08.780 --> 00:26:11.119
<v UPOL>situations that A.I. does not perform really well

607
00:26:11.120 --> 00:26:13.279
<v UPOL>because even if the performance is not uniform,

608
00:26:13.280 --> 00:26:15.889
<v UPOL>that's the other reality in these real world

609
00:26:15.890 --> 00:26:17.179
<v UPOL>systems.

610
00:26:17.180 --> 00:26:19.699
<v UPOL>So that's just, you know, just a quick summarization

611
00:26:19.700 --> 00:26:22.579
<v UPOL>of this out of that case study,

612
00:26:22.580 --> 00:26:25.129
<v UPOL>which kind of showed me that there were elements

613
00:26:25.130 --> 00:26:27.799
<v UPOL>outside the black box that we really

614
00:26:27.800 --> 00:26:31.009
<v UPOL>needed to incorporate in the decision making

615
00:26:31.010 --> 00:26:33.769
<v UPOL>to help decision makers

616
00:26:33.770 --> 00:26:36.739
<v UPOL>do it right and to make sure accountability

617
00:26:36.740 --> 00:26:39.829
<v UPOL>was shared rather than be inappropriately

618
00:26:39.830 --> 00:26:42.529
<v UPOL>placed all on the human and nothing on

619
00:26:42.530 --> 00:26:43.879
<v UPOL>the machine.

620
00:26:43.880 --> 00:26:45.529
<v ANDREY>Yeah, yeah, it's interesting.

621
00:26:45.530 --> 00:26:49.099
<v ANDREY>I think a lot of listeners might now appreciate

622
00:26:49.100 --> 00:26:52.359
<v ANDREY>the importance of this kind of work in terms of,

623
00:26:52.360 --> 00:26:53.719
<v ANDREY>you know, outcome come here.

624
00:26:53.720 --> 00:26:56.599
<v ANDREY>And I think we'll dig in a bit more into

625
00:26:56.600 --> 00:26:59.389
<v ANDREY>where you want are in terms of how

626
00:26:59.390 --> 00:27:01.279
<v ANDREY>you do it, which was really interesting.

627
00:27:02.390 --> 00:27:05.239
<v ANDREY>Now, with a lot of these concepts laid out

628
00:27:05.240 --> 00:27:07.849
<v ANDREY>before we get into kind of our main

629
00:27:07.850 --> 00:27:10.519
<v ANDREY>focus, I try to be fun to walk through

630
00:27:10.520 --> 00:27:13.339
<v ANDREY>kind of your journey

631
00:27:13.340 --> 00:27:16.789
<v ANDREY>in some sense of your trajectory,

632
00:27:16.790 --> 00:27:19.399
<v ANDREY>starting out less human centered and then sort

633
00:27:19.400 --> 00:27:22.549
<v ANDREY>of discovering that and more and more coming closer

634
00:27:22.550 --> 00:27:24.499
<v ANDREY>to where you are now.

635
00:27:24.500 --> 00:27:27.309
<v ANDREY>So first, you had kind of,

636
00:27:27.310 --> 00:27:30.039
<v ANDREY>let's say, a more traditional maybe X

637
00:27:30.040 --> 00:27:32.809
<v ANDREY>hardware called rationalization and neural

638
00:27:32.810 --> 00:27:35.599
<v ANDREY>machine translation approach to generating

639
00:27:35.600 --> 00:27:38.269
<v ANDREY>natural language explanations.

640
00:27:38.270 --> 00:27:40.909
<v ANDREY>So just in brief, you know, what was

641
00:27:40.910 --> 00:27:44.629
<v ANDREY>this paper and sort of what was the

642
00:27:44.630 --> 00:27:46.039
<v ANDREY>contribution there?

643
00:27:46.040 --> 00:27:47.329
<v UPOL>No, thank you for asking that.

644
00:27:47.330 --> 00:27:50.029
<v UPOL>I think this is the phase in my dissertation that I

645
00:27:50.030 --> 00:27:51.829
<v UPOL>call turn to the machine.

646
00:27:51.830 --> 00:27:54.439
<v UPOL>Mm hmm. I've kind of takes a few turns in

647
00:27:54.440 --> 00:27:57.319
<v UPOL>this turn to the machine mark

648
00:27:57.320 --> 00:27:59.149
<v UPOL>and I kind of end Brandt.

649
00:27:59.150 --> 00:28:01.249
<v UPOL>So I just when I thought on my coauthors like Brant

650
00:28:01.250 --> 00:28:03.079
<v UPOL>Harrison, who is at the University of Kentucky, my

651
00:28:03.080 --> 00:28:05.299
<v UPOL>fiddle, obviously his tech

652
00:28:06.470 --> 00:28:09.409
<v UPOL>and per now is also now I think it's eastern

653
00:28:09.410 --> 00:28:12.259
<v UPOL>at Georgia Tech and Larry Chen, who

654
00:28:12.260 --> 00:28:14.359
<v UPOL>is now graduated from Georgia Tech.

655
00:28:14.360 --> 00:28:17.390
<v UPOL>We kind of started thinking that, you know,

656
00:28:18.530 --> 00:28:21.199
<v UPOL>wouldn't it be nice if

657
00:28:21.200 --> 00:28:24.079
<v UPOL>the AI system talk to you or thought

658
00:28:24.080 --> 00:28:26.269
<v UPOL>out loud in plain English?

659
00:28:26.270 --> 00:28:29.059
<v UPOL>Mm hmm. And the reason why we kind of thought

660
00:28:29.060 --> 00:28:32.359
<v UPOL>about that was, Hey, I'm

661
00:28:32.360 --> 00:28:35.509
<v UPOL>not everyone has the background to interpret

662
00:28:35.510 --> 00:28:37.159
<v UPOL>models, right?

663
00:28:37.160 --> 00:28:40.009
<v UPOL>And our a lot of our end users are not A.I.

664
00:28:40.010 --> 00:28:42.649
<v UPOL>experts, but everyone, if they

665
00:28:42.650 --> 00:28:44.809
<v UPOL>can speak and read and write in English, could

666
00:28:44.810 --> 00:28:46.009
<v UPOL>understand English, right?

667
00:28:46.010 --> 00:28:48.499
<v UPOL>In fact, that's how we even communicate.

668
00:28:48.500 --> 00:28:51.499
<v UPOL>So I in this paper actually do a lot

669
00:28:51.500 --> 00:28:54.469
<v UPOL>of inspiration from philosophy

670
00:28:54.470 --> 00:28:57.199
<v UPOL>of language, namely the work of Jerry Fodor

671
00:28:58.430 --> 00:29:01.369
<v UPOL>to kind of and work with Brant to kind of develop

672
00:29:01.370 --> 00:29:04.339
<v UPOL>the algorithmic infrastructure to

673
00:29:04.340 --> 00:29:05.929
<v UPOL>answer the following question.

674
00:29:05.930 --> 00:29:08.659
<v UPOL>And then this is the question that is asked me

675
00:29:08.660 --> 00:29:10.319
<v UPOL>in this paper Can we?

676
00:29:10.320 --> 00:29:12.679
<v UPOL>This is almost like an existence proof, like can we

677
00:29:12.680 --> 00:29:15.799
<v UPOL>generate rationales from

678
00:29:15.800 --> 00:29:18.829
<v UPOL>using a neural machine translation approach?

679
00:29:18.830 --> 00:29:20.809
<v UPOL>And this was the first one.

680
00:29:20.810 --> 00:29:23.839
<v UPOL>Yeah, to our knowledge, that uses an NMT

681
00:29:23.840 --> 00:29:27.079
<v UPOL>mechanism. Instead of translating from

682
00:29:27.080 --> 00:29:29.689
<v UPOL>like English to Bengali, like natural

683
00:29:29.690 --> 00:29:32.329
<v UPOL>language to natural language, we we felt what if

684
00:29:32.330 --> 00:29:34.549
<v UPOL>we replace one of the natural languages with some

685
00:29:34.550 --> 00:29:36.589
<v UPOL>data structures? Right, right.

686
00:29:36.590 --> 00:29:38.659
<v UPOL>And that's the insight in this case.

687
00:29:38.660 --> 00:29:41.359
<v UPOL>And the innovation was we were able to back

688
00:29:41.360 --> 00:29:43.489
<v UPOL>in the day, like when this paper was published back

689
00:29:43.490 --> 00:29:46.129
<v UPOL>in 2017 18, there was a lot of work

690
00:29:46.130 --> 00:29:47.329
<v UPOL>going on automated image.

691
00:29:47.330 --> 00:29:49.939
<v UPOL>Captioning and stuff like that, but very little

692
00:29:49.940 --> 00:29:52.789
<v UPOL>work was done on sequential

693
00:29:52.790 --> 00:29:54.679
<v UPOL>decision making, right?

694
00:29:54.680 --> 00:29:56.779
<v UPOL>So like, you know, if you can think of robotics,

695
00:29:56.780 --> 00:29:59.519
<v UPOL>right, like getting a robot from one point

696
00:29:59.520 --> 00:30:02.149
<v UPOL>in the kitchen to be in the kitchen is a

697
00:30:02.150 --> 00:30:04.069
<v UPOL>sequential decision making task.

698
00:30:04.070 --> 00:30:06.769
<v UPOL>So we actually took a sequential decision making

699
00:30:06.770 --> 00:30:09.709
<v UPOL>environment and need an agent

700
00:30:09.710 --> 00:30:12.349
<v UPOL>navigate it while being

701
00:30:12.350 --> 00:30:15.019
<v UPOL>able to think out loud in plain English.

702
00:30:15.020 --> 00:30:17.299
<v ANDREY>If I remember correctly, this was like a game.

703
00:30:17.300 --> 00:30:18.929
<v ANDREY>Frogger? Yes.

704
00:30:18.930 --> 00:30:20.419
<v UPOL>Yes, yes.

705
00:30:20.420 --> 00:30:23.029
<v UPOL>Yes. So that that was an homage to a lot

706
00:30:23.030 --> 00:30:25.669
<v UPOL>of the game work that goes at the entertainment

707
00:30:25.670 --> 00:30:28.069
<v UPOL>intelligent and Human-Centered AI Lab at Georgia

708
00:30:28.070 --> 00:30:30.649
<v UPOL>Tech. So we kind of leveraged a lot of our game A.I.

709
00:30:30.650 --> 00:30:33.319
<v UPOL>history, which I know, you know, I know you

710
00:30:33.320 --> 00:30:35.509
<v UPOL>were at Georgia Tech for undergrad, so I think you

711
00:30:35.510 --> 00:30:37.729
<v UPOL>might also be familiar with a bit of that.

712
00:30:37.730 --> 00:30:38.889
<v ANDREY>Oh, yeah, yeah, yeah.

713
00:30:38.890 --> 00:30:41.689
<v ANDREY>And yeah, Frontier is a fine example because it's

714
00:30:41.690 --> 00:30:43.479
<v ANDREY>pretty intuitive, right?

715
00:30:43.480 --> 00:30:45.769
<v ANDREY>You know, why do you want to jump forward well as a

716
00:30:45.770 --> 00:30:48.379
<v ANDREY>car racing towards Minnesota to

717
00:30:48.380 --> 00:30:49.380
<v ANDREY>avoid it?

718
00:30:50.420 --> 00:30:53.179
<v ANDREY>Yeah, but that was a cool start and

719
00:30:53.180 --> 00:30:55.819
<v ANDREY>certainly interesting. But since when you have

720
00:30:55.820 --> 00:30:59.479
<v ANDREY>moved more towards the humor center aspect,

721
00:30:59.480 --> 00:31:02.419
<v ANDREY>so that's going to the next step, I suppose

722
00:31:02.420 --> 00:31:05.209
<v ANDREY>return to a human, which I think started

723
00:31:05.210 --> 00:31:07.939
<v ANDREY>in the way of this other paper automated rationale

724
00:31:07.940 --> 00:31:10.729
<v ANDREY>generation kind of extending this, but then

725
00:31:10.730 --> 00:31:13.279
<v ANDREY>a technique for explainable AI and its effects on

726
00:31:13.280 --> 00:31:14.280
<v ANDREY>human perception.

727
00:31:15.650 --> 00:31:17.929
<v ANDREY>So how did that come about?

728
00:31:17.930 --> 00:31:20.569
<v UPOL>So, yeah, so this one, so after we

729
00:31:20.570 --> 00:31:22.489
<v UPOL>ask the question, can we generate?

730
00:31:22.490 --> 00:31:24.289
<v UPOL>And the answer was yes.

731
00:31:24.290 --> 00:31:26.899
<v UPOL>Now we ask the question, OK.

732
00:31:26.900 --> 00:31:30.349
<v UPOL>These generated rationales, are they any good,

733
00:31:30.350 --> 00:31:32.899
<v UPOL>right? Like because back then, if you think about

734
00:31:32.900 --> 00:31:35.749
<v UPOL>how we used to evaluate these generative

735
00:31:35.750 --> 00:31:39.229
<v UPOL>systems, you know, blue score or other procedural

736
00:31:39.230 --> 00:31:41.899
<v UPOL>techniques are good, but we

737
00:31:41.900 --> 00:31:44.089
<v UPOL>don't really get a sense of how good they are to

738
00:31:44.090 --> 00:31:45.769
<v UPOL>human beings. Right?

739
00:31:45.770 --> 00:31:48.979
<v UPOL>Like, do people actually find these plausible?

740
00:31:48.980 --> 00:31:51.979
<v UPOL>So in this paper, ours are like kind of starts

741
00:31:51.980 --> 00:31:53.479
<v UPOL>to turn to the human.

742
00:31:53.480 --> 00:31:57.139
<v UPOL>We presented the first work that

743
00:31:57.140 --> 00:32:00.349
<v UPOL>gave a robust human centered use our study

744
00:32:00.350 --> 00:32:03.829
<v UPOL>along certain dimensions of user perceptions

745
00:32:03.830 --> 00:32:06.739
<v UPOL>to evaluate these rationale

746
00:32:06.740 --> 00:32:08.389
<v UPOL>generating systems.

747
00:32:08.390 --> 00:32:11.239
<v UPOL>And what we found was that

748
00:32:11.240 --> 00:32:12.499
<v UPOL>we bridged a lot of work.

749
00:32:12.500 --> 00:32:15.139
<v UPOL>So I took all of these measures and adapted

750
00:32:15.140 --> 00:32:17.779
<v UPOL>it from work in HCI human robot

751
00:32:17.780 --> 00:32:20.899
<v UPOL>interaction, as well as the technology acceptance

752
00:32:20.900 --> 00:32:23.599
<v UPOL>models from back in the 90s when automation was

753
00:32:23.600 --> 00:32:24.890
<v UPOL>becoming hot.

754
00:32:25.970 --> 00:32:28.969
<v UPOL>And we found fascinating things around, not just

755
00:32:28.970 --> 00:32:31.579
<v UPOL>the fact that these were plausible in this paper.

756
00:32:31.580 --> 00:32:34.369
<v UPOL>We just didn't make the Frogger kind of say

757
00:32:34.370 --> 00:32:37.069
<v UPOL>things. In one way, we were able to tweak

758
00:32:37.070 --> 00:32:39.949
<v UPOL>the network in a way that I could make

759
00:32:39.950 --> 00:32:42.589
<v UPOL>Frogger cough more in detail

760
00:32:42.590 --> 00:32:45.379
<v UPOL>versus, say things more shortly

761
00:32:45.380 --> 00:32:46.400
<v UPOL>in its rationales.

762
00:32:47.540 --> 00:32:50.209
<v UPOL>And we found that the level of detail

763
00:32:50.210 --> 00:32:52.939
<v UPOL>also had a lot of interesting

764
00:32:52.940 --> 00:32:56.359
<v UPOL>interweaving effects on people's trust,

765
00:32:56.360 --> 00:32:59.029
<v UPOL>people's confidence, how tolerant,

766
00:32:59.030 --> 00:33:02.119
<v UPOL>where they when the robots like that further failed,

767
00:33:02.120 --> 00:33:04.969
<v UPOL>right? So this was a really interesting deep

768
00:33:04.970 --> 00:33:07.009
<v UPOL>dove. And we just not only did the quality

769
00:33:07.010 --> 00:33:09.979
<v UPOL>quantitative part, we did a really good qualitative

770
00:33:09.980 --> 00:33:11.509
<v UPOL>part as well.

771
00:33:11.510 --> 00:33:13.369
<v UPOL>These are the crowd workers.

772
00:33:13.370 --> 00:33:16.129
<v UPOL>And, you know, getting Amazon Mechanical

773
00:33:16.130 --> 00:33:18.799
<v UPOL>Turk first to take a forty five minute

774
00:33:18.800 --> 00:33:21.499
<v UPOL>task is not easy, I

775
00:33:21.500 --> 00:33:24.499
<v UPOL>think. And so we were liking the methodology

776
00:33:24.500 --> 00:33:27.019
<v UPOL>part. I think we were very happy with it, and I'm so

777
00:33:27.020 --> 00:33:28.460
<v UPOL>proud of the team that did it.

778
00:33:30.160 --> 00:33:32.869
<v UPOL>They were saying Han and another research

779
00:33:32.870 --> 00:33:35.179
<v UPOL>assistant were undergrads at Georgia Tech who helped

780
00:33:35.180 --> 00:33:37.789
<v UPOL>us create a really good data

781
00:33:37.790 --> 00:33:40.459
<v UPOL>collection pipeline that helped us collect these

782
00:33:40.460 --> 00:33:42.259
<v UPOL>rationales to train.

783
00:33:42.260 --> 00:33:44.719
<v UPOL>And then we not only train, but we also tested it.

784
00:33:44.720 --> 00:33:47.119
<v UPOL>So that was the end to end kind of application of

785
00:33:47.120 --> 00:33:49.519
<v UPOL>this that really made the paper.

786
00:33:49.520 --> 00:33:52.549
<v UPOL>One of my favorite papers that I've gotten great.

787
00:33:52.550 --> 00:33:55.429
<v ANDREY>Yeah, is this reminds me a little bit

788
00:33:55.430 --> 00:33:58.189
<v ANDREY>of the whole like.

789
00:33:58.190 --> 00:34:00.679
<v ANDREY>Some field of social robotics is quite interesting

790
00:34:00.680 --> 00:34:03.469
<v ANDREY>because again, there's a lot to do with human

791
00:34:03.470 --> 00:34:06.499
<v ANDREY>perceptions and like, how do you communicate intent

792
00:34:06.500 --> 00:34:09.169
<v ANDREY>of grasping a couch in a way that

793
00:34:09.170 --> 00:34:10.759
<v ANDREY>you know, people can understand?

794
00:34:10.760 --> 00:34:13.428
<v ANDREY>Or how do you appear

795
00:34:13.429 --> 00:34:14.669
<v ANDREY>friendly and so on?

796
00:34:14.670 --> 00:34:17.299
<v ANDREY>That's its own whole thing, and it's always

797
00:34:17.300 --> 00:34:19.968
<v ANDREY>interesting to see that, you know, aside from

798
00:34:19.969 --> 00:34:22.819
<v ANDREY>all of the social models, if you need air

799
00:34:22.820 --> 00:34:25.609
<v ANDREY>during the real world, this is also a big

800
00:34:25.610 --> 00:34:28.019
<v ANDREY>challenge indeed.

801
00:34:28.020 --> 00:34:30.869
<v ANDREY>Yes, so in this one, one

802
00:34:30.870 --> 00:34:33.869
<v ANDREY>aspect that differs from

803
00:34:33.870 --> 00:34:36.928
<v ANDREY>the other white we'll we'll get to soon is

804
00:34:36.929 --> 00:34:39.448
<v ANDREY>here, you sort of are still dealing with one to one

805
00:34:39.449 --> 00:34:42.299
<v ANDREY>interaction versus playing game and then the

806
00:34:42.300 --> 00:34:44.939
<v ANDREY>agent is kind of trying

807
00:34:44.940 --> 00:34:47.339
<v ANDREY>to make it clear what's going on.

808
00:34:47.340 --> 00:34:49.979
<v ANDREY>And you already mentioned in your example that you

809
00:34:49.980 --> 00:34:52.649
<v ANDREY>know you need and many real war situations

810
00:34:52.650 --> 00:34:55.379
<v ANDREY>to go beyond that, you need organizational context.

811
00:34:55.380 --> 00:34:57.539
<v ANDREY>You need to understand groups of people, so to

812
00:34:57.540 --> 00:35:01.169
<v ANDREY>speak. And that takes us to

813
00:35:01.170 --> 00:35:03.899
<v ANDREY>the concept of socio technical

814
00:35:03.900 --> 00:35:04.900
<v ANDREY>challenges.

815
00:35:06.100 --> 00:35:08.969
<v ANDREY>Yes. So how did you make that

816
00:35:08.970 --> 00:35:11.779
<v ANDREY>turn and

817
00:35:11.780 --> 00:35:15.239
<v ANDREY>what is that compared to this one to one paradigm?

818
00:35:15.240 --> 00:35:18.089
<v UPOL>Absolutely. So you hit the

819
00:35:18.090 --> 00:35:20.759
<v UPOL>nail on the head, right? There is like a lot of the

820
00:35:20.760 --> 00:35:23.129
<v UPOL>way we were thinking about the rational generation

821
00:35:23.130 --> 00:35:25.709
<v UPOL>or the interaction paradigm was very much one to

822
00:35:25.710 --> 00:35:28.619
<v UPOL>one. And, you know, based on my prior

823
00:35:28.620 --> 00:35:31.559
<v UPOL>work in industry settings, I started realizing

824
00:35:31.560 --> 00:35:34.530
<v UPOL>that is that truly representative of what happens.

825
00:35:35.700 --> 00:35:38.399
<v UPOL>And I started realizing that, no, we need to think

826
00:35:38.400 --> 00:35:41.129
<v UPOL>more about like these AI systems.

827
00:35:41.130 --> 00:35:43.199
<v UPOL>I'm never in a vacuum.

828
00:35:43.200 --> 00:35:45.929
<v UPOL>They're often situated in larger organizational

829
00:35:45.930 --> 00:35:47.399
<v UPOL>environments.

830
00:35:47.400 --> 00:35:50.219
<v UPOL>So in that case, how do we think about this?

831
00:35:50.220 --> 00:35:52.409
<v UPOL>How do we conceptualize this?

832
00:35:52.410 --> 00:35:55.019
<v UPOL>So this kind of forced us, and this

833
00:35:55.020 --> 00:35:57.809
<v UPOL>is probably the first kind of conceptual paper that

834
00:35:57.810 --> 00:36:00.569
<v UPOL>I have written is to kind of outline.

835
00:36:00.570 --> 00:36:03.539
<v UPOL>So we kind of coined the term human centered essay,

836
00:36:03.540 --> 00:36:06.419
<v UPOL>but we also wanted to seen how do you operationalize

837
00:36:06.420 --> 00:36:09.419
<v UPOL>this thing? So we bridged theories

838
00:36:09.420 --> 00:36:12.149
<v UPOL>from critical A.I. studies like critical technical

839
00:36:12.150 --> 00:36:15.179
<v UPOL>practice in HCI, like reflective

840
00:36:15.180 --> 00:36:18.509
<v UPOL>design and value sensitive design.

841
00:36:18.510 --> 00:36:21.359
<v UPOL>And we kind of talked a little bit about, OK,

842
00:36:21.360 --> 00:36:24.119
<v UPOL>now we have this insight that we have to not just

843
00:36:24.120 --> 00:36:26.189
<v UPOL>care about one person, but also multiple

844
00:36:26.190 --> 00:36:28.289
<v UPOL>stakeholders in the system.

845
00:36:28.290 --> 00:36:30.839
<v UPOL>So going back to the cybersecurity example, right,

846
00:36:30.840 --> 00:36:32.819
<v UPOL>it's not just the analyst who is making the

847
00:36:32.820 --> 00:36:36.209
<v UPOL>decision, it's also the decision of the analysts,

848
00:36:36.210 --> 00:36:38.879
<v UPOL>previous, who had made similar decisions in

849
00:36:38.880 --> 00:36:41.309
<v UPOL>the past. So that kind of forced us to kind of

850
00:36:41.310 --> 00:36:43.949
<v UPOL>imagine and envision AI

851
00:36:43.950 --> 00:36:46.889
<v UPOL>explainable AI paradigm that is more human centered

852
00:36:46.890 --> 00:36:50.039
<v UPOL>and not just one human, but also incorporates

853
00:36:50.040 --> 00:36:51.040
<v UPOL>many humans.

854
00:36:52.240 --> 00:36:55.089
<v ANDREY>Mm hmm. Yeah, so there comes a socio technical

855
00:36:55.090 --> 00:36:58.779
<v ANDREY>aspect. You know, social being,

856
00:36:58.780 --> 00:37:01.299
<v ANDREY>you know, interactions between people and even

857
00:37:01.300 --> 00:37:02.799
<v ANDREY>organizations.

858
00:37:02.800 --> 00:37:05.739
<v ANDREY>So where you Mary, sort of the groups of people

859
00:37:05.740 --> 00:37:08.649
<v ANDREY>with the technical problem, which

860
00:37:08.650 --> 00:37:12.009
<v ANDREY>now you really need to think about both.

861
00:37:12.010 --> 00:37:14.829
<v ANDREY>And that as far

862
00:37:14.830 --> 00:37:17.499
<v ANDREY>as other sound was sort of kind of

863
00:37:17.500 --> 00:37:20.229
<v ANDREY>new direction that wasn't

864
00:37:20.230 --> 00:37:23.679
<v ANDREY>really the norm or stylish in the field.

865
00:37:23.680 --> 00:37:26.079
<v UPOL>Yeah. And I think that's a very important point.

866
00:37:26.080 --> 00:37:28.719
<v UPOL>In this case. I had drawn a lot

867
00:37:28.720 --> 00:37:31.959
<v UPOL>of inspiration from the fact

868
00:37:31.960 --> 00:37:33.969
<v UPOL>literature of the fairness, accountability and

869
00:37:33.970 --> 00:37:36.819
<v UPOL>transparency literature where they were

870
00:37:36.820 --> 00:37:39.129
<v UPOL>very much at that time thinking very socially or

871
00:37:39.130 --> 00:37:41.799
<v UPOL>technically. And I am always reminded of

872
00:37:41.800 --> 00:37:44.529
<v UPOL>I watched this video from Microsoft Research

873
00:37:44.530 --> 00:37:45.849
<v UPOL>is like responsible.

874
00:37:45.850 --> 00:37:48.579
<v UPOL>I kind of in visions.

875
00:37:48.580 --> 00:37:51.369
<v UPOL>And Hannah Wallach, who is at Amazon

876
00:37:51.370 --> 00:37:54.129
<v UPOL>New York, had this fascinating line that I cannot

877
00:37:54.130 --> 00:37:55.959
<v UPOL>like repeat verbatim. But the version that I

878
00:37:55.960 --> 00:37:58.119
<v UPOL>remember is today.

879
00:37:58.120 --> 00:38:00.729
<v UPOL>Our systems are AI

880
00:38:00.730 --> 00:38:03.939
<v UPOL>systems are embedded in very complex

881
00:38:03.940 --> 00:38:05.679
<v UPOL>social environments.

882
00:38:05.680 --> 00:38:08.289
<v UPOL>So that means out the effects that

883
00:38:08.290 --> 00:38:11.169
<v UPOL>these technical systems have our

884
00:38:11.170 --> 00:38:12.309
<v UPOL>social.

885
00:38:12.310 --> 00:38:15.129
<v UPOL>So that means that fundamentally socio technical

886
00:38:15.130 --> 00:38:17.709
<v UPOL>in nature, in terms of their complexities as well as

887
00:38:17.710 --> 00:38:20.379
<v UPOL>that impacts. So when we keep that

888
00:38:20.380 --> 00:38:23.229
<v UPOL>in mind, I started asking myself,

889
00:38:23.230 --> 00:38:26.409
<v UPOL>how can we get a good idea

890
00:38:26.410 --> 00:38:29.049
<v UPOL>about explainable AI if we

891
00:38:29.050 --> 00:38:32.619
<v UPOL>do not take a socio technical perspective

892
00:38:32.620 --> 00:38:35.049
<v UPOL>given, you know, in the real world, that's how these

893
00:38:35.050 --> 00:38:36.669
<v UPOL>systems are.

894
00:38:36.670 --> 00:38:39.309
<v UPOL>So that's actually a lot of the things that drove

895
00:38:39.310 --> 00:38:42.009
<v UPOL>these socio technical lens, so to speak.

896
00:38:42.010 --> 00:38:44.769
<v UPOL>And you are right, like this was the first paper, to

897
00:38:44.770 --> 00:38:47.739
<v UPOL>our knowledge, to kind of highlight that explicitly

898
00:38:47.740 --> 00:38:49.919
<v UPOL>in the context of explainable.

899
00:38:49.920 --> 00:38:52.569
<v ANDREY>I yeah, I find it interesting.

900
00:38:52.570 --> 00:38:55.509
<v ANDREY>I think it it seems like it would be easy to

901
00:38:55.510 --> 00:38:58.179
<v ANDREY>not have this realization if

902
00:38:58.180 --> 00:39:00.879
<v ANDREY>you come from a traditional sort of A.I.

903
00:39:00.880 --> 00:39:03.159
<v ANDREY>computer science research background where you just

904
00:39:03.160 --> 00:39:05.859
<v ANDREY>jump into a Ph.D., you know where you

905
00:39:05.860 --> 00:39:08.439
<v ANDREY>work, in your office, in the computer science

906
00:39:08.440 --> 00:39:10.779
<v ANDREY>building, you know, doing your research.

907
00:39:10.780 --> 00:39:13.689
<v ANDREY>And it's easy to forget sort of about the outside

908
00:39:13.690 --> 00:39:16.599
<v ANDREY>world. So I think it's interesting also

909
00:39:16.600 --> 00:39:19.899
<v ANDREY>that having had all this background outside

910
00:39:19.900 --> 00:39:23.019
<v ANDREY>working in actual organizations, I think

911
00:39:23.020 --> 00:39:24.159
<v ANDREY>I would imagine that also

912
00:39:25.660 --> 00:39:28.449
<v ANDREY>made it easier for you to get here.

913
00:39:28.450 --> 00:39:30.069
<v UPOL>Yeah, it was. And it's humbling, right?

914
00:39:30.070 --> 00:39:32.529
<v UPOL>Because you fail so many times trying to do this,

915
00:39:32.530 --> 00:39:34.569
<v UPOL>and that's the only way sometimes we learn.

916
00:39:34.570 --> 00:39:37.359
<v UPOL>Right? I, you know, my consulting projects,

917
00:39:37.360 --> 00:39:39.489
<v UPOL>I never like linear or straightforward because they

918
00:39:39.490 --> 00:39:41.619
<v UPOL>often reach out to me when problems are so

919
00:39:41.620 --> 00:39:44.619
<v UPOL>complicated that in-house teams need external

920
00:39:44.620 --> 00:39:45.620
<v UPOL>help.

921
00:39:46.060 --> 00:39:48.729
<v UPOL>And I think, you know, a lot of us then learn

922
00:39:48.730 --> 00:39:50.859
<v UPOL>the lesson that I have learned through all of this

923
00:39:50.860 --> 00:39:53.799
<v UPOL>is embracing a sense

924
00:39:53.800 --> 00:39:56.649
<v UPOL>of, you know, taking a learning

925
00:39:56.650 --> 00:39:59.349
<v UPOL>mentality from a lot of the there

926
00:39:59.350 --> 00:40:00.519
<v UPOL>is a famous paper.

927
00:40:00.520 --> 00:40:03.369
<v UPOL>I forget the name of the author who kind of framed

928
00:40:03.370 --> 00:40:07.059
<v UPOL>mistakes as mistakes

929
00:40:07.060 --> 00:40:09.849
<v UPOL>like, you know, in a movie, you take multiple takes

930
00:40:09.850 --> 00:40:11.739
<v UPOL>and not all the takes work.

931
00:40:11.740 --> 00:40:14.229
<v UPOL>So a lot of them are mistakes, right?

932
00:40:14.230 --> 00:40:17.889
<v UPOL>So I really embrace that mentality of mistakes.

933
00:40:17.890 --> 00:40:19.659
<v UPOL>Not all projects will work out.

934
00:40:19.660 --> 00:40:22.509
<v UPOL>You have a lot of mistakes, I guess.

935
00:40:22.510 --> 00:40:24.009
<v UPOL>Nothing is a mistake, per se.

936
00:40:24.010 --> 00:40:26.589
<v UPOL>And I think that really helped me have a more

937
00:40:26.590 --> 00:40:29.589
<v UPOL>iterative mindset, which has paid a lot of dividends

938
00:40:29.590 --> 00:40:31.749
<v UPOL>in getting a lot of this work done.

939
00:40:31.750 --> 00:40:34.569
<v ANDREY>Yeah, I think it's interesting how

940
00:40:34.570 --> 00:40:37.579
<v ANDREY>in some sense, especially doing it

941
00:40:37.580 --> 00:40:39.879
<v ANDREY>really enforces that.

942
00:40:39.880 --> 00:40:42.639
<v ANDREY>You really have to adapt to that because you

943
00:40:42.640 --> 00:40:45.279
<v ANDREY>got to have failures, but almost always

944
00:40:45.280 --> 00:40:47.859
<v ANDREY>will inform your understanding and ultimately guide

945
00:40:47.860 --> 00:40:50.040
<v ANDREY>you to something interesting, ideally, you know.

946
00:40:51.400 --> 00:40:54.099
<v ANDREY>Yeah. So as you did research

947
00:40:54.100 --> 00:40:56.829
<v ANDREY>that led to this

948
00:40:56.830 --> 00:40:59.739
<v ANDREY>paper, human centered explainable AI towards

949
00:40:59.740 --> 00:41:02.449
<v ANDREY>a reflective socio technical approach where

950
00:41:02.450 --> 00:41:05.349
<v ANDREY>you lay a lot of groundwork for how

951
00:41:05.350 --> 00:41:07.809
<v ANDREY>you can move towards that.

952
00:41:07.810 --> 00:41:10.419
<v ANDREY>And we we really can't get too much into

953
00:41:10.420 --> 00:41:12.939
<v ANDREY>it. It's it's quite detailed and self.

954
00:41:12.940 --> 00:41:16.419
<v ANDREY>But he did write this excellent piece on a gradient

955
00:41:16.420 --> 00:41:19.179
<v ANDREY>towards human centered, explainable A.I.

956
00:41:19.180 --> 00:41:21.249
<v ANDREY>every journey so far.

957
00:41:21.250 --> 00:41:23.589
<v ANDREY>So we're going to link to that in the description,

958
00:41:23.590 --> 00:41:26.419
<v ANDREY>and you can just fly a gradient and

959
00:41:26.420 --> 00:41:28.449
<v ANDREY>recommend you read that.

960
00:41:28.450 --> 00:41:30.729
<v ANDREY>But for now, we're going to actually focus on a more

961
00:41:30.730 --> 00:41:33.459
<v ANDREY>recent work expanding explainability

962
00:41:33.460 --> 00:41:36.579
<v ANDREY>towards social transparency

963
00:41:36.580 --> 00:41:37.810
<v ANDREY>in AI systems.

964
00:41:39.100 --> 00:41:41.739
<v ANDREY>So to get into it before even

965
00:41:41.740 --> 00:41:43.059
<v ANDREY>getting to any of the details,

966
00:41:44.350 --> 00:41:46.629
<v ANDREY>you know, what was your goal in starting this

967
00:41:46.630 --> 00:41:49.719
<v ANDREY>project and sort of a problem that motivated

968
00:41:49.720 --> 00:41:50.720
<v ANDREY>it?

969
00:41:51.130 --> 00:41:52.329
<v UPOL>This is.

970
00:41:52.330 --> 00:41:55.179
<v UPOL>Frankly, I feel like this was the paper

971
00:41:55.180 --> 00:41:57.999
<v UPOL>that, like the Human-Centered AI paper,

972
00:41:58.000 --> 00:42:00.459
<v UPOL>was the paper that needed to be written first for me

973
00:42:00.460 --> 00:42:02.349
<v UPOL>to actually write this paper.

974
00:42:02.350 --> 00:42:04.149
<v UPOL>And you know, a lot of the work in that

975
00:42:04.150 --> 00:42:06.939
<v UPOL>cybersecurity company kind of really

976
00:42:06.940 --> 00:42:08.169
<v UPOL>informed this.

977
00:42:08.170 --> 00:42:10.239
<v UPOL>So, you know, for the longest time, I have been kind

978
00:42:10.240 --> 00:42:13.089
<v UPOL>of arguing that we need to look outside

979
00:42:13.090 --> 00:42:14.090
<v UPOL>the box, right?

980
00:42:15.100 --> 00:42:17.709
<v UPOL>Yeah. So then, you know, largely speaking,

981
00:42:17.710 --> 00:42:19.839
<v UPOL>the community will come back and ask her to call.

982
00:42:19.840 --> 00:42:22.839
<v UPOL>I kind of get what you're trying to say, but what

983
00:42:22.840 --> 00:42:24.129
<v UPOL>outside? What is outside?

984
00:42:24.130 --> 00:42:25.859
<v UPOL>What do you want us to think about?

985
00:42:26.860 --> 00:42:29.529
<v UPOL>And the the kernel of this paper

986
00:42:29.530 --> 00:42:32.199
<v UPOL>is fundamentally and as as the title kind

987
00:42:32.200 --> 00:42:35.559
<v UPOL>of says, extending explainability, it expands

988
00:42:35.560 --> 00:42:38.569
<v UPOL>our conception of explainable

989
00:42:38.570 --> 00:42:41.529
<v UPOL>A.I. beyond the realms

990
00:42:41.530 --> 00:42:43.689
<v UPOL>of algorithmic transparency.

991
00:42:43.690 --> 00:42:45.069
<v UPOL>By doing what?

992
00:42:45.070 --> 00:42:47.679
<v UPOL>By adding this new

993
00:42:47.680 --> 00:42:49.689
<v UPOL>concept called social transparency, which is

994
00:42:49.690 --> 00:42:51.969
<v UPOL>actually like, not like New New in the sense that we

995
00:42:51.970 --> 00:42:54.629
<v UPOL>created it in the in the context of XXXII,

996
00:42:54.630 --> 00:42:56.649
<v UPOL>it's new. There is sort of transparency in online

997
00:42:56.650 --> 00:42:59.309
<v UPOL>systems back from the 90s.

998
00:42:59.310 --> 00:43:02.109
<v UPOL>And in the paper, we kind of pay homage to a lot

999
00:43:02.110 --> 00:43:05.019
<v UPOL>of those work, but it's fundamentally

1000
00:43:05.020 --> 00:43:07.749
<v UPOL>making the following observation.

1001
00:43:07.750 --> 00:43:10.389
<v UPOL>So within AI systems, and I think

1002
00:43:10.390 --> 00:43:13.059
<v UPOL>this is where it becomes very tricky when

1003
00:43:13.060 --> 00:43:15.729
<v UPOL>when we say AI systems is actually somewhat of

1004
00:43:15.730 --> 00:43:18.849
<v UPOL>a misnomer because when we say AI systems

1005
00:43:18.850 --> 00:43:21.549
<v UPOL>are a very important part is left out and it's often

1006
00:43:21.550 --> 00:43:24.219
<v UPOL>implicit, which is the human part.

1007
00:43:24.220 --> 00:43:26.889
<v UPOL>Implicit in AI systems are what

1008
00:43:26.890 --> 00:43:30.279
<v UPOL>we call human AI assemblages.

1009
00:43:30.280 --> 00:43:32.889
<v UPOL>Right? So these are two coupled points.

1010
00:43:32.890 --> 00:43:35.679
<v UPOL>So ideally, what you're really going for

1011
00:43:35.680 --> 00:43:38.469
<v UPOL>is the explainability of this assemblage, right?

1012
00:43:38.470 --> 00:43:41.379
<v UPOL>The human part of being often implicit.

1013
00:43:41.380 --> 00:43:44.319
<v UPOL>But but how can you

1014
00:43:44.320 --> 00:43:45.939
<v UPOL>get the explainability?

1015
00:43:45.940 --> 00:43:48.429
<v UPOL>All of these assemblage, these two part system, the

1016
00:43:48.430 --> 00:43:51.099
<v UPOL>human and the AI by just focusing

1017
00:43:51.100 --> 00:43:53.919
<v UPOL>on the air and asking the question that is asking

1018
00:43:53.920 --> 00:43:55.089
<v UPOL>in this paper?

1019
00:43:55.090 --> 00:43:56.679
<v UPOL>So then the question becomes IRA to Paul.

1020
00:43:56.680 --> 00:43:58.869
<v UPOL>I get it like, you know, you can add, know you need

1021
00:43:58.870 --> 00:44:01.569
<v UPOL>the human part, but what about it in looking very

1022
00:44:01.570 --> 00:44:04.179
<v UPOL>Typekit? So that's where if you

1023
00:44:04.180 --> 00:44:06.789
<v UPOL>add the transparency on the human side, we kind

1024
00:44:06.790 --> 00:44:09.579
<v UPOL>of introduce this notion of social transparency in

1025
00:44:09.580 --> 00:44:11.609
<v UPOL>AI systems, right?

1026
00:44:11.610 --> 00:44:14.349
<v UPOL>Operationalize a little bit of this in the paper.

1027
00:44:14.350 --> 00:44:17.319
<v ANDREY>Right. So yeah, it's I think very

1028
00:44:17.320 --> 00:44:20.229
<v ANDREY>interesting about this in terms of operationalizing

1029
00:44:20.230 --> 00:44:22.989
<v ANDREY>that. Not only do you highlight this need, which

1030
00:44:22.990 --> 00:44:25.739
<v ANDREY>I think is very intuitive, but actually exquisite

1031
00:44:25.740 --> 00:44:28.389
<v ANDREY>talk about how to do this, how to be useful and

1032
00:44:28.390 --> 00:44:31.449
<v ANDREY>really beyond technical transparency and

1033
00:44:31.450 --> 00:44:33.279
<v ANDREY>how to integrate that.

1034
00:44:33.280 --> 00:44:35.919
<v ANDREY>And actually, you know, where do

1035
00:44:35.920 --> 00:44:38.529
<v ANDREY>you start? How do you how do you do it and

1036
00:44:38.530 --> 00:44:39.799
<v ANDREY>so on, right?

1037
00:44:39.800 --> 00:44:40.800
<v ANDREY>Hmm.

1038
00:44:41.560 --> 00:44:44.229
<v ANDREY>So I guess maybe can

1039
00:44:44.230 --> 00:44:46.749
<v ANDREY>we dove in a bit more about this idea of social

1040
00:44:46.750 --> 00:44:49.309
<v ANDREY>transparency? Ethics is so important.

1041
00:44:49.310 --> 00:44:51.939
<v ANDREY>And so we know because fancy sort of trying

1042
00:44:51.940 --> 00:44:55.929
<v ANDREY>to understand what the algorithmic

1043
00:44:55.930 --> 00:44:58.989
<v ANDREY>side of it is doing, what the model is thinking, but

1044
00:44:58.990 --> 00:45:01.689
<v ANDREY>what is social rationality one

1045
00:45:01.690 --> 00:45:02.889
<v ANDREY>of its components?

1046
00:45:02.890 --> 00:45:06.109
<v ANDREY>And yeah, how should people understand it?

1047
00:45:06.110 --> 00:45:08.349
<v UPOL>Yeah. Well, that's a that's a fascinating question.

1048
00:45:09.370 --> 00:45:12.789
<v UPOL>So to understand social transparency,

1049
00:45:12.790 --> 00:45:15.579
<v UPOL>I think we have to accept a few things.

1050
00:45:15.580 --> 00:45:18.189
<v UPOL>First, we have to understand and

1051
00:45:18.190 --> 00:45:20.799
<v UPOL>acknowledge that work

1052
00:45:20.800 --> 00:45:23.079
<v UPOL>is social, right?

1053
00:45:23.080 --> 00:45:24.909
<v UPOL>You know, we don't work in silos.

1054
00:45:24.910 --> 00:45:27.759
<v UPOL>Most of us, we work within teams.

1055
00:45:27.760 --> 00:45:29.529
<v UPOL>So there meet. That means right.

1056
00:45:29.530 --> 00:45:32.649
<v UPOL>There is some need to add this transparency

1057
00:45:32.650 --> 00:45:35.349
<v UPOL>data in an office when you're working with a team

1058
00:45:35.350 --> 00:45:37.699
<v UPOL>or virtually through Slack, right?

1059
00:45:37.700 --> 00:45:39.669
<v UPOL>There's a lot of chatter that is going on and there

1060
00:45:39.670 --> 00:45:41.529
<v UPOL>is a necessity behind that.

1061
00:45:41.530 --> 00:45:44.949
<v UPOL>So that is fast the the fast realization

1062
00:45:44.950 --> 00:45:46.449
<v UPOL>that work is social.

1063
00:45:46.450 --> 00:45:49.239
<v UPOL>That means there might be a need to make that social

1064
00:45:49.240 --> 00:45:51.669
<v UPOL>nature a little bit transparent, especially when

1065
00:45:51.670 --> 00:45:53.709
<v UPOL>we're dealing with A.I. mediated decision support

1066
00:45:53.710 --> 00:45:56.349
<v UPOL>systems. So and you know,

1067
00:45:56.350 --> 00:45:59.049
<v UPOL>as we share in the paper, we were trying

1068
00:45:59.050 --> 00:46:01.719
<v UPOL>to. So this is also difficult as well

1069
00:46:01.720 --> 00:46:03.669
<v UPOL>to some extent, right? One of the challenges that

1070
00:46:03.670 --> 00:46:06.369
<v UPOL>A.I. researchers face is how

1071
00:46:06.370 --> 00:46:09.139
<v UPOL>do we know what the future really looks like

1072
00:46:10.150 --> 00:46:12.879
<v UPOL>without really investing months

1073
00:46:12.880 --> 00:46:15.729
<v UPOL>and months of work, building large

1074
00:46:15.730 --> 00:46:17.919
<v UPOL>infrastructures and models and then realizing we're

1075
00:46:17.920 --> 00:46:20.709
<v UPOL>actually not very useful, but that that

1076
00:46:20.710 --> 00:46:23.679
<v UPOL>is a very hard cost of

1077
00:46:23.680 --> 00:46:26.709
<v UPOL>doing this. So due to kind of explain

1078
00:46:26.710 --> 00:46:29.769
<v UPOL>that we used this notion

1079
00:46:29.770 --> 00:46:31.119
<v UPOL>of scenario based design.

1080
00:46:31.120 --> 00:46:34.119
<v UPOL>So this is coming from the traditions of design

1081
00:46:34.120 --> 00:46:37.329
<v UPOL>fiction, as I'm drawing a lot of this actually

1082
00:46:37.330 --> 00:46:39.999
<v UPOL>from the theoretical underpinnings

1083
00:46:40.000 --> 00:46:42.219
<v UPOL>of the human centered explainable AI paper that we

1084
00:46:42.220 --> 00:46:43.899
<v UPOL>just talked about. Mm-Hmm.

1085
00:46:43.900 --> 00:46:46.749
<v UPOL>And so using scenario based design, we

1086
00:46:46.750 --> 00:46:50.139
<v UPOL>conducted around four workshops

1087
00:46:50.140 --> 00:46:51.339
<v UPOL>with a lot of people.

1088
00:46:51.340 --> 00:46:53.499
<v UPOL>From different technology companies, just to get a

1089
00:46:53.500 --> 00:46:56.649
<v UPOL>sense of what are the things

1090
00:46:56.650 --> 00:46:59.679
<v UPOL>that are outside the black box that people

1091
00:46:59.680 --> 00:47:02.739
<v UPOL>want when they make a decision

1092
00:47:02.740 --> 00:47:04.249
<v UPOL>within the AI system.

1093
00:47:04.250 --> 00:47:06.429
<v UPOL>Right. So that's the workshop is meant to kind of

1094
00:47:06.430 --> 00:47:09.069
<v UPOL>get a more formative understanding, right?

1095
00:47:09.070 --> 00:47:11.799
<v ANDREY>What needs to be made transparent in this social

1096
00:47:11.800 --> 00:47:13.929
<v ANDREY>system in terms of

1097
00:47:13.930 --> 00:47:16.179
<v UPOL>right, because there are so many things you can make

1098
00:47:16.180 --> 00:47:18.609
<v UPOL>transparent, right? Because how do you know which

1099
00:47:18.610 --> 00:47:20.499
<v UPOL>one is the right thing to do right?

1100
00:47:20.500 --> 00:47:23.259
<v UPOL>And I think through these workshops and this

1101
00:47:23.260 --> 00:47:25.989
<v UPOL>is pre-COVID, so we have the ability

1102
00:47:25.990 --> 00:47:27.759
<v UPOL>to kind of get in person and kind of have these

1103
00:47:27.760 --> 00:47:28.989
<v UPOL>workshops.

1104
00:47:28.990 --> 00:47:31.779
<v UPOL>And what we learn was that

1105
00:47:31.780 --> 00:47:34.629
<v UPOL>out of this and this is, I think, what we what we

1106
00:47:34.630 --> 00:47:37.599
<v UPOL>call in the paper, the four ws, right?

1107
00:47:37.600 --> 00:47:40.869
<v UPOL>So in addition to

1108
00:47:40.870 --> 00:47:43.989
<v UPOL>the i's technical transparency or algorithmic

1109
00:47:43.990 --> 00:47:47.169
<v UPOL>transparency, these practitioners,

1110
00:47:47.170 --> 00:47:50.349
<v UPOL>data scientists, analysts and others

1111
00:47:50.350 --> 00:47:53.079
<v UPOL>wanted to know for things

1112
00:47:53.080 --> 00:47:56.409
<v UPOL>who did what, when

1113
00:47:56.410 --> 00:47:57.410
<v UPOL>and why.

1114
00:47:58.180 --> 00:48:00.879
<v UPOL>So those became again, we are not saying this

1115
00:48:00.880 --> 00:48:03.159
<v UPOL>is the end all, be all to all social transparency.

1116
00:48:03.160 --> 00:48:05.799
<v UPOL>There might be other socially transparent systems

1117
00:48:05.800 --> 00:48:08.109
<v UPOL>that that do very well, but actually in the

1118
00:48:08.110 --> 00:48:10.899
<v UPOL>cybersecurity example, going back to that

1119
00:48:10.900 --> 00:48:13.659
<v UPOL>when we implemented this aspect

1120
00:48:13.660 --> 00:48:15.219
<v UPOL>of who did what, when and why.

1121
00:48:15.220 --> 00:48:18.549
<v UPOL>So imagine, like, you know, next to a threat,

1122
00:48:18.550 --> 00:48:20.259
<v UPOL>you know, close these ports, right?

1123
00:48:20.260 --> 00:48:22.599
<v UPOL>Let's imagine that if Julie have social

1124
00:48:22.600 --> 00:48:24.699
<v UPOL>transparency, what would do we have seen?

1125
00:48:24.700 --> 00:48:26.649
<v UPOL>Julie, who got fired before?

1126
00:48:26.650 --> 00:48:29.169
<v UPOL>So when you get this

1127
00:48:30.580 --> 00:48:32.979
<v UPOL>new disease, the air is recommending the ports to be

1128
00:48:32.980 --> 00:48:35.439
<v UPOL>closed and you're like, OK.

1129
00:48:35.440 --> 00:48:37.539
<v UPOL>Is that true? Like, is that real or not?

1130
00:48:37.540 --> 00:48:39.669
<v UPOL>I don't know if it's a false positive.

1131
00:48:39.670 --> 00:48:42.429
<v UPOL>But then Julie is able to see, you know, maybe

1132
00:48:42.430 --> 00:48:44.799
<v UPOL>10 other people dealing with a very similar

1133
00:48:44.800 --> 00:48:46.629
<v UPOL>situation in the past.

1134
00:48:46.630 --> 00:48:49.359
<v UPOL>And in one of those Julie scenes, I one

1135
00:48:49.360 --> 00:48:51.939
<v UPOL>of the who's right? Maybe imagine this is Bob, and

1136
00:48:51.940 --> 00:48:54.639
<v UPOL>Bob is a veteran in the industry.

1137
00:48:54.640 --> 00:48:57.669
<v UPOL>He's like a level three analyst, and

1138
00:48:57.670 --> 00:49:00.549
<v UPOL>he says, Oh, these are backup

1139
00:49:00.550 --> 00:49:02.319
<v UPOL>site reports in law.

1140
00:49:02.320 --> 00:49:03.459
<v UPOL>Mm. Right?

1141
00:49:03.460 --> 00:49:05.529
<v UPOL>So who did what?

1142
00:49:05.530 --> 00:49:06.530
<v UPOL>Right?

1143
00:49:06.940 --> 00:49:09.669
<v UPOL>When? Maybe, let's say, three months ago?

1144
00:49:09.670 --> 00:49:11.799
<v UPOL>And why? So the why is the reasoning right?

1145
00:49:11.800 --> 00:49:13.389
<v UPOL>Like these are backup center reports

1146
00:49:14.830 --> 00:49:17.529
<v UPOL>ignored by situating this extra

1147
00:49:17.530 --> 00:49:20.119
<v UPOL>piece of information that is actually capturing?

1148
00:49:20.120 --> 00:49:22.239
<v UPOL>You know, one might argue, Hey, people like that

1149
00:49:22.240 --> 00:49:24.099
<v UPOL>seems like a bad problem. They should've just added

1150
00:49:24.100 --> 00:49:26.529
<v UPOL>back to the data center, right?

1151
00:49:26.530 --> 00:49:29.049
<v UPOL>That's not good. And that is where I think the

1152
00:49:29.050 --> 00:49:30.369
<v UPOL>critical insight lies.

1153
00:49:30.370 --> 00:49:32.979
<v UPOL>There is not enough things you

1154
00:49:32.980 --> 00:49:34.539
<v UPOL>can add in the dataset.

1155
00:49:34.540 --> 00:49:37.149
<v UPOL>It's like a golden goose chase

1156
00:49:37.150 --> 00:49:39.129
<v ANDREY>because it's all inside the model, right?

1157
00:49:39.130 --> 00:49:42.129
<v UPOL>Exactly. And sometimes things happen dynamically.

1158
00:49:42.130 --> 00:49:44.949
<v UPOL>Remember, data sets are basically snapshots

1159
00:49:44.950 --> 00:49:45.950
<v UPOL>of the past.

1160
00:49:46.700 --> 00:49:49.489
<v UPOL>And work norms actually change over

1161
00:49:49.490 --> 00:49:52.159
<v UPOL>time due to the sensitive

1162
00:49:52.160 --> 00:49:54.439
<v UPOL>nature of certain cybersecurity explanation

1163
00:49:54.440 --> 00:49:57.289
<v UPOL>institutions. You do not want certain

1164
00:49:57.290 --> 00:49:59.109
<v UPOL>things to be coded into a data set.

1165
00:49:59.110 --> 00:50:01.549
<v UPOL>Right. Because what if that gets hacked, then all

1166
00:50:01.550 --> 00:50:02.929
<v UPOL>your secrets are out.

1167
00:50:02.930 --> 00:50:06.439
<v UPOL>So there will always be elements

1168
00:50:06.440 --> 00:50:09.889
<v UPOL>that are not quantifiable, that are not acceptable

1169
00:50:09.890 --> 00:50:12.619
<v UPOL>in a cleanly named dataset.

1170
00:50:12.620 --> 00:50:15.289
<v UPOL>In those cases, those very things that are

1171
00:50:15.290 --> 00:50:18.259
<v UPOL>hard to quantify, hard to incorporate often

1172
00:50:18.260 --> 00:50:20.959
<v UPOL>can be the difference maker between right and

1173
00:50:20.960 --> 00:50:23.329
<v UPOL>wrong decisions where they are.

1174
00:50:23.330 --> 00:50:26.299
<v UPOL>So by adding this social transparency, you are able

1175
00:50:26.300 --> 00:50:29.089
<v UPOL>to inform someone to

1176
00:50:29.090 --> 00:50:31.429
<v UPOL>know when to trust the A.I.

1177
00:50:31.430 --> 00:50:32.430
<v UPOL>versus not.

1178
00:50:33.130 --> 00:50:35.829
<v ANDREY>Mm hmm. Yeah, exactly, and

1179
00:50:35.830 --> 00:50:37.509
<v ANDREY>to dig a bit deeper.

1180
00:50:37.510 --> 00:50:40.479
<v ANDREY>I would love to hear how did this scenario

1181
00:50:40.480 --> 00:50:43.249
<v ANDREY>based design process

1182
00:50:43.250 --> 00:50:45.909
<v ANDREY>work? I think figure one of your work is

1183
00:50:45.910 --> 00:50:48.220
<v ANDREY>really interesting is that the scenario used.

1184
00:50:50.020 --> 00:50:52.929
<v UPOL>So you know, in this scenario, we

1185
00:50:52.930 --> 00:50:56.649
<v UPOL>asked our participants to kind of envision being in

1186
00:50:56.650 --> 00:50:59.889
<v UPOL>using a AI powered pricing tool

1187
00:50:59.890 --> 00:51:02.529
<v UPOL>to price and access management

1188
00:51:02.530 --> 00:51:05.419
<v UPOL>product to a customer called Scout.

1189
00:51:05.420 --> 00:51:06.849
<v UPOL>Right. So the A.I.

1190
00:51:06.850 --> 00:51:09.219
<v UPOL>kind of does its analysis and recommends that, hey,

1191
00:51:09.220 --> 00:51:11.499
<v UPOL>you got to sell it at 100 bucks per month per

1192
00:51:11.500 --> 00:51:12.939
<v UPOL>account.

1193
00:51:12.940 --> 00:51:16.299
<v UPOL>And it did also share some post-rock explanations

1194
00:51:16.300 --> 00:51:19.029
<v UPOL>and kind of justifies, White said, what it's saying

1195
00:51:19.030 --> 00:51:21.339
<v UPOL>and that the model, the technical transparency

1196
00:51:21.340 --> 00:51:23.559
<v UPOL>pieces like the item, the quotable goes off a

1197
00:51:23.560 --> 00:51:25.419
<v UPOL>salesperson into account.

1198
00:51:25.420 --> 00:51:27.459
<v UPOL>It did a comparative pricing of what similar

1199
00:51:27.460 --> 00:51:30.249
<v UPOL>customers pray, and also it gave you the floor.

1200
00:51:30.250 --> 00:51:33.249
<v UPOL>So what is the cost price for doing this product?

1201
00:51:33.250 --> 00:51:35.439
<v UPOL>So these are so imagine that's the first letter and

1202
00:51:35.440 --> 00:51:37.209
<v UPOL>today, right?

1203
00:51:37.210 --> 00:51:38.769
<v UPOL>That's the state of the art.

1204
00:51:38.770 --> 00:51:40.779
<v UPOL>Nothing is better than that, right?

1205
00:51:40.780 --> 00:51:43.299
<v UPOL>We don't have the social transparency that we kind

1206
00:51:43.300 --> 00:51:46.149
<v UPOL>of envision in this paper, but that

1207
00:51:46.150 --> 00:51:47.859
<v UPOL>is where the state of the art was.

1208
00:51:47.860 --> 00:51:49.569
<v UPOL>So that was our grounding moment.

1209
00:51:49.570 --> 00:51:51.879
<v UPOL>So we would ask our people as they went through the

1210
00:51:51.880 --> 00:51:54.399
<v UPOL>walk to what would you do right now?

1211
00:51:54.400 --> 00:51:55.929
<v UPOL>Do you think this is, you know, before we showed

1212
00:51:55.930 --> 00:51:57.329
<v UPOL>them any social transparency?

1213
00:51:57.330 --> 00:52:00.249
<v UPOL>Right? And we will see that most people

1214
00:52:00.250 --> 00:52:01.449
<v UPOL>agree with that.

1215
00:52:01.450 --> 00:52:03.099
<v UPOL>Yeah, this seems like a decent. We also kind of

1216
00:52:03.100 --> 00:52:06.009
<v UPOL>calibrated the price point by asking experts.

1217
00:52:06.010 --> 00:52:08.139
<v UPOL>So we kind of grounded a lot of his data, even

1218
00:52:08.140 --> 00:52:10.449
<v UPOL>though it's a scenario. If it's fictional, the

1219
00:52:10.450 --> 00:52:12.489
<v UPOL>fiction is grounded in reality, our version of

1220
00:52:12.490 --> 00:52:13.629
<v UPOL>reality.

1221
00:52:13.630 --> 00:52:16.419
<v UPOL>And then we told them, like, now imagine

1222
00:52:16.420 --> 00:52:17.629
<v UPOL>what have you found out?

1223
00:52:17.630 --> 00:52:20.229
<v UPOL>And that only one out of 10 people

1224
00:52:20.230 --> 00:52:22.989
<v UPOL>sold this product and the recommended price?

1225
00:52:22.990 --> 00:52:24.759
<v UPOL>What would you do?

1226
00:52:24.760 --> 00:52:26.799
<v UPOL>And you could see our participants kind of get very

1227
00:52:26.800 --> 00:52:28.329
<v UPOL>interested, like those like, oh, that's really

1228
00:52:28.330 --> 00:52:31.659
<v UPOL>interesting information that

1229
00:52:31.660 --> 00:52:34.029
<v UPOL>helps me calibrate what to do.

1230
00:52:34.030 --> 00:52:36.219
<v UPOL>So then we kind of dug deeper, which is like the

1231
00:52:36.220 --> 00:52:39.009
<v UPOL>bullet points three four five to share

1232
00:52:39.010 --> 00:52:41.739
<v UPOL>three examples of past colleagues

1233
00:52:41.740 --> 00:52:44.499
<v UPOL>who have dealt with the same customer

1234
00:52:44.500 --> 00:52:47.079
<v UPOL>scout on similar products.

1235
00:52:47.080 --> 00:52:50.109
<v UPOL>And then one of the most important comments

1236
00:52:50.110 --> 00:52:52.809
<v UPOL>were made by Jessica or Jess, who's

1237
00:52:52.810 --> 00:52:54.459
<v UPOL>a sales director.

1238
00:52:54.460 --> 00:52:57.009
<v UPOL>And it turns out Jess had rejected the

1239
00:52:57.010 --> 00:53:00.309
<v UPOL>recommendation, but the sale did happen,

1240
00:53:00.310 --> 00:53:02.439
<v UPOL>and the comment was the most important where they

1241
00:53:02.440 --> 00:53:04.599
<v UPOL>say that, Hey, is COVID 19.

1242
00:53:04.600 --> 00:53:06.310
<v UPOL>And this was done at the height of the pandemic.

1243
00:53:08.080 --> 00:53:11.049
<v UPOL>I can't lose a long term, profitable customer.

1244
00:53:11.050 --> 00:53:13.719
<v UPOL>So they offered 10 percent below the

1245
00:53:13.720 --> 00:53:16.599
<v UPOL>cost price. And that's an important part, right?

1246
00:53:16.600 --> 00:53:19.439
<v UPOL>That not only did they give you a discount, but

1247
00:53:19.440 --> 00:53:22.119
<v UPOL>just the director had given them below the

1248
00:53:22.120 --> 00:53:25.209
<v UPOL>cost price and that social

1249
00:53:25.210 --> 00:53:27.849
<v UPOL>context of what was going on that was outside of

1250
00:53:27.850 --> 00:53:29.169
<v UPOL>the algorithm, right?

1251
00:53:29.170 --> 00:53:32.139
<v UPOL>Very much inform how people acted on it

1252
00:53:32.140 --> 00:53:34.959
<v UPOL>because remember, without any of this context, they

1253
00:53:34.960 --> 00:53:36.339
<v UPOL>fell. The price was fair.

1254
00:53:36.340 --> 00:53:37.719
<v UPOL>It was done the right way.

1255
00:53:37.720 --> 00:53:40.359
<v UPOL>You know, the justifications were right,

1256
00:53:40.360 --> 00:53:44.379
<v UPOL>but very few people actually,

1257
00:53:44.380 --> 00:53:47.289
<v UPOL>you know, offered the same price when

1258
00:53:47.290 --> 00:53:49.449
<v UPOL>they knew what others had done, especially when a

1259
00:53:49.450 --> 00:53:52.659
<v UPOL>director level person had done it before.

1260
00:53:52.660 --> 00:53:55.539
<v ANDREY>Yes. So in a sense, I think going

1261
00:53:55.540 --> 00:53:57.789
<v ANDREY>back to something you mentioned, it's letting you

1262
00:53:57.790 --> 00:54:00.219
<v ANDREY>know what the model doesn't know.

1263
00:54:00.220 --> 00:54:03.039
<v ANDREY>Right? It doesn't know about COVID

1264
00:54:03.040 --> 00:54:04.869
<v ANDREY>and these four W's.

1265
00:54:04.870 --> 00:54:06.909
<v ANDREY>The social scenario doesn't really explain the

1266
00:54:06.910 --> 00:54:09.549
<v ANDREY>mottoes decisions, but it

1267
00:54:09.550 --> 00:54:12.299
<v ANDREY>does like to understand via

1268
00:54:12.300 --> 00:54:15.039
<v ANDREY>a system better in the sense of like

1269
00:54:15.040 --> 00:54:18.969
<v ANDREY>the AI system is situated within the organization.

1270
00:54:18.970 --> 00:54:21.819
<v ANDREY>And so you get to know more its weaknesses

1271
00:54:21.820 --> 00:54:23.439
<v ANDREY>and where and when to follow it.

1272
00:54:23.440 --> 00:54:26.079
<v ANDREY>Maybe when not, which I

1273
00:54:26.080 --> 00:54:28.689
<v ANDREY>think would be a lot harder about seeing like,

1274
00:54:28.690 --> 00:54:30.729
<v ANDREY>OK, this first person accepted discrimination.

1275
00:54:30.730 --> 00:54:32.110
<v ANDREY>This person didn't.

1276
00:54:33.460 --> 00:54:35.919
<v ANDREY>And this figure, I think, illustrates that really

1277
00:54:35.920 --> 00:54:36.819
<v ANDREY>well.

1278
00:54:36.820 --> 00:54:39.189
<v UPOL>And I think it's kind of asking the question like,

1279
00:54:39.190 --> 00:54:42.129
<v UPOL>what are the eyes blind spots and can

1280
00:54:42.130 --> 00:54:44.439
<v UPOL>other humans who have interacted with this system in

1281
00:54:44.440 --> 00:54:46.239
<v UPOL>the past and address it?

1282
00:54:46.240 --> 00:54:49.029
<v UPOL>So like, for instance, I am now currently working

1283
00:54:49.030 --> 00:54:51.849
<v UPOL>with radiation oncologists on a very similar

1284
00:54:51.850 --> 00:54:54.610
<v UPOL>project and in radiation oncology,

1285
00:54:55.990 --> 00:54:59.259
<v UPOL>just like in other fields, there is no

1286
00:54:59.260 --> 00:55:00.579
<v UPOL>absolute ground truth.

1287
00:55:00.580 --> 00:55:02.709
<v UPOL>With 80 senators, we are so comfortable with the

1288
00:55:02.710 --> 00:55:05.109
<v UPOL>terminology of ground truth, right?

1289
00:55:05.110 --> 00:55:07.989
<v UPOL>But when it comes to using radiation to treat

1290
00:55:07.990 --> 00:55:10.539
<v UPOL>cancers, they're established practices.

1291
00:55:10.540 --> 00:55:13.479
<v UPOL>There is no like absolute gold thing that everyone

1292
00:55:13.480 --> 00:55:16.869
<v UPOL>must do because each patient is different.

1293
00:55:16.870 --> 00:55:19.239
<v UPOL>Each treatment facility is different.

1294
00:55:19.240 --> 00:55:20.590
<v UPOL>So in that case?

1295
00:55:21.840 --> 00:55:24.989
<v UPOL>Knowing when to trust these recommendations,

1296
00:55:24.990 --> 00:55:27.269
<v UPOL>foreign by saying, you know, give this much

1297
00:55:27.270 --> 00:55:30.179
<v UPOL>radiation to the patient's left optic nerve.

1298
00:55:30.180 --> 00:55:32.819
<v UPOL>Right. That's a very high stakes decision,

1299
00:55:32.820 --> 00:55:34.679
<v UPOL>right? Because if you do it the wrong way, you can

1300
00:55:34.680 --> 00:55:36.839
<v UPOL>blast away my left optic there and take away my

1301
00:55:36.840 --> 00:55:38.369
<v UPOL>vision. Right?

1302
00:55:38.370 --> 00:55:40.079
<v UPOL>But guess what? What the A.I.

1303
00:55:40.080 --> 00:55:42.689
<v UPOL>system might not have known is that

1304
00:55:42.690 --> 00:55:45.059
<v UPOL>the patient is blind in the right die.

1305
00:55:45.060 --> 00:55:47.669
<v UPOL>So all of the calculus goes away because we

1306
00:55:47.670 --> 00:55:49.649
<v UPOL>know there's no like central blindness data in

1307
00:55:49.650 --> 00:55:51.519
<v UPOL>randomized controlled trials.

1308
00:55:51.520 --> 00:55:54.269
<v UPOL>Right? So just knowing that extra

1309
00:55:54.270 --> 00:55:56.969
<v UPOL>piece can help you calibrate how much

1310
00:55:56.970 --> 00:55:59.579
<v UPOL>treatment you want to give it and knowing what

1311
00:55:59.580 --> 00:56:01.889
<v UPOL>your peers done right, because in these kind of

1312
00:56:01.890 --> 00:56:04.649
<v UPOL>communities of practices is very much

1313
00:56:04.650 --> 00:56:06.119
<v UPOL>community driven, right?

1314
00:56:06.120 --> 00:56:08.759
<v UPOL>Like the radiation oncologist kind of

1315
00:56:08.760 --> 00:56:11.489
<v UPOL>have these standards that they co-developed

1316
00:56:11.490 --> 00:56:13.199
<v UPOL>together are two studies.

1317
00:56:13.200 --> 00:56:15.899
<v UPOL>So this social transparency starts mattering

1318
00:56:15.900 --> 00:56:18.839
<v UPOL>extremely when the cost of failure

1319
00:56:18.840 --> 00:56:21.179
<v UPOL>is also very high, right?

1320
00:56:21.180 --> 00:56:23.759
<v UPOL>Like, you know, blasting someone's optic nerve nerve

1321
00:56:23.760 --> 00:56:26.849
<v UPOL>out there, the radiation is a pretty high cost

1322
00:56:26.850 --> 00:56:28.529
<v UPOL>rather than, you know, missing a song

1323
00:56:28.530 --> 00:56:29.969
<v UPOL>recommendation.

1324
00:56:29.970 --> 00:56:31.739
<v UPOL>And I think that's the other part like you don't

1325
00:56:31.740 --> 00:56:34.049
<v UPOL>think I don't. Social transparency is not really

1326
00:56:34.050 --> 00:56:36.839
<v UPOL>helpful when the stakes are low

1327
00:56:36.840 --> 00:56:38.699
<v UPOL>or when the nature of the job is not very

1328
00:56:38.700 --> 00:56:40.529
<v UPOL>collaborative, right?

1329
00:56:40.530 --> 00:56:42.989
<v UPOL>But the more the stakes are high, the more

1330
00:56:42.990 --> 00:56:44.429
<v UPOL>collaboration is needed.

1331
00:56:44.430 --> 00:56:47.129
<v UPOL>Social transparency becomes important because what

1332
00:56:47.130 --> 00:56:48.389
<v UPOL>then becomes very social.

1333
00:56:49.770 --> 00:56:52.469
<v ANDREY>Yeah, it's just makes me feel like you can

1334
00:56:52.470 --> 00:56:55.139
<v ANDREY>almost consider

1335
00:56:55.140 --> 00:56:57.929
<v ANDREY>this like what if the AI

1336
00:56:57.930 --> 00:57:00.719
<v ANDREY>model is in some sense, a coworker,

1337
00:57:00.720 --> 00:57:03.449
<v ANDREY>right? When you work with people, some

1338
00:57:03.450 --> 00:57:06.299
<v ANDREY>people you trust more and less and when you do

1339
00:57:06.300 --> 00:57:08.459
<v ANDREY>decision making, it is sort of collaborative.

1340
00:57:08.460 --> 00:57:10.859
<v ANDREY>You know, you might debate, you might ask whatever

1341
00:57:10.860 --> 00:57:13.379
<v ANDREY>you consider, address, if you consider that that's

1342
00:57:13.380 --> 00:57:16.079
<v ANDREY>not something that you can do with any AI

1343
00:57:16.080 --> 00:57:18.629
<v ANDREY>system, at least for now, you can't say, well, you

1344
00:57:18.630 --> 00:57:21.449
<v ANDREY>know, have you taken this into that into account?

1345
00:57:21.450 --> 00:57:24.059
<v ANDREY>But seeing the social context,

1346
00:57:24.060 --> 00:57:27.449
<v ANDREY>it seems to me, was what people might

1347
00:57:27.450 --> 00:57:29.459
<v ANDREY>have realized that to me, this comment and now, you

1348
00:57:29.460 --> 00:57:32.039
<v ANDREY>know, didn't take into account the COVID thing.

1349
00:57:33.390 --> 00:57:35.459
<v ANDREY>So, yeah, she gets interesting in the sense of like

1350
00:57:35.460 --> 00:57:38.879
<v ANDREY>you get to know the system

1351
00:57:38.880 --> 00:57:41.609
<v ANDREY>as another entity you work with

1352
00:57:41.610 --> 00:57:43.049
<v ANDREY>almost.

1353
00:57:43.050 --> 00:57:44.279
<v UPOL>Yeah, yeah, exactly.

1354
00:57:44.280 --> 00:57:46.559
<v UPOL>And I think that's kind of changes the way we think

1355
00:57:46.560 --> 00:57:49.439
<v UPOL>of human collaboration, right?

1356
00:57:49.440 --> 00:57:52.049
<v UPOL>Because you are now like, I often think about it,

1357
00:57:52.050 --> 00:57:54.479
<v UPOL>it's like, you know, Avatar The Last Airbender.

1358
00:57:54.480 --> 00:57:55.179
<v UPOL>I don't know.

1359
00:57:55.180 --> 00:57:55.769
<v ANDREY>Yeah, yeah.

1360
00:57:55.770 --> 00:57:57.329
<v UPOL>So it's like, what does Avatar do?

1361
00:57:57.330 --> 00:57:59.999
<v UPOL>And new faces around there, like Avatar and right?

1362
00:58:00.000 --> 00:58:02.159
<v UPOL>Like when he faces some difficult choices, he kind

1363
00:58:02.160 --> 00:58:04.799
<v UPOL>of seeks the counsel of

1364
00:58:04.800 --> 00:58:07.619
<v UPOL>past avatars who had come before

1365
00:58:07.620 --> 00:58:10.259
<v UPOL>him. And so the social transparency

1366
00:58:10.260 --> 00:58:13.199
<v UPOL>is in a weird way of capturing that historical

1367
00:58:13.200 --> 00:58:16.349
<v UPOL>context in a system

1368
00:58:16.350 --> 00:58:19.349
<v UPOL>in situ that really makes your decision

1369
00:58:19.350 --> 00:58:22.529
<v UPOL>making in that moment much more informed.

1370
00:58:22.530 --> 00:58:23.969
<v UPOL>Because at the end of the day, we have to ask

1371
00:58:23.970 --> 00:58:27.029
<v UPOL>ourselves why these explanations aren't there.

1372
00:58:27.030 --> 00:58:30.509
<v UPOL>They're there to make things actionable.

1373
00:58:30.510 --> 00:58:32.759
<v UPOL>If you if someone cannot do something without

1374
00:58:32.760 --> 00:58:35.339
<v UPOL>explanation, then there might not be any

1375
00:58:35.340 --> 00:58:37.149
<v UPOL>explanation, right?

1376
00:58:37.150 --> 00:58:39.089
<v UPOL>Like if the machine is explaining itself and I

1377
00:58:39.090 --> 00:58:41.609
<v UPOL>cannot do anything with it, that's very difficult.

1378
00:58:41.610 --> 00:58:44.309
<v UPOL>Like, I don't know what purpose of serving

1379
00:58:44.310 --> 00:58:45.719
<v UPOL>other than just understanding.

1380
00:58:45.720 --> 00:58:47.729
<v UPOL>But if I understand and cannot do anything with

1381
00:58:47.730 --> 00:58:50.069
<v UPOL>understanding, what is it there for anyway?

1382
00:58:50.070 --> 00:58:52.919
<v UPOL>So social transparency can make things

1383
00:58:52.920 --> 00:58:55.979
<v UPOL>more actionable, even if right, even

1384
00:58:55.980 --> 00:58:57.199
<v UPOL>if.

1385
00:58:57.200 --> 00:59:00.169
<v UPOL>The participants are saying no

1386
00:59:00.170 --> 00:59:02.509
<v UPOL>to the system, and that's the crucial part.

1387
00:59:02.510 --> 00:59:05.119
<v UPOL>I think it changes how we formulate

1388
00:59:05.120 --> 00:59:07.759
<v UPOL>trust because a lot of the work around

1389
00:59:07.760 --> 00:59:10.369
<v UPOL>trust that we see is around user

1390
00:59:10.370 --> 00:59:13.019
<v UPOL>acceptance. I want my user to like the

1391
00:59:13.020 --> 00:59:15.409
<v UPOL>I want my user to accept me.

1392
00:59:15.410 --> 00:59:17.629
<v UPOL>But I think what we are seeing is it's not about

1393
00:59:17.630 --> 00:59:20.269
<v UPOL>just mindlessly fostering

1394
00:59:20.270 --> 00:59:23.449
<v UPOL>trust. It's about mindfully

1395
00:59:23.450 --> 00:59:25.489
<v UPOL>calibrating trust.

1396
00:59:25.490 --> 00:59:28.009
<v UPOL>You don't want people to over trust your system

1397
00:59:28.010 --> 00:59:29.570
<v UPOL>because then there are liability issues.

1398
00:59:30.780 --> 00:59:32.259
<v ANDREY>Yeah, exactly.

1399
00:59:32.260 --> 00:59:34.979
<v ANDREY>And, yeah, so in terms of

1400
00:59:34.980 --> 00:59:38.309
<v ANDREY>this process, he started these four workshops

1401
00:59:38.310 --> 00:59:41.159
<v ANDREY>you, I think, carried out this idea of the four

1402
00:59:41.160 --> 00:59:44.469
<v ANDREY>W's what who lives.

1403
00:59:44.470 --> 00:59:46.709
<v ANDREY>And if I understand correctly after the workshops,

1404
00:59:46.710 --> 00:59:49.409
<v ANDREY>you then had sort of a more

1405
00:59:49.410 --> 00:59:52.769
<v ANDREY>controlled study where 29 participants.

1406
00:59:52.770 --> 00:59:53.879
<v ANDREY>Is that right?

1407
00:59:53.880 --> 00:59:56.459
<v UPOL>Yeah, yeah. So then then we really did this once we

1408
00:59:56.460 --> 00:59:57.839
<v UPOL>built the scenario, right?

1409
00:59:57.840 --> 00:59:59.939
<v UPOL>Then you kind of when made people go through the

1410
00:59:59.940 --> 01:00:01.469
<v UPOL>scenario of the study.

1411
01:00:01.470 --> 01:00:03.509
<v UPOL>So we would walk them through the scenario just the

1412
01:00:03.510 --> 01:00:06.359
<v UPOL>way I kind of described a few minutes ago.

1413
01:00:06.360 --> 01:00:08.879
<v UPOL>And we will start seeing that how they start

1414
01:00:08.880 --> 01:00:10.859
<v UPOL>thinking through this and this is the beauty of

1415
01:00:10.860 --> 01:00:12.359
<v UPOL>scenario, these design, right?

1416
01:00:12.360 --> 01:00:14.639
<v UPOL>You can think of this as a probe.

1417
01:00:14.640 --> 01:00:17.849
<v UPOL>So you are probing for reactions about

1418
01:00:17.850 --> 01:00:20.819
<v UPOL>an air power system without really

1419
01:00:20.820 --> 01:00:23.339
<v UPOL>needing to invest the severe infrastructure that is

1420
01:00:23.340 --> 01:00:26.579
<v UPOL>needed to make a like a full fledged AI system.

1421
01:00:26.580 --> 01:00:29.519
<v UPOL>But you're getting very good design elements

1422
01:00:29.520 --> 01:00:32.639
<v UPOL>out of this for a lot less cost.

1423
01:00:32.640 --> 01:00:35.099
<v UPOL>It does not mean that you don't build a system, of

1424
01:00:35.100 --> 01:00:37.799
<v UPOL>course you do. So, for instance, in

1425
01:00:37.800 --> 01:00:40.439
<v UPOL>the cybersecurity case, once we did

1426
01:00:40.440 --> 01:00:43.289
<v UPOL>very similar studies with them, with scenarios,

1427
01:00:43.290 --> 01:00:45.479
<v UPOL>we went and we built out, this takes.

1428
01:00:45.480 --> 01:00:46.639
<v UPOL>And guess what, right?

1429
01:00:46.640 --> 01:00:50.099
<v UPOL>Like two years into the project with them,

1430
01:00:50.100 --> 01:00:52.859
<v UPOL>that company actually now lives

1431
01:00:52.860 --> 01:00:55.709
<v UPOL>in a socially transparent world where all

1432
01:00:55.710 --> 01:00:58.979
<v UPOL>their decisions are actually automatically situated

1433
01:00:58.980 --> 01:01:00.839
<v UPOL>with prior history.

1434
01:01:00.840 --> 01:01:03.959
<v UPOL>And they are actually you able to use

1435
01:01:03.960 --> 01:01:07.109
<v UPOL>the four W's as training

1436
01:01:07.110 --> 01:01:09.689
<v UPOL>to retrain their models so that the decision is not

1437
01:01:09.690 --> 01:01:12.389
<v UPOL>only just algorithmically situated but also

1438
01:01:12.390 --> 01:01:14.099
<v UPOL>socially situated, right?

1439
01:01:14.100 --> 01:01:17.099
<v ANDREY>So it becomes of this sort of feature space

1440
01:01:17.100 --> 01:01:19.829
<v ANDREY>to model is informed by it seems.

1441
01:01:19.830 --> 01:01:22.059
<v UPOL>Absolutely. And then think about it that way, right?

1442
01:01:22.060 --> 01:01:24.869
<v UPOL>Like you are also getting a corpus without actually

1443
01:01:24.870 --> 01:01:25.870
<v UPOL>building a corpus.

1444
01:01:26.970 --> 01:01:29.189
<v UPOL>Right. Because over time, what is going to happen?

1445
01:01:29.190 --> 01:01:31.679
<v UPOL>These four W's are going to get enough density

1446
01:01:31.680 --> 01:01:34.109
<v UPOL>depending on who knows, right where they become

1447
01:01:34.110 --> 01:01:37.109
<v UPOL>large enough that you can feed back into the model.

1448
01:01:37.110 --> 01:01:39.959
<v UPOL>But the cool part is from

1449
01:01:39.960 --> 01:01:42.959
<v UPOL>day one, they're giving value to the user.

1450
01:01:42.960 --> 01:01:45.150
<v UPOL>Right? So they're not like being

1451
01:01:46.440 --> 01:01:49.019
<v UPOL>a grunt work of a data set building task.

1452
01:01:49.020 --> 01:01:51.179
<v UPOL>That is the one thing that a lot of my cybersecurity

1453
01:01:51.180 --> 01:01:53.879
<v UPOL>analyst stakeholders would counter that like,

1454
01:01:53.880 --> 01:01:55.769
<v UPOL>Hey, this is actually useful.

1455
01:01:55.770 --> 01:01:58.229
<v UPOL>I like doing this because it doesn't make me feel

1456
01:01:58.230 --> 01:02:00.659
<v UPOL>like I'm building a stupid dataset that I might not

1457
01:02:00.660 --> 01:02:03.299
<v UPOL>ever see. So they're actually building a

1458
01:02:03.300 --> 01:02:05.939
<v UPOL>corpus while getting value from

1459
01:02:05.940 --> 01:02:08.339
<v UPOL>it, which is very hard to achieve in any kind of

1460
01:02:08.340 --> 01:02:09.989
<v UPOL>dataset building tasks.

1461
01:02:09.990 --> 01:02:12.329
<v ANDREY>Mm hmm. Yeah, exactly.

1462
01:02:12.330 --> 01:02:15.749
<v ANDREY>And it's interesting to hear that they are

1463
01:02:15.750 --> 01:02:17.069
<v ANDREY>more into it.

1464
01:02:17.070 --> 01:02:19.769
<v ANDREY>And I think it's also funny that, you know, having

1465
01:02:19.770 --> 01:02:23.129
<v ANDREY>done a study in the paper, you are able to

1466
01:02:23.130 --> 01:02:26.429
<v ANDREY>not just give you on tape, but actually quotes

1467
01:02:26.430 --> 01:02:29.269
<v ANDREY>the study participants and really,

1468
01:02:29.270 --> 01:02:32.369
<v ANDREY>you know, showing their words

1469
01:02:32.370 --> 01:02:35.069
<v ANDREY>very concretely how you

1470
01:02:35.070 --> 01:02:38.019
<v ANDREY>came to your conclusion. So for instance, one quote

1471
01:02:38.020 --> 01:02:40.799
<v ANDREY>that I think is really relevant is I hate

1472
01:02:40.800 --> 01:02:43.559
<v ANDREY>how it just gives me a confidence level in gibberish

1473
01:02:43.560 --> 01:02:46.449
<v ANDREY>to engineers will understand for zero context,

1474
01:02:46.450 --> 01:02:49.169
<v ANDREY>right? It's a very human reaction

1475
01:02:49.170 --> 01:02:52.149
<v ANDREY>that really tells you, Well, you know,

1476
01:02:52.150 --> 01:02:53.939
<v ANDREY>this person, what's the context, right?

1477
01:02:55.230 --> 01:02:57.269
<v ANDREY>So then, you know, we've talked through somebody

1478
01:02:57.270 --> 01:02:59.969
<v ANDREY>resembles your study, and

1479
01:02:59.970 --> 01:03:03.059
<v ANDREY>now I think we can dove in a little more.

1480
01:03:03.060 --> 01:03:05.729
<v ANDREY>So we've talked about social concerns.

1481
01:03:05.730 --> 01:03:08.069
<v ANDREY>And I think in the paper, you also break down a

1482
01:03:08.070 --> 01:03:10.859
<v ANDREY>little bit what exactly is

1483
01:03:10.860 --> 01:03:13.529
<v ANDREY>made visible, what you want to

1484
01:03:13.530 --> 01:03:16.229
<v ANDREY>make visible. So the decision making context and the

1485
01:03:16.230 --> 01:03:18.179
<v ANDREY>organizational context.

1486
01:03:18.180 --> 01:03:20.909
<v ANDREY>So yeah, what is involved in

1487
01:03:20.910 --> 01:03:23.699
<v ANDREY>these things that people really need to understand?

1488
01:03:23.700 --> 01:03:26.459
<v UPOL>Yeah. So, you know, through the four W's.

1489
01:03:26.460 --> 01:03:28.739
<v UPOL>So these are the kind of like that you can think of

1490
01:03:28.740 --> 01:03:31.559
<v UPOL>them, the vehicles that carry this context,

1491
01:03:31.560 --> 01:03:32.709
<v UPOL>right? Mm hmm.

1492
01:03:32.710 --> 01:03:35.489
<v UPOL>So the four is the first thing,

1493
01:03:35.490 --> 01:03:38.159
<v UPOL>and I think we kind of shared a framework that

1494
01:03:38.160 --> 01:03:40.769
<v UPOL>sees how it makes context

1495
01:03:40.770 --> 01:03:43.429
<v UPOL>visible at three levels, which do the

1496
01:03:43.430 --> 01:03:45.749
<v UPOL>technical, the decision making and the

1497
01:03:45.750 --> 01:03:47.669
<v UPOL>organizational right.

1498
01:03:47.670 --> 01:03:48.839
<v UPOL>So but

1499
01:03:50.280 --> 01:03:53.069
<v UPOL>with the umbrella of all three

1500
01:03:53.070 --> 01:03:55.829
<v UPOL>is the first to use what we call cool knowledge,

1501
01:03:55.830 --> 01:03:58.769
<v UPOL>right? So crew knowledge is really

1502
01:03:58.770 --> 01:04:01.949
<v UPOL>an important part of these informal

1503
01:04:01.950 --> 01:04:03.869
<v UPOL>knowledge that is acquired through hands on

1504
01:04:03.870 --> 01:04:06.839
<v UPOL>experience. It's part, and it's tacit

1505
01:04:06.840 --> 01:04:09.059
<v UPOL>of every job that anyone has ever done.

1506
01:04:09.060 --> 01:04:12.199
<v UPOL>Right? And it's often situated locally

1507
01:04:12.200 --> 01:04:14.999
<v UPOL>in a in a tight knit community of practice,

1508
01:04:15.000 --> 01:04:17.609
<v UPOL>sort of like an aggregated set of Milhouse.

1509
01:04:17.610 --> 01:04:20.249
<v UPOL>Right? So the Y

1510
01:04:20.250 --> 01:04:23.189
<v UPOL>right, the Y is actually giving insight

1511
01:04:23.190 --> 01:04:24.899
<v UPOL>into that knowledge.

1512
01:04:24.900 --> 01:04:26.829
<v UPOL>These are the variables that would be important for

1513
01:04:26.830 --> 01:04:29.759
<v UPOL>decision making, but sometimes are not captured

1514
01:04:29.760 --> 01:04:32.119
<v UPOL>in the eyes kind of feature space.

1515
01:04:32.120 --> 01:04:34.769
<v UPOL>Right? The other part is like social

1516
01:04:34.770 --> 01:04:36.960
<v UPOL>transparency can support analogical reasoning

1517
01:04:37.980 --> 01:04:39.929
<v UPOL>in terms of like, OK, if someone has made the

1518
01:04:39.930 --> 01:04:42.659
<v UPOL>decision in the past, like you remember, just I just

1519
01:04:42.660 --> 01:04:45.599
<v UPOL>gave a discount for the COVID case.

1520
01:04:45.600 --> 01:04:48.719
<v UPOL>So that means I too can give the discount

1521
01:04:48.720 --> 01:04:51.059
<v UPOL>on the Kovic case. Right?

1522
01:04:51.060 --> 01:04:52.949
<v UPOL>So it's aiming.

1523
01:04:52.950 --> 01:04:55.409
<v UPOL>So at the at the technical level, it helps you

1524
01:04:55.410 --> 01:04:58.349
<v UPOL>calibrate the trust on the air right

1525
01:04:58.350 --> 01:05:00.179
<v UPOL>and the decision making level.

1526
01:05:00.180 --> 01:05:02.829
<v UPOL>It can foster a sense of confidence and a

1527
01:05:02.830 --> 01:05:05.429
<v UPOL>decision making resilience that you

1528
01:05:05.430 --> 01:05:06.839
<v UPOL>know. How good are you?

1529
01:05:06.840 --> 01:05:09.509
<v UPOL>Can you trust A.I.? Can you not trust and

1530
01:05:09.510 --> 01:05:10.859
<v UPOL>self confidence, right?

1531
01:05:10.860 --> 01:05:13.609
<v UPOL>Yes, these difference between like, do I trust the

1532
01:05:13.610 --> 01:05:17.109
<v UPOL>A.I. versus do I trust myself to act on that?

1533
01:05:17.110 --> 01:05:18.689
<v UPOL>And I think those two are slightly different

1534
01:05:18.690 --> 01:05:22.139
<v UPOL>constructs and organizationally

1535
01:05:22.140 --> 01:05:24.959
<v UPOL>leases. So for them to use is capturing

1536
01:05:24.960 --> 01:05:27.719
<v UPOL>these tacit knowledge and the meta knowledge of

1537
01:05:27.720 --> 01:05:29.139
<v UPOL>how an organization will work.

1538
01:05:29.140 --> 01:05:32.129
<v UPOL>So you get an understanding of norms and values,

1539
01:05:32.130 --> 01:05:33.329
<v UPOL>right?

1540
01:05:33.330 --> 01:05:36.929
<v UPOL>It kind of encodes a level of institutional memory,

1541
01:05:36.930 --> 01:05:39.839
<v UPOL>and it promotes accountability because

1542
01:05:39.840 --> 01:05:41.129
<v UPOL>you can audit it, right?

1543
01:05:41.130 --> 01:05:43.829
<v UPOL>If you know who did what, when and why you can

1544
01:05:43.830 --> 01:05:45.420
<v UPOL>go back and audit things.

1545
01:05:46.470 --> 01:05:48.719
<v UPOL>So those are some of the things that we got out of

1546
01:05:48.720 --> 01:05:51.809
<v UPOL>this that are helpful when it comes to making

1547
01:05:51.810 --> 01:05:53.969
<v UPOL>the AI powered decision making.

1548
01:05:53.970 --> 01:05:56.909
<v ANDREY>Yeah, it's quite interesting this notion of decision

1549
01:05:56.910 --> 01:05:58.739
<v ANDREY>making and organizational context.

1550
01:05:58.740 --> 01:06:01.529
<v ANDREY>I think you define decision as

1551
01:06:01.530 --> 01:06:04.559
<v ANDREY>sort of, you know, localized to a decision.

1552
01:06:04.560 --> 01:06:07.199
<v ANDREY>So like, you know, you choose a price quarter,

1553
01:06:07.200 --> 01:06:09.719
<v ANDREY>you you think about similar price quotas.

1554
01:06:09.720 --> 01:06:12.429
<v ANDREY>Organization context is something that's

1555
01:06:12.430 --> 01:06:15.059
<v ANDREY>easier to forget, but it's sort of, you

1556
01:06:15.060 --> 01:06:17.459
<v ANDREY>know, what do we stand for?

1557
01:06:17.460 --> 01:06:20.049
<v ANDREY>You know, how aggressive are we?

1558
01:06:20.050 --> 01:06:22.289
<v ANDREY>You know, these sorts of things that are or in

1559
01:06:22.290 --> 01:06:25.259
<v ANDREY>general and

1560
01:06:25.260 --> 01:06:26.389
<v ANDREY>yeah, pretty interesting to.

1561
01:06:26.390 --> 01:06:29.159
<v ANDREY>I think and I guess you came

1562
01:06:29.160 --> 01:06:32.159
<v ANDREY>to understanding this sort of split

1563
01:06:32.160 --> 01:06:34.919
<v ANDREY>by just seeing what people used

1564
01:06:34.920 --> 01:06:37.739
<v ANDREY>or included in their four W's.

1565
01:06:37.740 --> 01:06:40.349
<v UPOL>Yes, I think this came back from those workshops.

1566
01:06:40.350 --> 01:06:42.449
<v UPOL>I think where we kind of understand like, OK,

1567
01:06:42.450 --> 01:06:44.129
<v UPOL>because, you know, we were thinking, maybe there is

1568
01:06:44.130 --> 01:06:46.829
<v UPOL>an h like how maybe there is another

1569
01:06:46.830 --> 01:06:48.659
<v UPOL>W like where.

1570
01:06:48.660 --> 01:06:50.400
<v UPOL>So we were trying to understand

1571
01:06:51.450 --> 01:06:54.089
<v UPOL>what would be the minimum viable product, so

1572
01:06:54.090 --> 01:06:56.969
<v UPOL>to speak about in the in the social transparency,

1573
01:06:56.970 --> 01:06:58.769
<v UPOL>because we didn't also want to overwhelm people.

1574
01:06:59.820 --> 01:07:02.159
<v UPOL>So the way we kind of understood like what they were

1575
01:07:02.160 --> 01:07:05.009
<v UPOL>doing is actually through the case studies

1576
01:07:05.010 --> 01:07:07.339
<v UPOL>that we have done in addition to this study.

1577
01:07:07.340 --> 01:07:08.789
<v UPOL>Right. So we were trying.

1578
01:07:08.790 --> 01:07:11.429
<v UPOL>We were inspired by that aspect.

1579
01:07:11.430 --> 01:07:13.679
<v UPOL>And that's why, like, you know, in the in table

1580
01:07:13.680 --> 01:07:16.289
<v UPOL>three, we kind of talk about, you know

1581
01:07:16.290 --> 01:07:17.829
<v UPOL>what? What was it?

1582
01:07:17.830 --> 01:07:20.759
<v UPOL>So it's an action taken on the API, the decision

1583
01:07:20.760 --> 01:07:23.699
<v UPOL>outcome. Why is like the comments

1584
01:07:23.700 --> 01:07:26.879
<v UPOL>with the rationale that justifies the decision

1585
01:07:26.880 --> 01:07:29.309
<v UPOL>because of what and why is always linked?

1586
01:07:29.310 --> 01:07:32.159
<v UPOL>And then who the name the organizational

1587
01:07:32.160 --> 01:07:35.129
<v UPOL>role, because sometimes seniority starts

1588
01:07:35.130 --> 01:07:38.039
<v UPOL>playing a role like it was a director.

1589
01:07:38.040 --> 01:07:40.709
<v UPOL>You kind of take their their view a little

1590
01:07:40.710 --> 01:07:42.659
<v UPOL>more than others.

1591
01:07:42.660 --> 01:07:44.939
<v UPOL>And then when is the time of decision?

1592
01:07:44.940 --> 01:07:47.219
<v UPOL>And that's important because sometimes something

1593
01:07:47.220 --> 01:07:49.319
<v UPOL>some decisions are not relevant, right?

1594
01:07:49.320 --> 01:07:52.349
<v UPOL>So think about like, you know, pre-COVID, decisions

1595
01:07:52.350 --> 01:07:54.779
<v UPOL>do not become very relevant during COVID.

1596
01:07:54.780 --> 01:07:57.419
<v UPOL>So those are some of the things that we found, not

1597
01:07:57.420 --> 01:08:00.569
<v UPOL>just by our workshops, but also analyzing the data,

1598
01:08:00.570 --> 01:08:02.909
<v UPOL>the qualitative data through the interviews and the

1599
01:08:02.910 --> 01:08:03.929
<v UPOL>walk throughs that we had.

1600
01:08:05.280 --> 01:08:08.099
<v ANDREY>Yeah. And then you found, you know, the what

1601
01:08:08.100 --> 01:08:11.189
<v ANDREY>is really important, the why is important, the when,

1602
01:08:11.190 --> 01:08:13.059
<v ANDREY>you know, sometimes, but you know.

1603
01:08:13.060 --> 01:08:15.839
<v ANDREY>Yeah. And then also, I think probably

1604
01:08:15.840 --> 01:08:18.528
<v ANDREY>in fungi, sort of how you present

1605
01:08:18.529 --> 01:08:19.559
<v ANDREY>things.

1606
01:08:19.560 --> 01:08:21.629
<v UPOL>We actually asked our participants at the end of it.

1607
01:08:21.630 --> 01:08:23.969
<v UPOL>I'm like, Can you rank it and tell me why?

1608
01:08:23.970 --> 01:08:27.059
<v UPOL>Right? So we would make them rank the them

1609
01:08:27.060 --> 01:08:29.278
<v UPOL>like, tell me what you cannot live without and

1610
01:08:29.279 --> 01:08:32.068
<v UPOL>everyone saying, I can't live without the what.

1611
01:08:32.069 --> 01:08:34.019
<v UPOL>And then I said, OK, imagine that I can give you one

1612
01:08:34.020 --> 01:08:35.429
<v UPOL>more. What would that be like?

1613
01:08:35.430 --> 01:08:37.108
<v UPOL>Oh, I need another wide.

1614
01:08:37.109 --> 01:08:39.398
<v UPOL>And then I said, Now imagine I give you one more.

1615
01:08:39.399 --> 01:08:41.099
<v UPOL>That's I need to know the whole.

1616
01:08:41.100 --> 01:08:43.469
<v UPOL>So that's how we kind of made them do this ranking

1617
01:08:43.470 --> 01:08:46.169
<v UPOL>task and then get a sense of importance,

1618
01:08:46.170 --> 01:08:48.869
<v UPOL>because sometimes many companies might not have all

1619
01:08:48.870 --> 01:08:50.278
<v UPOL>before that.

1620
01:08:50.279 --> 01:08:52.889
<v UPOL>There might be privacy concerns that prevent

1621
01:08:52.890 --> 01:08:54.629
<v UPOL>the HU from being shot.

1622
01:08:54.630 --> 01:08:56.789
<v UPOL>Right. Because you can also see, like, you know,

1623
01:08:56.790 --> 01:08:59.489
<v UPOL>biases creep up, like if I show the profile

1624
01:08:59.490 --> 01:09:02.249
<v UPOL>picture and you know, if you can guess the person's

1625
01:09:02.250 --> 01:09:05.309
<v UPOL>race or gender from the profile picture,

1626
01:09:05.310 --> 01:09:08.459
<v UPOL>it can create certain biased viewpoints

1627
01:09:08.460 --> 01:09:09.778
<v UPOL>or even the location, right?

1628
01:09:09.779 --> 01:09:12.509
<v UPOL>Because certain companies are multinational

1629
01:09:12.510 --> 01:09:14.909
<v UPOL>and it could be that, you know, certain locations

1630
01:09:14.910 --> 01:09:17.938
<v UPOL>are not often looked positively enough.

1631
01:09:17.939 --> 01:09:20.608
<v UPOL>And that's my bias, the receiver's

1632
01:09:20.609 --> 01:09:21.898
<v UPOL>perception.

1633
01:09:21.899 --> 01:09:22.899
<v UPOL>Mm hmm.

1634
01:09:23.490 --> 01:09:26.728
<v ANDREY>Yeah, exactly. And I think again, it's interesting

1635
01:09:26.729 --> 01:09:29.818
<v ANDREY>here, just reading the paper,

1636
01:09:29.819 --> 01:09:32.519
<v ANDREY>which I think is, is, you know, I would recommend

1637
01:09:32.520 --> 01:09:34.739
<v ANDREY>it. I think it's quite approachable.

1638
01:09:34.740 --> 01:09:37.438
<v ANDREY>Is again, you have these quotes

1639
01:09:37.439 --> 01:09:40.318
<v ANDREY>from the study participants that make it very

1640
01:09:40.319 --> 01:09:43.019
<v ANDREY>concrete. One of them is the outcome should

1641
01:09:43.020 --> 01:09:46.169
<v ANDREY>be a tldr the why is

1642
01:09:46.170 --> 01:09:47.520
<v ANDREY>there if I'm interested,

1643
01:09:48.819 --> 01:09:50.249
<v ANDREY>then there's also the issue.

1644
01:09:50.250 --> 01:09:52.469
<v ANDREY>Someone said if I knew for to reach out to, I could

1645
01:09:52.470 --> 01:09:55.109
<v ANDREY>find out the rest of the story and so on.

1646
01:09:55.110 --> 01:09:58.109
<v ANDREY>So again, it's it's really giving you a sense

1647
01:09:58.110 --> 01:10:00.839
<v ANDREY>of how your study

1648
01:10:00.840 --> 01:10:03.479
<v ANDREY>and interaction with people led

1649
01:10:03.480 --> 01:10:06.509
<v ANDREY>you to your conclusions, which I really enjoyed

1650
01:10:06.510 --> 01:10:07.510
<v ANDREY>in reading the paper.

1651
01:10:08.960 --> 01:10:11.419
<v UPOL>No, you know, thank you so much for the kind words

1652
01:10:11.420 --> 01:10:13.400
<v UPOL>we put a lot of love into this paper.

1653
01:10:14.440 --> 01:10:15.440
<v ANDREY>Yeah.

1654
01:10:15.940 --> 01:10:18.789
<v ANDREY>And, yeah, you know, that's what you need

1655
01:10:18.790 --> 01:10:21.549
<v ANDREY>to make a paper really enjoyable, so your work

1656
01:10:21.550 --> 01:10:22.550
<v ANDREY>pay off.

1657
01:10:23.420 --> 01:10:26.019
<v ANDREY>Now I think we can touch on a lot of

1658
01:10:26.020 --> 01:10:29.139
<v ANDREY>it and it went through, I think hopefully the most

1659
01:10:29.140 --> 01:10:31.269
<v ANDREY>this stuff in terms of the study elements and the

1660
01:10:31.270 --> 01:10:33.969
<v ANDREY>four W's and make clear a social

1661
01:10:33.970 --> 01:10:36.579
<v ANDREY>transparency is now on to a

1662
01:10:36.580 --> 01:10:39.219
<v ANDREY>couple final things.

1663
01:10:39.220 --> 01:10:40.810
<v ANDREY>So we've

1664
01:10:41.890 --> 01:10:45.309
<v ANDREY>said, you know, it's good to have

1665
01:10:45.310 --> 01:10:47.709
<v ANDREY>this on top of what is already there.

1666
01:10:47.710 --> 01:10:49.569
<v ANDREY>I'll go to make sure it's fancy.

1667
01:10:49.570 --> 01:10:52.269
<v ANDREY>So you need to add the social transparency

1668
01:10:52.270 --> 01:10:55.269
<v ANDREY>and one question there as well is it easy

1669
01:10:55.270 --> 01:10:57.969
<v ANDREY>or is it a very challenge is in place that

1670
01:10:57.970 --> 01:11:00.969
<v ANDREY>would make it harder to do that?

1671
01:11:00.970 --> 01:11:02.229
<v UPOL>Yeah, that's a that's a good point.

1672
01:11:02.230 --> 01:11:05.019
<v UPOL>I think, you know, as as with everything, there has

1673
01:11:05.020 --> 01:11:08.379
<v UPOL>to be the infrastructure that is supported,

1674
01:11:08.380 --> 01:11:09.380
<v UPOL>right?

1675
01:11:09.940 --> 01:11:12.549
<v UPOL>And there are challenges like privacy.

1676
01:11:12.550 --> 01:11:15.309
<v UPOL>There are challenges like biases

1677
01:11:15.310 --> 01:11:18.099
<v UPOL>or information overload, as well as incentives

1678
01:11:18.100 --> 01:11:20.889
<v UPOL>like if you want to engage in a socially

1679
01:11:20.890 --> 01:11:23.589
<v UPOL>transparent system that has to be incentive

1680
01:11:23.590 --> 01:11:26.289
<v UPOL>for people to engage with it

1681
01:11:26.290 --> 01:11:28.089
<v UPOL>like, you know, give those four W's as they're

1682
01:11:28.090 --> 01:11:29.979
<v UPOL>working, that is a burden that is added, right?

1683
01:11:29.980 --> 01:11:31.239
<v UPOL>Like no fees, lunches.

1684
01:11:32.290 --> 01:11:34.899
<v UPOL>And so that means we have to be

1685
01:11:34.900 --> 01:11:36.849
<v UPOL>very mindful of that.

1686
01:11:36.850 --> 01:11:38.349
<v UPOL>And you know, we can, you know, with the Ford

1687
01:11:38.350 --> 01:11:40.659
<v UPOL>family, you could also kind of promote groupthink,

1688
01:11:40.660 --> 01:11:43.419
<v UPOL>right? Imagine in a company culture where you're not

1689
01:11:43.420 --> 01:11:45.249
<v UPOL>allowed to go against your boss and you see a

1690
01:11:45.250 --> 01:11:47.229
<v UPOL>comment from your boss previously.

1691
01:11:47.230 --> 01:11:49.919
<v UPOL>So so we have to be careful.

1692
01:11:49.920 --> 01:11:51.189
<v UPOL>You know, it's not a golden bullet.

1693
01:11:52.240 --> 01:11:55.269
<v UPOL>So we have to be very careful when we operationalize

1694
01:11:55.270 --> 01:11:58.029
<v UPOL>the social transparency that we

1695
01:11:58.030 --> 01:12:00.729
<v UPOL>are trying to be very mindful of some of these

1696
01:12:00.730 --> 01:12:02.769
<v UPOL>challenges, like, you know, do we really want to see

1697
01:12:02.770 --> 01:12:05.289
<v UPOL>all the four W's at every single time?

1698
01:12:05.290 --> 01:12:07.509
<v UPOL>No, there are ways to summarize it.

1699
01:12:07.510 --> 01:12:09.939
<v UPOL>And we have done that in my project with the cyber

1700
01:12:09.940 --> 01:12:12.399
<v UPOL>security people, we have been able to figure out how

1701
01:12:12.400 --> 01:12:15.219
<v UPOL>to summarize these aspects at a level of detail

1702
01:12:15.220 --> 01:12:16.220
<v UPOL>that is actionable.

1703
01:12:17.810 --> 01:12:18.999
<v ANDREY>Yeah.

1704
01:12:19.000 --> 01:12:21.759
<v ANDREY>And so speaking of cyber security,

1705
01:12:21.760 --> 01:12:24.379
<v ANDREY>people say, take it outside the

1706
01:12:24.380 --> 01:12:27.039
<v ANDREY>study, I was interacting with

1707
01:12:27.040 --> 01:12:28.839
<v ANDREY>these participants and,

1708
01:12:30.130 --> 01:12:33.039
<v ANDREY>you know, figuring out the of context.

1709
01:12:33.040 --> 01:12:36.219
<v ANDREY>You also took this for context to

1710
01:12:36.220 --> 01:12:38.979
<v ANDREY>an actual organization and then tried it out.

1711
01:12:38.980 --> 01:12:41.919
<v UPOL>Is that right? Yeah. So like if you remember

1712
01:12:41.920 --> 01:12:44.119
<v UPOL>just from a timeline perspective, right?

1713
01:12:44.120 --> 01:12:46.989
<v UPOL>So by the time I think we wrote the paper

1714
01:12:46.990 --> 01:12:48.819
<v UPOL>we already had.

1715
01:12:48.820 --> 01:12:50.019
<v UPOL>This is obviously the study.

1716
01:12:50.020 --> 01:12:51.799
<v UPOL>So there was an empirical study that was done.

1717
01:12:51.800 --> 01:12:54.549
<v UPOL>Separate from this in parallel was

1718
01:12:54.550 --> 01:12:56.979
<v UPOL>the cyber security project that I was running for a

1719
01:12:56.980 --> 01:12:58.329
<v UPOL>long, long time.

1720
01:12:58.330 --> 01:13:00.939
<v UPOL>And what I had the, I guess, the luxury

1721
01:13:00.940 --> 01:13:03.639
<v UPOL>of knowing the future to some extent is

1722
01:13:03.640 --> 01:13:06.039
<v UPOL>we were able to incorporate a lot of these four ws

1723
01:13:06.040 --> 01:13:08.649
<v UPOL>into their system and they lived

1724
01:13:08.650 --> 01:13:11.409
<v UPOL>in a socially transparent world when we wrote this

1725
01:13:11.410 --> 01:13:14.139
<v UPOL>paper. So that's why we were able to talk a lot

1726
01:13:14.140 --> 01:13:16.899
<v UPOL>about these transfer cases challenges

1727
01:13:16.900 --> 01:13:18.699
<v UPOL>because those are some of the challenges we faced in

1728
01:13:18.700 --> 01:13:21.909
<v UPOL>the real world when we were trying to implement this

1729
01:13:21.910 --> 01:13:24.969
<v UPOL>in an enterprise setting that is multinational.

1730
01:13:24.970 --> 01:13:27.609
<v ANDREY>I see. So when you presented this

1731
01:13:27.610 --> 01:13:29.589
<v ANDREY>and sort of said, we should do this,

1732
01:13:30.880 --> 01:13:33.129
<v ANDREY>you know how receptive our people today sort of get

1733
01:13:33.130 --> 01:13:34.539
<v ANDREY>it right away or

1734
01:13:34.540 --> 01:13:37.689
<v UPOL>initially there was a little bit of like hesitation,

1735
01:13:37.690 --> 01:13:40.269
<v UPOL>I think, because someone said, like, how is this

1736
01:13:40.270 --> 01:13:41.270
<v UPOL>explainability,

1737
01:13:43.180 --> 01:13:45.669
<v UPOL>right? Because there is this and there is a very

1738
01:13:45.670 --> 01:13:48.279
<v UPOL>powerful like AI developer like this is not

1739
01:13:48.280 --> 01:13:50.139
<v UPOL>explainability. And I think that's kind of like the

1740
01:13:50.140 --> 01:13:53.289
<v UPOL>idea of the paper kind of came to light.

1741
01:13:53.290 --> 01:13:56.709
<v UPOL>Our idea of explainability is so narrow

1742
01:13:56.710 --> 01:13:59.499
<v UPOL>that we have a hard time

1743
01:13:59.500 --> 01:14:02.409
<v UPOL>kind of even envisioning more than that.

1744
01:14:02.410 --> 01:14:05.019
<v UPOL>So what we actually did to kind of address those

1745
01:14:05.020 --> 01:14:07.399
<v UPOL>kind of concerns is, you know, as we see as you saw

1746
01:14:07.400 --> 01:14:10.179
<v UPOL>also on the paper in this empirical study that we

1747
01:14:10.180 --> 01:14:11.260
<v UPOL>had concrete

1748
01:14:12.910 --> 01:14:15.759
<v UPOL>like directly from the stakeholder

1749
01:14:15.760 --> 01:14:18.699
<v UPOL>information about how these additional context

1750
01:14:18.700 --> 01:14:21.939
<v UPOL>help them understand the system, right?

1751
01:14:21.940 --> 01:14:24.129
<v UPOL>And then if we go back to our initial definition of

1752
01:14:24.130 --> 01:14:26.289
<v UPOL>explainability rights, things that helped me

1753
01:14:26.290 --> 01:14:28.359
<v UPOL>understand the AI systems, right?

1754
01:14:28.360 --> 01:14:30.969
<v UPOL>And in this case, the AI systems are not algorithm.

1755
01:14:30.970 --> 01:14:32.919
<v UPOL>These are human AI assemblages.

1756
01:14:32.920 --> 01:14:36.549
<v UPOL>Right? So and they're socio technically situated.

1757
01:14:36.550 --> 01:14:37.899
<v UPOL>So there you go.

1758
01:14:37.900 --> 01:14:40.689
<v UPOL>So initially, there was a lot of

1759
01:14:40.690 --> 01:14:43.239
<v UPOL>pushback, but what the proof is often in the

1760
01:14:43.240 --> 01:14:46.359
<v UPOL>pudding. So when we added social transparency,

1761
01:14:46.360 --> 01:14:48.979
<v UPOL>the engagement went from like two percent

1762
01:14:48.980 --> 01:14:50.639
<v UPOL>to ninety six percent.

1763
01:14:50.640 --> 01:14:53.549
<v UPOL>Right? That you can't ignore.

1764
01:14:53.550 --> 01:14:56.229
<v UPOL>Right. And so so those are some of the

1765
01:14:56.230 --> 01:14:58.869
<v UPOL>things that helped a lot of the stakeholders have

1766
01:14:58.870 --> 01:15:01.539
<v UPOL>more buy in and get a sense of, OK,

1767
01:15:01.540 --> 01:15:03.819
<v UPOL>now this is important. This might not look

1768
01:15:03.820 --> 01:15:06.579
<v UPOL>algorithmic, but it has everything to do with

1769
01:15:06.580 --> 01:15:08.069
<v UPOL>the algorithm, right?

1770
01:15:09.130 --> 01:15:11.499
<v ANDREY>Yeah, I guess it harkens back to the title of a

1771
01:15:11.500 --> 01:15:14.889
<v ANDREY>work, right? Expanding its pie ability,

1772
01:15:14.890 --> 01:15:17.319
<v ANDREY>you know, as person said, how is this expandability

1773
01:15:17.320 --> 01:15:19.989
<v ANDREY>while you've pointed out, then you sort of make the

1774
01:15:19.990 --> 01:15:23.859
<v ANDREY>argument that this should be part of expandability,

1775
01:15:23.860 --> 01:15:26.739
<v ANDREY>and by adding it, you get sort

1776
01:15:26.740 --> 01:15:29.829
<v ANDREY>of a more holistic, full understanding.

1777
01:15:29.830 --> 01:15:32.199
<v ANDREY>Is that kind of a fair characterization?

1778
01:15:32.200 --> 01:15:34.539
<v UPOL>Yeah, yeah. And I think, you know, sometimes the

1779
01:15:34.540 --> 01:15:37.239
<v UPOL>simplicity is kind of elusive

1780
01:15:37.240 --> 01:15:38.709
<v UPOL>and deceptive.

1781
01:15:38.710 --> 01:15:41.439
<v UPOL>But, you know, we also have to understand

1782
01:15:41.440 --> 01:15:43.659
<v UPOL>that sometimes very powerful ideas and also very

1783
01:15:43.660 --> 01:15:46.359
<v UPOL>simple ideals. And I think within AI, we

1784
01:15:46.360 --> 01:15:48.879
<v UPOL>have to kind of go back to those roots at some

1785
01:15:48.880 --> 01:15:51.579
<v UPOL>point, like not everything that is complex is good.

1786
01:15:51.580 --> 01:15:54.639
<v UPOL>Neither is not everything that is simple is bad.

1787
01:15:54.640 --> 01:15:57.219
<v UPOL>You can have very good ideas that are very simple.

1788
01:15:57.220 --> 01:15:59.919
<v ANDREY>Yeah, exactly. Simple ideas can be very powerful.

1789
01:15:59.920 --> 01:16:02.739
<v ANDREY>And I guess one of the key insights here is

1790
01:16:02.740 --> 01:16:05.499
<v ANDREY>social transparency as a concept and as something

1791
01:16:05.500 --> 01:16:08.289
<v ANDREY>that needs to be part of expandability.

1792
01:16:08.290 --> 01:16:11.019
<v ANDREY>So just to go back and situate within

1793
01:16:11.020 --> 01:16:13.749
<v ANDREY>the XIII research

1794
01:16:13.750 --> 01:16:15.789
<v ANDREY>field, you know,

1795
01:16:16.810 --> 01:16:19.419
<v ANDREY>I don't know too much about the context of that

1796
01:16:19.420 --> 01:16:20.749
<v ANDREY>field and what is going on there.

1797
01:16:20.750 --> 01:16:22.240
<v ANDREY>So what do you think could be

1798
01:16:23.440 --> 01:16:26.169
<v ANDREY>hopefully, I guess, the impact and

1799
01:16:26.170 --> 01:16:30.039
<v ANDREY>what this could enable as far as future research?

1800
01:16:30.040 --> 01:16:33.069
<v UPOL>First of all, I think it makes this very nebulous

1801
01:16:33.070 --> 01:16:36.429
<v UPOL>topic of socio organizational context tractable,

1802
01:16:36.430 --> 01:16:39.039
<v UPOL>right? Like for concrete things to

1803
01:16:39.040 --> 01:16:41.679
<v UPOL>go for, and that's a good starting point.

1804
01:16:41.680 --> 01:16:44.469
<v UPOL>It gives people to grasp on to that and build

1805
01:16:44.470 --> 01:16:46.269
<v UPOL>on it. And I think that's what we actually invite

1806
01:16:46.270 --> 01:16:49.269
<v UPOL>people to do right is

1807
01:16:49.270 --> 01:16:52.659
<v UPOL>now that we have at least started the conversation.

1808
01:16:52.660 --> 01:16:55.719
<v UPOL>That explainability is beyond

1809
01:16:55.720 --> 01:16:58.899
<v UPOL>algorithmic transparency and given the community

1810
01:16:58.900 --> 01:17:01.809
<v UPOL>one way of capturing the socio

1811
01:17:01.810 --> 01:17:04.779
<v UPOL>organizational context, I think now it starts

1812
01:17:04.780 --> 01:17:07.389
<v UPOL>to seed more

1813
01:17:07.390 --> 01:17:09.549
<v UPOL>ideas. And I think there is some fascinating paper

1814
01:17:09.550 --> 01:17:12.549
<v UPOL>that I've seen after that around

1815
01:17:12.550 --> 01:17:15.189
<v UPOL>and ideas actually that

1816
01:17:15.190 --> 01:17:17.869
<v UPOL>talk. Using this notion of social transparency

1817
01:17:17.870 --> 01:17:21.229
<v UPOL>talked about end to end lifecycle perspectives

1818
01:17:21.230 --> 01:17:23.779
<v UPOL>within explainability, like who needs to know what,

1819
01:17:23.780 --> 01:17:26.359
<v UPOL>when and why, like sheep and ocher and Christine

1820
01:17:26.360 --> 01:17:28.669
<v UPOL>Wolfe and others have kind of written about it.

1821
01:17:28.670 --> 01:17:31.699
<v UPOL>So I didn't get it. It gives us bedrock

1822
01:17:31.700 --> 01:17:34.549
<v UPOL>for future work to kind of build on it, and I hope

1823
01:17:34.550 --> 01:17:37.249
<v UPOL>it does, and I'll work

1824
01:17:37.250 --> 01:17:39.979
<v UPOL>within explainability takes far beyond social

1825
01:17:39.980 --> 01:17:41.239
<v UPOL>transparency.

1826
01:17:41.240 --> 01:17:43.429
<v UPOL>There are other things that are outside the box that

1827
01:17:43.430 --> 01:17:45.019
<v UPOL>also need to be included.

1828
01:17:45.020 --> 01:17:46.639
<v UPOL>And how do we encode that?

1829
01:17:46.640 --> 01:17:48.829
<v UPOL>I hope people use this kind of scenario, these

1830
01:17:48.830 --> 01:17:51.439
<v UPOL>design techniques, and it is also not shy

1831
01:17:51.440 --> 01:17:53.449
<v UPOL>away from the fact that if something as simple,

1832
01:17:53.450 --> 01:17:56.059
<v UPOL>right, as long as powerful, that's

1833
01:17:56.060 --> 01:17:58.130
<v UPOL>still a valid and good contribution.

1834
01:17:59.140 --> 01:18:01.599
<v ANDREY>Yeah, I guess in a sense, that's how you want

1835
01:18:01.600 --> 01:18:03.669
<v ANDREY>research to work, someone reads a paper and it's

1836
01:18:03.670 --> 01:18:06.549
<v ANDREY>like, Wow, this is cool, but what if he did this

1837
01:18:06.550 --> 01:18:08.899
<v ANDREY>or this thing doesn't work?

1838
01:18:08.900 --> 01:18:10.539
<v ANDREY>You know, I have this idea.

1839
01:18:10.540 --> 01:18:12.639
<v ANDREY>So that makes a lot of sense.

1840
01:18:12.640 --> 01:18:15.689
<v ANDREY>And also to that notion of sort of the context

1841
01:18:15.690 --> 01:18:18.399
<v ANDREY>and the field itself, we talked about on a

1842
01:18:18.400 --> 01:18:21.249
<v ANDREY>bit of a push back, it got at the

1843
01:18:21.250 --> 01:18:24.489
<v ANDREY>industry level within the research community.

1844
01:18:24.490 --> 01:18:26.319
<v ANDREY>You know, when you submitted it, when you got

1845
01:18:26.320 --> 01:18:28.929
<v ANDREY>reviews, when you presented it, what was the

1846
01:18:28.930 --> 01:18:31.149
<v ANDREY>reception of your colleagues?

1847
01:18:32.230 --> 01:18:33.999
<v UPOL>I think it was surprising to us.

1848
01:18:34.000 --> 01:18:35.889
<v UPOL>We always thought when we wrote the paper that

1849
01:18:35.890 --> 01:18:39.099
<v UPOL>people either hate it or they will love it.

1850
01:18:39.100 --> 01:18:41.709
<v UPOL>I don't think anyone who's going to be neutral to

1851
01:18:41.710 --> 01:18:44.349
<v UPOL>it because it was making a very provocative

1852
01:18:44.350 --> 01:18:46.989
<v UPOL>argument. It was making the argument that

1853
01:18:46.990 --> 01:18:49.419
<v UPOL>explainability is not transgressive.

1854
01:18:49.420 --> 01:18:51.279
<v UPOL>It is more than that. And it's not just saying that

1855
01:18:51.280 --> 01:18:53.769
<v UPOL>it's like this one way of doing it.

1856
01:18:53.770 --> 01:18:57.729
<v UPOL>So and clearly it was well-received,

1857
01:18:57.730 --> 01:19:00.489
<v UPOL>and the presentation at Chi went

1858
01:19:00.490 --> 01:19:01.490
<v UPOL>very well.

1859
01:19:02.830 --> 01:19:05.289
<v UPOL>And, you know, we were very lucky to receive this

1860
01:19:05.290 --> 01:19:07.089
<v UPOL>paper, honorable mention on it as well.

1861
01:19:08.170 --> 01:19:10.869
<v UPOL>So I think overall, it went better than we

1862
01:19:10.870 --> 01:19:13.009
<v UPOL>expected it, to be honest.

1863
01:19:13.010 --> 01:19:16.329
<v ANDREY>Yeah, it's good to hear that given

1864
01:19:16.330 --> 01:19:19.059
<v ANDREY>again, this was it looks to be quite

1865
01:19:19.060 --> 01:19:22.269
<v ANDREY>Stafford guy is pretty big, right?

1866
01:19:22.270 --> 01:19:24.969
<v UPOL>Yeah, it is the premier HCI conference.

1867
01:19:24.970 --> 01:19:27.339
<v UPOL>So like, not like nervous for now because nervous

1868
01:19:27.340 --> 01:19:29.589
<v UPOL>runs at a different scale, but like in terms of like

1869
01:19:29.590 --> 01:19:30.849
<v UPOL>the premiere venue.

1870
01:19:30.850 --> 01:19:33.169
<v UPOL>Right? Hi, is that for HCI?

1871
01:19:33.170 --> 01:19:34.709
<v UPOL>What nerve sign before?

1872
01:19:34.710 --> 01:19:36.969
<v UPOL>No, I guess that's a different way of looking at it.

1873
01:19:36.970 --> 01:19:39.309
<v ANDREY>Well, so yeah, that's really cool.

1874
01:19:39.310 --> 01:19:41.949
<v ANDREY>And we'll have a link to that paper again and

1875
01:19:41.950 --> 01:19:44.499
<v ANDREY>a description and our Substack.

1876
01:19:44.500 --> 01:19:47.379
<v ANDREY>So if you do want to get more into it,

1877
01:19:47.380 --> 01:19:49.569
<v ANDREY>you can just click and read it.

1878
01:19:49.570 --> 01:19:52.989
<v ANDREY>And that's just to touch on a bit

1879
01:19:52.990 --> 01:19:54.879
<v ANDREY>what has happened since.

1880
01:19:54.880 --> 01:19:57.369
<v ANDREY>In your research, you had actually a couple of

1881
01:19:57.370 --> 01:19:58.659
<v ANDREY>weeks.

1882
01:19:58.660 --> 01:20:01.689
<v ANDREY>So first up, you have the WHO

1883
01:20:01.690 --> 01:20:04.569
<v ANDREY>and explainable AI how AI background

1884
01:20:04.570 --> 01:20:07.179
<v ANDREY>shapes perceptions of AI explanations.

1885
01:20:07.180 --> 01:20:09.819
<v ANDREY>How does that relate to your prior work

1886
01:20:09.820 --> 01:20:12.609
<v ANDREY>and endless work? And sort of what what was

1887
01:20:12.610 --> 01:20:13.779
<v ANDREY>what is it?

1888
01:20:13.780 --> 01:20:16.449
<v UPOL>Absolutely. So I mean, this is directly related

1889
01:20:16.450 --> 01:20:17.799
<v UPOL>to a human centered, explainable way.

1890
01:20:17.800 --> 01:20:20.619
<v UPOL>I kind of work in the sense that

1891
01:20:20.620 --> 01:20:23.649
<v UPOL>not all humans are the same when it comes

1892
01:20:23.650 --> 01:20:25.599
<v UPOL>to interacting with the AI system.

1893
01:20:25.600 --> 01:20:28.269
<v UPOL>I don't think anyone will challenge that

1894
01:20:28.270 --> 01:20:29.859
<v UPOL>observation. Right.

1895
01:20:29.860 --> 01:20:32.559
<v UPOL>But then the question becomes, OK, who are

1896
01:20:32.560 --> 01:20:33.969
<v UPOL>these people?

1897
01:20:33.970 --> 01:20:36.819
<v UPOL>How do their different views or characteristics

1898
01:20:36.820 --> 01:20:39.939
<v UPOL>impact how they interpret explanations?

1899
01:20:39.940 --> 01:20:42.639
<v UPOL>So in this paper, it's just something

1900
01:20:42.640 --> 01:20:45.099
<v UPOL>that we looked at like a very critical dimension,

1901
01:20:45.100 --> 01:20:46.509
<v UPOL>which is any AI background.

1902
01:20:46.510 --> 01:20:48.459
<v UPOL>Like if you think about consumers of A.I.

1903
01:20:48.460 --> 01:20:50.349
<v UPOL>technology versus creators of A.I.

1904
01:20:50.350 --> 01:20:53.319
<v UPOL>technology, oftentimes consumers don't

1905
01:20:53.320 --> 01:20:55.539
<v UPOL>have the level of AI background that the creators

1906
01:20:55.540 --> 01:20:57.219
<v UPOL>have, right?

1907
01:20:57.220 --> 01:20:59.589
<v UPOL>So given that this background is a consequential

1908
01:20:59.590 --> 01:21:02.439
<v UPOL>dimension, but also the fact that it might be absent

1909
01:21:02.440 --> 01:21:05.469
<v UPOL>in the users of systems that we build.

1910
01:21:05.470 --> 01:21:09.189
<v UPOL>How does that background actually impact

1911
01:21:09.190 --> 01:21:10.749
<v UPOL>the perceptions of these A.I.

1912
01:21:10.750 --> 01:21:13.359
<v UPOL>explanations, right? Because again, we're making the

1913
01:21:13.360 --> 01:21:16.869
<v UPOL>explanations also for the receiver explaining

1914
01:21:16.870 --> 01:21:19.329
<v UPOL>that and then they explain that so that this is the

1915
01:21:19.330 --> 01:21:21.819
<v UPOL>paper that is, I think, the first paper that kind of

1916
01:21:21.820 --> 01:21:24.789
<v UPOL>explores the AI background

1917
01:21:24.790 --> 01:21:27.649
<v UPOL>as there's a dimension to

1918
01:21:27.650 --> 01:21:30.369
<v UPOL>to see like, well, how does that impact like we see

1919
01:21:30.370 --> 01:21:32.469
<v UPOL>humans, humans, but who are these humans?

1920
01:21:32.470 --> 01:21:35.799
<v UPOL>Well, let's look at two two groups of humans

1921
01:21:35.800 --> 01:21:38.109
<v UPOL>like people with and people without.

1922
01:21:38.110 --> 01:21:41.199
<v UPOL>So this paper kind of presents a study

1923
01:21:41.200 --> 01:21:43.809
<v UPOL>based largely actually on the Frogger work

1924
01:21:43.810 --> 01:21:46.539
<v UPOL>now way back when to kind of get

1925
01:21:46.540 --> 01:21:47.540
<v UPOL>at these questions.

1926
01:21:48.640 --> 01:21:51.249
<v ANDREY>Yeah, it makes me think also,

1927
01:21:51.250 --> 01:21:54.129
<v ANDREY>aside from like, you know, I develop

1928
01:21:54.130 --> 01:21:57.159
<v ANDREY>or not add it all up, or even just like programmer

1929
01:21:57.160 --> 01:22:00.559
<v ANDREY>who were and resources a person in sales.

1930
01:22:00.560 --> 01:22:03.309
<v ANDREY>You might interact with the AI system

1931
01:22:03.310 --> 01:22:05.919
<v ANDREY>differently, so it seems no good

1932
01:22:05.920 --> 01:22:07.449
<v ANDREY>to take into account, for sure.

1933
01:22:07.450 --> 01:22:08.450
<v ANDREY>Yeah.

1934
01:22:09.190 --> 01:22:12.759
<v ANDREY>And then I think also you had

1935
01:22:12.760 --> 01:22:15.519
<v ANDREY>this elevator explainability pitfalls

1936
01:22:15.520 --> 01:22:18.519
<v ANDREY>beyond dark patterns and explainable

1937
01:22:18.520 --> 01:22:21.579
<v ANDREY>AI, which sounds a little bit exciting.

1938
01:22:22.590 --> 01:22:24.339
<v ANDREY>So, yeah, what's that about?

1939
01:22:24.340 --> 01:22:27.009
<v UPOL>So this paper is actually related to the WHO

1940
01:22:27.010 --> 01:22:29.769
<v UPOL>in my paper, because one of the findings in WHO

1941
01:22:29.770 --> 01:22:32.859
<v UPOL>and say that we got was we're both

1942
01:22:32.860 --> 01:22:35.739
<v UPOL>groups. The group with AI and NONYE

1943
01:22:35.740 --> 01:22:38.469
<v UPOL>backgrounds had exhibited

1944
01:22:38.470 --> 01:22:41.259
<v UPOL>unwarranted faith in

1945
01:22:41.260 --> 01:22:44.499
<v UPOL>numerical based explanations

1946
01:22:44.500 --> 01:22:46.779
<v UPOL>that had no

1947
01:22:49.180 --> 01:22:50.709
<v UPOL>meaning behind them, so to speak.

1948
01:22:50.710 --> 01:22:52.989
<v UPOL>But even if people did not understand what the

1949
01:22:52.990 --> 01:22:55.779
<v UPOL>numbers meant, there was a level of over

1950
01:22:55.780 --> 01:22:57.219
<v UPOL>trust in them.

1951
01:22:57.220 --> 01:22:58.539
<v UPOL>So based on.

1952
01:22:58.540 --> 01:23:00.729
<v UPOL>That observation, what is interesting is like we

1953
01:23:00.730 --> 01:23:02.089
<v UPOL>were not trying to trick anyone, right?

1954
01:23:02.090 --> 01:23:04.539
<v UPOL>Like that's the importance of this finding that in

1955
01:23:04.540 --> 01:23:06.379
<v UPOL>the study, we were not trying to trick anyone.

1956
01:23:06.380 --> 01:23:08.589
<v UPOL>We just use the numerical explanations as a

1957
01:23:08.590 --> 01:23:11.229
<v UPOL>baseline. Our main instrument was the

1958
01:23:11.230 --> 01:23:13.750
<v UPOL>textual explanations, the actual rationale.

1959
01:23:15.130 --> 01:23:17.529
<v UPOL>And while trying to examine that, we were like, Oh

1960
01:23:17.530 --> 01:23:20.169
<v UPOL>my God, why are people like so in love

1961
01:23:20.170 --> 01:23:22.839
<v UPOL>with these numbers, then that they don't

1962
01:23:22.840 --> 01:23:25.809
<v UPOL>understand? Because we have qualitative data where

1963
01:23:25.810 --> 01:23:28.299
<v UPOL>they tell us, I don't understand it now, but I can

1964
01:23:28.300 --> 01:23:30.039
<v UPOL>understand it later.

1965
01:23:30.040 --> 01:23:33.099
<v UPOL>And what is interesting is that people with

1966
01:23:33.100 --> 01:23:35.619
<v UPOL>AI background and those without.

1967
01:23:35.620 --> 01:23:38.319
<v UPOL>Have different results

1968
01:23:38.320 --> 01:23:40.989
<v UPOL>for over trusting the A.I.,

1969
01:23:40.990 --> 01:23:42.879
<v UPOL>right? So over trusting the numbers.

1970
01:23:42.880 --> 01:23:45.849
<v UPOL>Excuse me. Yeah. So we started asking the questions.

1971
01:23:45.850 --> 01:23:48.939
<v UPOL>All right. There are many times where

1972
01:23:48.940 --> 01:23:52.569
<v UPOL>harmful effects can happen, like over trust,

1973
01:23:52.570 --> 01:23:55.239
<v UPOL>even when best of

1974
01:23:55.240 --> 01:23:57.969
<v UPOL>intentions are there, like in our case.

1975
01:23:57.970 --> 01:24:01.239
<v UPOL>Right. A lot of harmful work and explainable

1976
01:24:01.240 --> 01:24:03.969
<v UPOL>AI is couched under this term called dark patterns,

1977
01:24:03.970 --> 01:24:06.519
<v UPOL>which are basically deceptive practices.

1978
01:24:06.520 --> 01:24:08.979
<v UPOL>It's easiest to explain it from the other side, like

1979
01:24:08.980 --> 01:24:11.049
<v UPOL>if you think about like, you know, in certain

1980
01:24:11.050 --> 01:24:14.079
<v UPOL>websites, they have all these like like transparent

1981
01:24:14.080 --> 01:24:16.749
<v UPOL>like ads. And when you're trying to click the play

1982
01:24:16.750 --> 01:24:19.239
<v UPOL>button like 10000 windows, open up, right?

1983
01:24:19.240 --> 01:24:21.129
<v UPOL>And you have to take them 10000 politicians to get

1984
01:24:21.130 --> 01:24:23.829
<v UPOL>it. So there's a dark side that kind of drives

1985
01:24:23.830 --> 01:24:26.859
<v UPOL>clicks by tricking the user,

1986
01:24:26.860 --> 01:24:29.379
<v UPOL>you know, not all harm patterns like harmful

1987
01:24:29.380 --> 01:24:31.119
<v UPOL>patterns are created equal.

1988
01:24:31.120 --> 01:24:34.029
<v UPOL>So what happens when

1989
01:24:34.030 --> 01:24:36.699
<v UPOL>harmful effects emerge, when there is

1990
01:24:36.700 --> 01:24:38.769
<v UPOL>no bad intention behind it?

1991
01:24:38.770 --> 01:24:41.469
<v UPOL>Right, right? So to answer that question, we

1992
01:24:41.470 --> 01:24:44.409
<v UPOL>wrote another kind of conceptual paper, and we call

1993
01:24:44.410 --> 01:24:47.319
<v UPOL>these things explainable the pitfalls.

1994
01:24:47.320 --> 01:24:49.989
<v UPOL>Right? So these pitfalls are certain

1995
01:24:49.990 --> 01:24:52.599
<v UPOL>things that you might not intend for bad things

1996
01:24:52.600 --> 01:24:55.179
<v UPOL>to happen, but like a pitfall in a real piece of

1997
01:24:55.180 --> 01:24:58.059
<v UPOL>like in the real world, you might inadvertently fall

1998
01:24:58.060 --> 01:24:59.349
<v UPOL>into it. Right?

1999
01:24:59.350 --> 01:25:00.819
<v UPOL>Because, you know, it's not like pitfalls have there

2000
01:25:00.820 --> 01:25:02.169
<v UPOL>to like trap people.

2001
01:25:02.170 --> 01:25:04.869
<v UPOL>Sometimes the pitfalls emerge in nature, in

2002
01:25:04.870 --> 01:25:08.049
<v UPOL>jungles and other places by the construction

2003
01:25:08.050 --> 01:25:10.839
<v UPOL>site, and that you might inadvertently fall into

2004
01:25:10.840 --> 01:25:13.509
<v UPOL>it. So this paper is kind of trying to articulate

2005
01:25:13.510 --> 01:25:15.819
<v UPOL>what are explainability pitfalls?

2006
01:25:15.820 --> 01:25:17.559
<v UPOL>How do you address them?

2007
01:25:17.560 --> 01:25:19.869
<v UPOL>What are some of the strategies to mitigate them?

2008
01:25:19.870 --> 01:25:22.089
<v UPOL>So this is more of another kind of a conceptual

2009
01:25:22.090 --> 01:25:24.939
<v UPOL>paper situated with a case study,

2010
01:25:24.940 --> 01:25:27.649
<v UPOL>and it recently got into the human centered

2011
01:25:27.650 --> 01:25:29.169
<v UPOL>A.I. workshop at in Europe.

2012
01:25:29.170 --> 01:25:31.029
<v UPOL>So this year, so we are looking forward to sharing

2013
01:25:31.030 --> 01:25:32.509
<v UPOL>it with the community as well.

2014
01:25:32.510 --> 01:25:35.319
<v ANDREY>Oh, it's exciting. Yeah, that's roughly

2015
01:25:35.320 --> 01:25:37.929
<v ANDREY>in a moment, right? Yeah, yeah.

2016
01:25:37.930 --> 01:25:39.529
<v ANDREY>Yeah, that's that's interesting.

2017
01:25:39.530 --> 01:25:42.279
<v ANDREY>This concept of sheer is something you

2018
01:25:42.280 --> 01:25:45.999
<v ANDREY>should avoid doing seems like a good idea,

2019
01:25:46.000 --> 01:25:48.759
<v ANDREY>almost publishing negative results, which

2020
01:25:48.760 --> 01:25:50.229
<v ANDREY>is which is fun.

2021
01:25:51.820 --> 01:25:54.939
<v ANDREY>Well, we went for a lot of your work and

2022
01:25:54.940 --> 01:25:57.579
<v ANDREY>then almost traced from

2023
01:25:57.580 --> 01:25:59.049
<v ANDREY>the beginning to the present.

2024
01:25:59.050 --> 01:26:01.269
<v ANDREY>But of course, it's also important again, to

2025
01:26:01.270 --> 01:26:04.239
<v ANDREY>mention, as you have done before, that this was,

2026
01:26:04.240 --> 01:26:06.249
<v ANDREY>you know, a lot of this was done with many

2027
01:26:06.250 --> 01:26:09.579
<v ANDREY>collaborators and you built on a lot of

2028
01:26:09.580 --> 01:26:12.099
<v ANDREY>prior research, obviously in many fields.

2029
01:26:12.100 --> 01:26:14.859
<v ANDREY>This is true of any research job

2030
01:26:14.860 --> 01:26:16.030
<v ANDREY>because you were present,

2031
01:26:18.100 --> 01:26:21.159
<v ANDREY>maybe beyond your papers.

2032
01:26:21.160 --> 01:26:23.799
<v ANDREY>What kind of is the

2033
01:26:23.800 --> 01:26:26.409
<v ANDREY>situation when it comes to community

2034
01:26:26.410 --> 01:26:29.109
<v ANDREY>working on XIII, Nick's

2035
01:26:29.110 --> 01:26:32.859
<v ANDREY>family AI and also human centered XIII,

2036
01:26:32.860 --> 01:26:36.369
<v ANDREY>you know, is is is your being human centered

2037
01:26:36.370 --> 01:26:39.549
<v ANDREY>or socio technical? Is that becoming more

2038
01:26:39.550 --> 01:26:42.189
<v ANDREY>popular or are more people so aware

2039
01:26:42.190 --> 01:26:43.839
<v ANDREY>of it, that sort of thing?

2040
01:26:43.840 --> 01:26:46.149
<v UPOL>No, you're absolutely right. I think, you know, I

2041
01:26:46.150 --> 01:26:48.009
<v UPOL>stand in the shoulder of giants, right?

2042
01:26:48.010 --> 01:26:51.249
<v UPOL>There's no two ways about it without the fantastic

2043
01:26:51.250 --> 01:26:52.839
<v UPOL>people I work with.

2044
01:26:52.840 --> 01:26:55.180
<v UPOL>None of this work becomes reality

2045
01:26:56.330 --> 01:26:58.989
<v UPOL>and the communities, and it's something

2046
01:26:58.990 --> 01:27:00.189
<v UPOL>that I care deeply about.

2047
01:27:00.190 --> 01:27:03.189
<v UPOL>So we have been very lucky in this context.

2048
01:27:03.190 --> 01:27:05.979
<v UPOL>And by 2020 one, we

2049
01:27:05.980 --> 01:27:08.499
<v UPOL>were able to host the first human centered

2050
01:27:08.500 --> 01:27:10.299
<v UPOL>explainable AI workshop.

2051
01:27:10.300 --> 01:27:12.579
<v UPOL>It was actually one of the largest attended

2052
01:27:12.580 --> 01:27:15.249
<v UPOL>workshops and trials during

2053
01:27:15.250 --> 01:27:17.889
<v UPOL>more than 100 people came over

2054
01:27:17.890 --> 01:27:19.209
<v UPOL>14 countries.

2055
01:27:20.470 --> 01:27:23.919
<v UPOL>So we had a stellar group of

2056
01:27:23.920 --> 01:27:25.389
<v UPOL>papers.

2057
01:27:25.390 --> 01:27:28.029
<v UPOL>We had a keynote from Tim Miller,

2058
01:27:28.030 --> 01:27:30.279
<v UPOL>an expert panel discussions.

2059
01:27:30.280 --> 01:27:32.769
<v UPOL>So I think that community is still going on.

2060
01:27:32.770 --> 01:27:35.769
<v UPOL>And actually, we did just propose to host the second

2061
01:27:35.770 --> 01:27:38.499
<v UPOL>workshop at Chi.

2062
01:27:38.500 --> 01:27:41.409
<v UPOL>And I think after this, we want to take it beyond.

2063
01:27:41.410 --> 01:27:43.449
<v UPOL>We want to take it down. Europe's who want to take

2064
01:27:43.450 --> 01:27:46.179
<v UPOL>it to triple AI, to try to see

2065
01:27:46.180 --> 01:27:48.829
<v UPOL>how more can we intersect with

2066
01:27:48.830 --> 01:27:51.559
<v UPOL>more other communities around

2067
01:27:51.560 --> 01:27:54.729
<v UPOL>HCI, like other relevant social groups, right?

2068
01:27:54.730 --> 01:27:57.559
<v UPOL>The computer vision people and this people.

2069
01:27:57.560 --> 01:28:00.289
<v UPOL>So. These are some things that we deeply care about,

2070
01:28:00.290 --> 01:28:02.959
<v UPOL>and that is something that I would

2071
01:28:02.960 --> 01:28:06.139
<v UPOL>that I'm kind of like

2072
01:28:06.140 --> 01:28:07.140
<v UPOL>looking forward.

2073
01:28:07.960 --> 01:28:09.939
<v ANDREY>Yeah, definitely so.

2074
01:28:09.940 --> 01:28:13.269
<v ANDREY>And just to get it a bit more into that, you know,

2075
01:28:13.270 --> 01:28:15.909
<v ANDREY>what's next for you both in terms of this community

2076
01:28:15.910 --> 01:28:18.579
<v ANDREY>aspect of, you know, having various

2077
01:28:18.580 --> 01:28:21.279
<v ANDREY>events to let

2078
01:28:21.280 --> 01:28:23.679
<v ANDREY>people know about to see you and also in terms of, I

2079
01:28:23.680 --> 01:28:25.809
<v ANDREY>guess, where your research is headed.

2080
01:28:25.810 --> 01:28:28.509
<v UPOL>Yeah, I think for me, I as I share, if there's

2081
01:28:28.510 --> 01:28:30.519
<v UPOL>a project that I'm doing with radiation oncology,

2082
01:28:30.520 --> 01:28:33.039
<v UPOL>it's actually exploring social transparency in their

2083
01:28:33.040 --> 01:28:35.619
<v UPOL>world and this has been actually a value long term

2084
01:28:35.620 --> 01:28:36.789
<v UPOL>engagement.

2085
01:28:36.790 --> 01:28:39.129
<v UPOL>I've been working with them for more than two years

2086
01:28:39.130 --> 01:28:41.919
<v UPOL>now. I've also kind of been working with the Data

2087
01:28:41.920 --> 01:28:44.619
<v UPOL>and Society Institute on

2088
01:28:44.620 --> 01:28:47.739
<v UPOL>Algorithmic Justice Issues around the Global South.

2089
01:28:47.740 --> 01:28:50.649
<v UPOL>So you know what happens when we all

2090
01:28:50.650 --> 01:28:53.569
<v UPOL>talk a lot about algorithmic deployment

2091
01:28:53.570 --> 01:28:56.229
<v UPOL>right before deployment?

2092
01:28:56.230 --> 01:28:57.639
<v UPOL>Dataset creation?

2093
01:28:57.640 --> 01:29:00.919
<v UPOL>But what happens when algorithms get taken out

2094
01:29:00.920 --> 01:29:03.339
<v UPOL>and what happens, then what happens when they're no

2095
01:29:03.340 --> 01:29:04.569
<v UPOL>longer used?

2096
01:29:04.570 --> 01:29:07.059
<v UPOL>So there is a project that I'm running that has

2097
01:29:07.060 --> 01:29:09.789
<v UPOL>explainability component, as well as algorithmic

2098
01:29:09.790 --> 01:29:13.269
<v UPOL>justice component around being creating

2099
01:29:13.270 --> 01:29:15.969
<v UPOL>the algorithmic trading of the

2100
01:29:15.970 --> 01:29:18.669
<v UPOL>DC exams, which are like

2101
01:29:18.670 --> 01:29:21.699
<v UPOL>basically international exams administered by Ofqual

2102
01:29:21.700 --> 01:29:24.459
<v UPOL>and UK governing boards.

2103
01:29:24.460 --> 01:29:26.829
<v UPOL>But these exams are actually administered in over

2104
01:29:26.830 --> 01:29:28.569
<v UPOL>one hundred and sixty countries.

2105
01:29:28.570 --> 01:29:31.269
<v UPOL>So you might recall that in August

2106
01:29:31.270 --> 01:29:33.819
<v UPOL>of twenty twenty, there were protests around an

2107
01:29:33.820 --> 01:29:36.879
<v UPOL>algorithm grading a lot of students know.

2108
01:29:36.880 --> 01:29:38.589
<v UPOL>While the reporting was great.

2109
01:29:38.590 --> 01:29:40.299
<v UPOL>It only focused on the U.K.

2110
01:29:40.300 --> 01:29:42.879
<v UPOL>we really don't know what happened in the other one

2111
01:29:42.880 --> 01:29:44.859
<v UPOL>hundred and sixty countries where these exams were

2112
01:29:44.860 --> 01:29:46.059
<v UPOL>administered.

2113
01:29:46.060 --> 01:29:48.909
<v UPOL>So, you know, beyond, you know, as I say, denies you

2114
01:29:48.910 --> 01:29:50.739
<v UPOL>kindly shared my bio, right?

2115
01:29:50.740 --> 01:29:53.749
<v UPOL>What happens to the people who are not on the table?

2116
01:29:53.750 --> 01:29:56.959
<v UPOL>And I think if you don't amplify

2117
01:29:56.960 --> 01:29:59.269
<v UPOL>people's voices, we're not at the table, they often

2118
01:29:59.270 --> 01:30:01.149
<v UPOL>end up on the menu.

2119
01:30:01.150 --> 01:30:03.729
<v UPOL>So I think coming for the circle like that,

2120
01:30:03.730 --> 01:30:06.039
<v UPOL>something that I'm deeply curious about, so that's

2121
01:30:06.040 --> 01:30:08.709
<v UPOL>roughly like, you know what things are

2122
01:30:08.710 --> 01:30:11.199
<v UPOL>and if I have the privilege of giving a keynote at

2123
01:30:11.200 --> 01:30:13.569
<v UPOL>the World Usability Day actually tomorrow on

2124
01:30:13.570 --> 01:30:16.299
<v UPOL>November 11th, I have some invited talks lined

2125
01:30:16.300 --> 01:30:19.479
<v UPOL>up at the University of Buffalo on the 30th

2126
01:30:19.480 --> 01:30:22.239
<v UPOL>and then an expert panel discussion actually at

2127
01:30:22.240 --> 01:30:25.119
<v UPOL>the university's medical school, the Stanford

2128
01:30:25.120 --> 01:30:27.909
<v UPOL>Medical School, to the conference.

2129
01:30:27.910 --> 01:30:30.639
<v UPOL>So that's that's pretty much like like a ramp up

2130
01:30:30.640 --> 01:30:31.899
<v UPOL>to the end of the year.

2131
01:30:31.900 --> 01:30:33.249
<v ANDREY>Cool, yeah.

2132
01:30:33.250 --> 01:30:35.919
<v ANDREY>Sadly, will release as five guests

2133
01:30:35.920 --> 01:30:37.029
<v ANDREY>pass through 11.

2134
01:30:38.230 --> 01:30:41.109
<v ANDREY>But will these talks be recorded

2135
01:30:41.110 --> 01:30:42.769
<v ANDREY>or public? Could be.

2136
01:30:42.770 --> 01:30:44.139
<v UPOL>That's a very good point.

2137
01:30:44.140 --> 01:30:46.779
<v UPOL>Thank you so much for asking. So I am going to check

2138
01:30:46.780 --> 01:30:49.209
<v UPOL>it. I wonder what I would recommend if the listeners

2139
01:30:49.210 --> 01:30:50.439
<v UPOL>are there.

2140
01:30:50.440 --> 01:30:53.079
<v UPOL>If you check out my Twitter, if they are

2141
01:30:53.080 --> 01:30:55.839
<v UPOL>public, I will be sure to make sure that they

2142
01:30:55.840 --> 01:30:58.599
<v UPOL>are published and shared widely.

2143
01:30:58.600 --> 01:31:01.279
<v UPOL>So as of now, I'm not sure which of these would

2144
01:31:01.280 --> 01:31:02.650
<v UPOL>be public versus not.

2145
01:31:03.670 --> 01:31:07.089
<v UPOL>But if they are, I will publish them on my Twitter.

2146
01:31:07.090 --> 01:31:09.789
<v UPOL>So if people are interested and I think we can also

2147
01:31:09.790 --> 01:31:12.399
<v UPOL>add links to them after the

2148
01:31:12.400 --> 01:31:13.400
<v UPOL>podcast.

2149
01:31:13.930 --> 01:31:15.399
<v ANDREY>Exactly. Yeah. So you can look down that

2150
01:31:15.400 --> 01:31:17.999
<v ANDREY>description. We'll figure it out and

2151
01:31:18.000 --> 01:31:20.829
<v ANDREY>we'll have links to

2152
01:31:20.830 --> 01:31:22.929
<v ANDREY>this and all papers and everything.

2153
01:31:24.100 --> 01:31:26.379
<v ANDREY>All right. So that's cool.

2154
01:31:26.380 --> 01:31:29.139
<v ANDREY>And then as I like to wrap up,

2155
01:31:29.140 --> 01:31:31.809
<v ANDREY>after all this intense discussion of research

2156
01:31:31.810 --> 01:31:34.749
<v ANDREY>and ideas and studies, just,

2157
01:31:34.750 --> 01:31:36.999
<v ANDREY>you know, a little bit about you and not your

2158
01:31:37.000 --> 01:31:38.679
<v ANDREY>research.

2159
01:31:38.680 --> 01:31:40.839
<v ANDREY>What do you do these days?

2160
01:31:40.840 --> 01:31:43.359
<v ANDREY>Or, you know, in general,

2161
01:31:44.440 --> 01:31:46.929
<v ANDREY>beyond research, what are your main hobbies?

2162
01:31:46.930 --> 01:31:48.699
<v ANDREY>What are your main interests?

2163
01:31:48.700 --> 01:31:51.339
<v UPOL>Yeah, I guess I'm, you know, I've been I

2164
01:31:51.340 --> 01:31:52.569
<v UPOL>love to cook.

2165
01:31:52.570 --> 01:31:54.189
<v UPOL>I think that is something that has been

2166
01:31:55.270 --> 01:31:57.969
<v UPOL>during the stay at home and pandemic

2167
01:31:57.970 --> 01:32:00.069
<v UPOL>mode has been a blessing.

2168
01:32:01.180 --> 01:32:04.209
<v UPOL>I absolutely love European

2169
01:32:04.210 --> 01:32:05.959
<v UPOL>football or soccer.

2170
01:32:05.960 --> 01:32:07.959
<v UPOL>All my team is not doing very well.

2171
01:32:07.960 --> 01:32:10.749
<v UPOL>Manchester United right now, but I

2172
01:32:10.750 --> 01:32:13.779
<v UPOL>tend to. That is my escape and I also

2173
01:32:13.780 --> 01:32:17.319
<v UPOL>play this game called Football Manager.

2174
01:32:17.320 --> 01:32:20.229
<v UPOL>I have not like fantasy football,

2175
01:32:20.230 --> 01:32:22.209
<v UPOL>but it's like kind of like that where it's a very

2176
01:32:22.210 --> 01:32:23.259
<v UPOL>data driven engine.

2177
01:32:24.850 --> 01:32:27.579
<v UPOL>And that's how it comes up to

2178
01:32:28.960 --> 01:32:31.599
<v UPOL>like this game engine that kind of predicts the

2179
01:32:31.600 --> 01:32:33.759
<v UPOL>future. I'm going to simulate games.

2180
01:32:33.760 --> 01:32:36.669
<v UPOL>That is my escape in terms of

2181
01:32:36.670 --> 01:32:38.979
<v UPOL>all the things in reality.

2182
01:32:38.980 --> 01:32:41.739
<v UPOL>But I absolutely a big

2183
01:32:41.740 --> 01:32:43.289
<v UPOL>fan of old school hip hop.

2184
01:32:43.290 --> 01:32:45.279
<v UPOL>So I listen to a lot of music.

2185
01:32:45.280 --> 01:32:48.639
<v UPOL>I, whenever I get some time, I do

2186
01:32:48.640 --> 01:32:49.779
<v UPOL>mix beats

2187
01:32:51.430 --> 01:32:54.459
<v UPOL>on my own time for my own

2188
01:32:54.460 --> 01:32:56.499
<v UPOL>enjoyment. I don't think I have a song called

2189
01:32:56.500 --> 01:32:59.499
<v UPOL>Account or anything now, but those are my

2190
01:32:59.500 --> 01:33:01.779
<v UPOL>ways of keeping sane.

2191
01:33:01.780 --> 01:33:04.239
<v UPOL>But most importantly, one of the most cherished

2192
01:33:04.240 --> 01:33:06.999
<v UPOL>things that I do is

2193
01:33:07.000 --> 01:33:09.819
<v UPOL>mentoring young researchers,

2194
01:33:09.820 --> 01:33:13.089
<v UPOL>especially who are underrepresented, especially

2195
01:33:13.090 --> 01:33:15.399
<v UPOL>who are from the global south.

2196
01:33:15.400 --> 01:33:18.009
<v UPOL>So I'm very proud of all

2197
01:33:18.010 --> 01:33:21.339
<v UPOL>the mentees that have taught me so much

2198
01:33:21.340 --> 01:33:23.649
<v UPOL>throughout the years, like ever since 2000.

2199
01:33:23.650 --> 01:33:26.259
<v UPOL>I think 11 12, I've

2200
01:33:26.260 --> 01:33:28.660
<v UPOL>had the privilege of mentoring around 100 hundred

2201
01:33:28.661 --> 01:33:30.909
<v UPOL>people from many different countries in Asia and

2202
01:33:30.910 --> 01:33:33.639
<v UPOL>Africa and kind of guiding them through

2203
01:33:33.640 --> 01:33:35.349
<v UPOL>high school and those.

2204
01:33:35.350 --> 01:33:38.319
<v UPOL>That is something that like gives me a lot of joy

2205
01:33:38.320 --> 01:33:40.149
<v UPOL>actually, like whenever I get free time.

2206
01:33:40.150 --> 01:33:42.399
<v UPOL>That's actually what I do. And during application

2207
01:33:42.400 --> 01:33:44.439
<v UPOL>season, it's usually gets tough because we have a

2208
01:33:44.440 --> 01:33:48.009
<v UPOL>lot of requests to review applications

2209
01:33:48.010 --> 01:33:49.629
<v UPOL>because, you know, sometimes as you can imagine,

2210
01:33:49.630 --> 01:33:51.279
<v UPOL>life like the application.

2211
01:33:51.280 --> 01:33:54.009
<v UPOL>The statement of purpose is often a black box,

2212
01:33:54.010 --> 01:33:55.839
<v UPOL>right? And you don't know what to write.

2213
01:33:55.840 --> 01:33:58.629
<v UPOL>So that is one thing that I get a lot of joy

2214
01:33:58.630 --> 01:33:59.859
<v UPOL>from.

2215
01:33:59.860 --> 01:34:01.549
<v ANDREY>Yeah, that's that's fantastic.

2216
01:34:01.550 --> 01:34:04.149
<v ANDREY>I think we all guys share

2217
01:34:04.150 --> 01:34:07.479
<v ANDREY>what a good deal of mentorship as Diaz's adviser

2218
01:34:07.480 --> 01:34:10.179
<v ANDREY>for a reason, you know, as an assigned

2219
01:34:10.180 --> 01:34:13.419
<v ANDREY>mentor. So it's it does feel nice to give back,

2220
01:34:13.420 --> 01:34:16.689
<v ANDREY>and I have always enjoyed being a teaching assistant

2221
01:34:16.690 --> 01:34:19.449
<v ANDREY>and these various things are always pretty

2222
01:34:19.450 --> 01:34:20.949
<v ANDREY>rewarding for me.

2223
01:34:22.330 --> 01:34:24.999
<v ANDREY>Well, that was a really fun interview.

2224
01:34:25.000 --> 01:34:27.609
<v ANDREY>It was great to see

2225
01:34:27.610 --> 01:34:30.669
<v ANDREY>or hear about this human

2226
01:34:30.670 --> 01:34:33.609
<v ANDREY>centered A.I. as a researcher who talks

2227
01:34:33.610 --> 01:34:36.219
<v ANDREY>of robots refreshing to think about

2228
01:34:36.220 --> 01:34:37.689
<v ANDREY>people for once.

2229
01:34:37.690 --> 01:34:40.329
<v ANDREY>Thank you so much for being

2230
01:34:40.330 --> 01:34:41.769
<v ANDREY>on the podcast.

2231
01:34:41.770 --> 01:34:42.909
<v UPOL>My pleasure. Thank you, Andre.

2232
01:34:42.910 --> 01:34:45.669
<v UPOL>I so appreciate the opportunity to talk to you

2233
01:34:45.670 --> 01:34:48.159
<v UPOL>and an animal in a way to you.

2234
01:34:48.160 --> 01:34:49.779
<v UPOL>Talk to the listeners.

2235
01:34:49.780 --> 01:34:50.829
<v UPOL>Thank you.

2236
01:34:50.830 --> 01:34:52.149
<v ANDREY>Absolutely.

2237
01:34:52.150 --> 01:34:55.269
<v ANDREY>And once again, this is The Gradient podcast.

2238
01:34:55.270 --> 01:34:58.149
<v ANDREY>Check out our magazine website

2239
01:34:58.150 --> 01:35:00.449
<v ANDREY>at The Gradient dot com.

2240
01:35:00.450 --> 01:35:02.939
<v ANDREY>To you, Earl. And our newsletter and actually this

2241
01:35:02.940 --> 01:35:05.819
<v ANDREY>podcast at The Gradient pub that Substack dot

2242
01:35:05.820 --> 01:35:08.999
<v ANDREY>com, you can support us there by subscribing

2243
01:35:09.000 --> 01:35:11.759
<v ANDREY>and also share all of this review on

2244
01:35:11.760 --> 01:35:14.429
<v ANDREY>this Apple and all these kinds

2245
01:35:14.430 --> 01:35:16.709
<v ANDREY>of things. So if you dig this stuff, we would

2246
01:35:16.710 --> 01:35:18.479
<v ANDREY>appreciate your support.

2247
01:35:18.480 --> 01:35:21.089
<v ANDREY>Thank you so much for listening and be sure to tune

2248
01:35:21.090 --> 01:35:22.890
<v ANDREY>into our future episodes.

